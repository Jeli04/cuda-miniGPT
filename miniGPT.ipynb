{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VE_CTcJ6P6L",
        "outputId": "a8d58804-03ca-4b2f-9bee-b5958bcc1ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n",
            "Device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# read it in to inspect it\n",
        "with open('All_Lecture_Transcript.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Settings\n",
        "layers = 6\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "head_dim = 64\n",
        "block_size = 128\n",
        "seq_len = 128\n",
        "\n",
        "# training\n",
        "split = 0.9\n",
        "batch_size = 16\n",
        "epochs = 8\n",
        "max_iters = 5000\n",
        "eval_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device\")\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, d_model, head_dim, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(d_model, head_dim, bias=False)\n",
        "    self.query = nn.Linear(d_model, head_dim, bias=False)\n",
        "    self.value = nn.Linear(d_model, head_dim, bias=False)\n",
        "    self.scale = torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
        "    self.mask = torch.tril(torch.ones(block_size, block_size)).to(device) # why?\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    # attn = softmax(QK/sqrt(head_dim)) * V\n",
        "    attn_weight = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
        "    attn_weight = attn_weight.masked_fill(self.mask[:T,:T] == 0, float('-inf'))\n",
        "    attn_weight = F.softmax(attn_weight, dim=-1)\n",
        "    attn_weight = self.dropout(attn_weight)\n",
        "    out = torch.matmul(attn_weight, v)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.attn_heads = nn.ModuleList([AttentionHead(d_model, self.head_dim) for _ in range(n_heads)])\n",
        "    self.proj = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.attn_heads], dim =-1) # (B, L, head_dim) -> (B, L, d_model)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.mha = MultiheadAttention(d_model, n_heads)\n",
        "    self.ffwd = nn.Sequential(\n",
        "        nn.Linear(d_model, 4 * d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * d_model, d_model),\n",
        "    )\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.mha(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, block_size, layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "    self.block = nn.Sequential(*[Block(d_model, n_heads) for _ in range(layers)])\n",
        "    self.ln_f = nn.LayerNorm(d_model)\n",
        "    self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    B, T = x.shape\n",
        "\n",
        "    x = self.token_embedding_table(x) + self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = self.block(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, x, max_tokens=500):\n",
        "    for _ in range(max_tokens):\n",
        "      x_cond = x[:, -block_size:]\n",
        "      logits, loss = self(x_cond)\n",
        "      logits = logits[:, -1, :] # get the last output\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      x_next = torch.multinomial(probs, num_samples=1)\n",
        "      x = torch.cat((x, x_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/model.pth\"\n",
        "model = MiniGPT(vocab_size, d_model, n_heads, block_size, layers).to(device)\n",
        "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
        "\n",
        "context = data[:50]\n",
        "context = context.unsqueeze(0).to(device)\n",
        "\n",
        "text = \"Okay. Alright. It seems like, the HDMI cable is not working, so I'm gonna go with plan b. I'm I'm zooming to myself right now. Right.\"\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "context = data.unsqueeze(0).to(device)\n",
        "start_time = time.time_ns()\n",
        "output = model.generate(context, max_tokens=200)[0].tolist()\n",
        "end_time = time.time_ns()\n",
        "print(f\"Time taken: {(end_time - start_time)/1000000000} s\")\n",
        "print(decode(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orHLEI5Q7ZO3",
        "outputId": "173743b9-fdb7-4476-8c21-5640b9e1902e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 5.517462883 s\n",
            "Okay. Alright. It seems like, the HDMI cable is not working, so I'm gonna go with plan b. I'm I'm zooming to myself right now. Right. Right? You say I mean, you access this? Yes.\n",
            "\n",
            "Okay. Good. Are we lost? Yes. Okay. Right.\n",
            "\n",
            "So If you have a cc, you actually up that address in this case? What happened? I think it's Yeah. That's, lik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "pth_path = \"/content/model.pth\"\n",
        "\n",
        "state = torch.load(pth_path, map_location='cpu')\n",
        "\n",
        "if isinstance(state, dict) and 'state_dict' in state:\n",
        "    state_dict = state['state_dict']\n",
        "elif isinstance(state, dict) and 'model' in state:\n",
        "    state_dict = state['model']\n",
        "else:\n",
        "    state_dict = state\n",
        "\n",
        "# Create output directory if not exists\n",
        "os.makedirs(\"/content/weights_dump\", exist_ok=True)\n",
        "\n",
        "print(len(state_dict))\n",
        "for name, weights in state_dict.items():\n",
        "    print(name, weights.shape)\n",
        "    weights = state_dict[name].cpu().numpy()\n",
        "    np.savetxt(f\"weights_dump/{name}.txt\", weights, fmt=\"%.8f\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8DJxkhpmHQ",
        "outputId": "7de0dd9c-d78d-4db4-da44-37a9b6451a59"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210\n",
            "token_embedding_table.weight torch.Size([75, 512])\n",
            "position_embedding_table.weight torch.Size([128, 512])\n",
            "block.0.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.0.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.0.mha.proj.weight torch.Size([512, 512])\n",
            "block.0.mha.proj.bias torch.Size([512])\n",
            "block.0.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.0.ffwd.0.bias torch.Size([2048])\n",
            "block.0.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.0.ffwd.2.bias torch.Size([512])\n",
            "block.0.ln1.weight torch.Size([512])\n",
            "block.0.ln1.bias torch.Size([512])\n",
            "block.0.ln2.weight torch.Size([512])\n",
            "block.0.ln2.bias torch.Size([512])\n",
            "block.1.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.1.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.1.mha.proj.weight torch.Size([512, 512])\n",
            "block.1.mha.proj.bias torch.Size([512])\n",
            "block.1.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.1.ffwd.0.bias torch.Size([2048])\n",
            "block.1.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.1.ffwd.2.bias torch.Size([512])\n",
            "block.1.ln1.weight torch.Size([512])\n",
            "block.1.ln1.bias torch.Size([512])\n",
            "block.1.ln2.weight torch.Size([512])\n",
            "block.1.ln2.bias torch.Size([512])\n",
            "block.2.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.2.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.2.mha.proj.weight torch.Size([512, 512])\n",
            "block.2.mha.proj.bias torch.Size([512])\n",
            "block.2.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.2.ffwd.0.bias torch.Size([2048])\n",
            "block.2.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.2.ffwd.2.bias torch.Size([512])\n",
            "block.2.ln1.weight torch.Size([512])\n",
            "block.2.ln1.bias torch.Size([512])\n",
            "block.2.ln2.weight torch.Size([512])\n",
            "block.2.ln2.bias torch.Size([512])\n",
            "block.3.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.3.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.3.mha.proj.weight torch.Size([512, 512])\n",
            "block.3.mha.proj.bias torch.Size([512])\n",
            "block.3.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.3.ffwd.0.bias torch.Size([2048])\n",
            "block.3.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.3.ffwd.2.bias torch.Size([512])\n",
            "block.3.ln1.weight torch.Size([512])\n",
            "block.3.ln1.bias torch.Size([512])\n",
            "block.3.ln2.weight torch.Size([512])\n",
            "block.3.ln2.bias torch.Size([512])\n",
            "block.4.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.4.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.4.mha.proj.weight torch.Size([512, 512])\n",
            "block.4.mha.proj.bias torch.Size([512])\n",
            "block.4.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.4.ffwd.0.bias torch.Size([2048])\n",
            "block.4.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.4.ffwd.2.bias torch.Size([512])\n",
            "block.4.ln1.weight torch.Size([512])\n",
            "block.4.ln1.bias torch.Size([512])\n",
            "block.4.ln2.weight torch.Size([512])\n",
            "block.4.ln2.bias torch.Size([512])\n",
            "block.5.mha.attn_heads.0.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.0.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.0.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.1.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.1.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.1.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.2.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.2.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.2.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.3.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.3.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.3.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.4.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.4.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.4.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.5.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.5.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.5.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.6.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.6.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.6.value.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.7.key.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.7.query.weight torch.Size([64, 512])\n",
            "block.5.mha.attn_heads.7.value.weight torch.Size([64, 512])\n",
            "block.5.mha.proj.weight torch.Size([512, 512])\n",
            "block.5.mha.proj.bias torch.Size([512])\n",
            "block.5.ffwd.0.weight torch.Size([2048, 512])\n",
            "block.5.ffwd.0.bias torch.Size([2048])\n",
            "block.5.ffwd.2.weight torch.Size([512, 2048])\n",
            "block.5.ffwd.2.bias torch.Size([512])\n",
            "block.5.ln1.weight torch.Size([512])\n",
            "block.5.ln1.bias torch.Size([512])\n",
            "block.5.ln2.weight torch.Size([512])\n",
            "block.5.ln2.bias torch.Size([512])\n",
            "ln_f.weight torch.Size([512])\n",
            "ln_f.bias torch.Size([512])\n",
            "lm_head.weight torch.Size([75, 512])\n",
            "lm_head.bias torch.Size([75])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "vocab_dict = {str(i): itos[i] for i in range(len(itos))}\n",
        "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved vocabulary dictionary to vocab.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c17M039FqX87",
        "outputId": "60d03712-733f-41b2-8f1f-6fab58e13dc3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'itos' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2496163392>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'itos' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MHd1Wa3tAl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}