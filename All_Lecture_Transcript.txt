
Okay. Alright. It seems like, the HDMI cable is not working, so I'm gonna go with plan b. I'm I'm zooming to myself right now. Right.

I'm recording. Okay. Yes. I'm recording. So, hopefully, the Internet doesn't die or else I won't be able to get the sub.

Okay. So I think what was it? Today's Thursday. Yes. So on Tuesday, right, it was just, introduction to the course.

So today, we're we're starting getting in a bit into, you know, kind of, introduction to how we could program GPUs, a little bit of CUDA at the end of the class. But I know a lot of you come from different backgrounds. Some of you probably never took operating systems or architecture or don't remember it anymore. Right? You you flush your cache or whatever.

So we'll just kinda do a really quick overview of it, hopefully, just to, you know, kinda give you a a bit of a rush here. Alright. So we're gonna do a super quick architecture OS type of overview. So, hopefully, you know, if you if you still remember, you know, you could speak up. Alright.

So how many of you have seen something like this before? Right. Yeah. It's the hardware software stack. Right?

So so throughout your undergrad education, you took a whole bunch of courses. Right? They kinda span this whole stack. Right? You learned c, c plus plus, and which course is c s 10.

Right? Yes. That's when you point in programming, I assume. Right. So they're like like, c s 10.

Operating systems is one five two. +1 53. Great. I keep getting those mixed up. Right?

Like, +1 53. You learn algorithms and all these other stuff. Right? These not just make your your applications more efficient. And then you learn assembly in 61.

Yes. Right. 61. Architecture in one six one. And then auto low level stuff.

Right? You learn how to make adders in one twenty b a. Oh, you don't need to take b anymore. Right? A.

Right? So you have one twenty a. EE people took a lot of stuff on the bottom. Right? So so EE folks, right, we have courses on transistors.

You learn about logic gates. I think, one six eight, I think you you make logic gates and and be assigned. Right? And then you lay out all the metal layers and everything else like that. Right?

So those are all those CME courses on the bottom. Right? So so, basically, you know, the the whole computer, right, down here on the grad curriculum. Right? You learn bits and pieces of everything, on on how computers work.

So, specifically, in this class in one four seven, we're we're basically gonna work in in this area. Right? So so everything we touch in this class, all the concepts, the algorithms, the way the runtime works, the way the hardware works, and how we can kinda co, optimize everything, right, between the software and the hardware. That kinda spans this whole region here. Right?

My minus the assembly and the and the machine code. We're we're not gonna touch assembly, in this class. Right? But we're gonna touch a bit of, you know, the algorithms, operating systems, and a bit of the architecture. So so today, you know, in the half of this class, we're just gonna, give, like, a really, really high level overview refresher on some of the main topics that, hopefully, you you will need to remember.

Okay. So the thing we'll talk about is the concept of a process versus a thread. So who can tell me what a process is? If you took operating system, you probably implemented it before, right, and then context switched it and automatic management associate with it. Right?

So so what's the process? Yeah. Like, one program file? It's a program file. Right?

Yeah. The program counter that we're calling that, that's on a virtual number? Yeah. Right. So all of those stuff encompasses what a what a process is.

Right? So, basically, when you, you know, compile a program, you get, like, a dot out, and then you run that a dot out or any app, really. Itself is is a process. Right? So the process consists of various things.

The code that you write gets compiled into into binary code that the computer could run. You know, that that's where the code part of your binary. You have data. You know, if it's constant data that you write in your code, it gets compiled into the data. Files sometimes are incorporated into it.

It's, like, statically linked, for example. And then you have the registers in the stack. Right? Which if you took 61, you basically use assembly language to manipulate it. Right?

You remember, like, popping things on the stack and everything else like that. Right? So it's all of that is basically bookkeeping to kind of more or less, keep track of the the states of each thread. Right? So so a thread in a process, is basically instructions that you run.

Right? So if you have a single thread, you can only run one instruction at a time. Right? So so the code that you write, right, you have a whole bunch of different assembly code. Right?

So this thread basically just kinda, you know, go from each instruction to the next and process it. Right? Which instruction you process, someone mentioned, is the program counter. Right? So the program counter basically keeps track of what instruction you're you're operating in.

Okay? And then we have the notion of a multithreaded process. Right? So, you you learn key threads in any course? Like, threading, multithreaded key threads?

No? Yes? Which course? Oh, one sixty. Oh, one sixty?

Oh, that's not okay. Okay. I'm surprised. Okay. So so if you took one sixty, compare programming.

Right? You learn the notion of threads. I'm surprised multi threading is a part of the normal curriculum. But but, you know, there's various ways to write code that uses multiple threads and, therefore, you know, use your multiple processors. So you could, you know, use p threads, which is like the Linux API.

You could create threads and port threads. And there's other higher level languages as well. Right? Like, OpenMP is the easiest one that both of you could use. But the whole thing is that, you know, we have multiple threads.

Now you could process, you know, the same program. Right? It's the same code file. But now you can run it in parallel. Right?

So, typically, if you wanna process, you know, different pieces of data, you run the same piece of code, but you just process different pieces of data in a in a multithreaded way. Or if you're writing a game, right, you have one thread that runs the engine, one thread that runs the, you know, logic, and one thread that does graphics or something like that. One thread that handles networking, for example. Right? So so the threads can do different things.

They they would just instead of the same code file, they just operate in different regions of your code. Right? So so the threads will have to do the same thing. So each of these threads, they have different stacks and registers. Right?

Okay. So, hypothetically, now let's say we have multiple threads, but they all share the same register and share the same stack. What would happen in that case? So so why don't we have separate registers and stacks for each thread? Yeah.

So each thread can operate independently with the it's own, like, vertical. That's, like, the original independence, but then they offer share and then, overall, possibly. Yeah. Basically. Right?

So so you you think it back to, like, say 61. Right? All of these instructions are are basically just, operations on registers, essentially. Right? So imagine if all of these threads are running this instruction, but they're supposed to be doing different data.

Right? But then they're all running the same r one, r two, for example. If they all share the same register file, they're basically overriding each other. They're just corrupting everything. So, yeah, we need each thread needs to basically maintain their own state.

Right? So so, you know, when you in in operating systems, right, when you do context switching, there's some state that's that's inherently, required for that program or or in the case of thread to run properly. Right? So so the state is basically held in in the registers in the stack. So that's why every single thread has its own register and stack.

And then we'll see we'll see a similar concept later when we start talking about the GPU architecture as well. Alright. Any questions? Well, okay. So so we know what a process and a thread is.

Right? Yes. Okay. Great. Alright.

So the next concept is virtual memory. You touched virtual memory, I believe, in operating systems and also one six one architecture too. Right? But yeah. I think so.

Okay. Alright. So so the whole notion of a a virtual memory, is basically to give you the illusion that you have a lot more memory than than physically that's available. Right? The the phones that you have, the laptops that you have, you might have, what, 32 gigs of memory.

No. That's a lot. Right? Not on both. On laptops.

On laptop, maybe. 32. Right? Do you have a gaming desktop? What what do you need nowadays?

Five twelve? No. No? Okay. What is it?

$2.56? Does that say a lot? 32. Still? That's it?

Yeah. Oh, wow. Okay. I I haven't done a gaming desktop in a while. Mostly we're servers, so they're, like, seven hundreds of gigs and everything else like that.

So so I'm not used to this. Wow. Still that small. Okay. Wow.

These are very memory efficient still. Okay. Let's say speed of 16. 16? 16 used to be the the baseline, but now 16 is slowly not being.

Interesting. Okay. Yeah. So so, yeah, no matter how many how much memory you have on your physical hardware, right, the the software itself gives you the illusion of of infinite space. So so let's say you write a program.

Let's say it has a memory leak, and it's using, all of a sudden, 400 gigs of memory. Right? Which I'm sure some of you probably did. No? I wrote a piece of code, and then your computer just gets slower and slower, and then it crashes over time.

No? Or we use Algorand. Algorand. Yeah. You would use Algorand to cache that.

But yeah. So sometimes, you know, you have memory leaks. Right? That's why that's why you might have to, like, restart your browser once in a while because it gets slow. So so, anyways, right, you have this virtual memory, and then you have, you know, a smaller physical memory.

Right? So the way you would basically map this to each other, right, is that, you know, we can't fit everything into physical memory, so we kinda have to swap it out back and Right? So if you took operating systems, how do we keep track of this mapping and swapping back and between your your DRAM and your disk, for example? You implemented this in operating systems. Right?

What did you implement? Page tables. Right? Yeah. Page tables.

Right? So so there's a page table that basically keeps a mapping of A a virtual memory address to a physical memory address. Right? If the physical memory gets full, you dig something, you know, using maybe, like, least recently used algorithms, like, page replacement algorithms, and then you can get back to to your disk or or whatever's backing it. Right?

So, you know, there's there's a whole bunch of mappings that that goes on. And and, you know, this only shows one process. Right? So every process has their own virtual memory space. Right?

So so, for example, you know, if there's a dot out. Right? If you have a b dot out, it would have itself its own virtual memory allocation. And, of course, you know, it's all mapped into it as well. Right?

You managed all that in operating systems. You implemented that management. If you haven't, this is the extent that you need to know. Right? At the high level, what a page table is.

And we'll we will, cover it again when we need to, especially when we talk about unified memory and everything. Okay. So so this is essentially how we manage the memory at the operating system level between, you know, what you you see when you do malloc, for example, and what the physical memory is. Any questions? No?

All good? Basic stuff? Are you is your cache being refilled with all these? Yeah. Okay.

Or was it, like, repressed because it was just horrible? Is is OS still, like, the hardest class in the undergrad curriculum? No. It always has certification. Right?

Okay. Alright. So, so besides that, right, we have an ISA or or assembly instructions. If you took 61 no. Wait.

If you took one six one yeah. You you'll be very familiar with this. Right? You remember seeing all these bit fields and then all the zeros and ones and then how they connect in the data path and everything else like that. And then if you took 61, right, you just did a whole bunch of programming assembly using instructions that look like this.

Right? So just as a refresher, what is that assembly instruction doing? What am I adding? So r 18 and placing it at a register inbox. Yeah.

Right. You're adding these two, and then the results go into here. Right? I I never understand why it's not like this. It it makes more logical sense to think that way, but but it's not.

Okay. Right? So so, yeah, they are seventeen and eighteen. Those those are the source registers. And then you're writing the results to r eight, which is the destination register.

Right? So, you know, subtract, multiply, divide, whatever. They all look like this. We have some special instructions like jump and branch, exit, all these other stuff. But, you know, we're we're not going at this low level, but, basically, the notion of, just having an an understanding of the notion of what assembly instructions are and how they flow through the pipeline, will be worth the price.

Right? So so, you know, we we have these programs. You know, you roll it in assembly, you you know, to do various I don't know what your assignments were, to do some manipulation of data, whatever like that. If you took compilers, right, you write code in c c plus plus, the compiler automatically, you know, generates assembly code, you know, that does all of these type of stuff. Right?

So once you have all of these assembly code, it runs on your five stage pipeline. Right? So if you took one six one, you're very familiar with how all of this works. Right? Every single line, you know how it works.

Yeah. Toronto side. Yeah. Alright. So so then the five stage pipeline.

This is, like, the most basic process that you can have. A lot of processes nowadays are still like this. Right? If you have a armed, you know, embedded processor or in order processor, Most of them are still variations of a five stage in order pipeline. In the graduate level class, you learn about in out of order pipelines and how you could make this faster with more, you know, accurate branch predictors and prefetchers and all these other stuff.

But for the purpose of this class and and GPU architecture, it's very similar to to what a five stage pipeline looks like. So so if you could tell me what are the five stages in a five stage pipeline. Yeah. Yeah. It's also listed there, but it's not fun.

So Thank you. I know you can read now, so thanks. Right. So so we have fetch, decode, execute, MAM, and write back. Right?

So so these are the main, five stages. So who can tell me what happens on a fetch stage? What happens on the instruction fetch stage? What do we do? Using the program counter to fetch an instruction.

Yeah. Right? So so we use a program counter to fetch an instruction, hence why it's called fetch stage. Right? So so let's say we have, you know, a whole bunch of code.

Right? We have this program counter that points to, you know, a specific instruction that we're processing. Right? So let's say it's a it's the add instruction, r one, r two, r three. Right?

So we fetch this instruction, and then it goes on to the next stage. So what happens in the in the decode stage? Any guesses? Did you decode the opcode? I decode the opcode?

Not really. This kinda splits up already. But, But, yeah, there there's other things that goes on there. Right? There's, like, literally only one unit in in that default stage.

Is that yeah. So it takes the binary of the instruction, and then it determines that we're what registers need to be read into, and then it also extracts any. Yeah. Yeah. Right.

So it's it kinda splits up instructions, but the more more important thing here is that it it reads the registers. Right? So you have two source registers, r two, r three, and then we read a register file. Right? So so the register file, it basically tell us what what's stored in in r two and r three.

Right? So r two and r three, for example, could be, you know, seven and eight. Right? So then the value seven and eight starts flowing out from this register file. K?

And then we have the execute stage. So what happens in the execute stage? You execute things. Yes. Yeah.

Okay. It's too early to be tired. Okay. Right. So so we execute things.

Right? So so the add would be an add instruction. You know, if it subtract, multiply, whatever, it it's whatever the instructions, the opcode is basically telling the the AOU to do. Right? And or compare and swap, whatever.

So then in this stage, we we do the addition. Right? So seven plus eight is 15, and then it flows into the next stage. So the memory stage would do what? I know it's an add instruction, but in general, you know, what what happens in that in that memory stage?

Why is it called a memory stage? Yeah. It it's so self explanatory. Right? Yeah.

So so now you wanna take C s one six one. Right? It's so easy. Yes. No.

Yeah. You you access memory. So so you access memory. You know, basically, you have load and store instructions that will access memory. It adds it doesn't do anything, so it just kinda bypasses it.

Right? This is a wire that just kinda bypass oh, sorry. This one. It just bypasses it right here. And then we have the right backstage.

So so what happens to write back? Anyone else? It's self explanatory. Yeah. Right.

It's standard. I'm sure it's on the service. Yeah. It writes back to the register. Right?

So you have that destination. Right? So so in this case, you know, the the address R 1 would, loop back, and we will write to R one. Right? So this is so, you know, this is basically your five stage pipeline.

This is c s one six one in, like, five minutes, essentially. Right? It's it's a lot more nuanced than this, but yeah. So so at a high level now, right, this is how all processes work. Right?

All the code that you write after it gets compiled, it's just a whole bunch of instructions that flows through this over and over and over again doing nothing but some computation, moving data around, and so on. And then you get your computer. Right? Alright. So so besides the five stage pipeline, right, there's other components in in the register file.

I mean, in, in a CPU as well. Right? It's not just the data path, right, that does the processing, but we also have other components as well. So we have the register file that that holds the the operands that we're operating on. We have caches.

Right? We have a separate instruction cache and a data cache. And depending on your processor, other components in here as well. Right? Accelerators and so on.

So, you know, that comprises the core. If you just copy and paste that multiple times, then that's your multi core processor. Right? So so you have, multiple threads that every thread just maps to one of these cores. Right?

If you have, you heard of, like, Intel hyperthreading, right, when if you buy a processor. So they call it hyperthreading AMD. I don't know what they call it. SMT or something like that. Right?

So so those type of processors, you can run two threads at a time in in in your core. So there's just some hardware that basically sounds, like, really fine grained context switching, essentially. Right? But but more or less, right, this is how this is how a multiple processor looks like. Right?

A thread and apps to a single core, more or less. Okay? So besides the core, you you also have caches. Lots lots of caches. Right?

You have l two cache and l three cache. Subprocessors might even have an l four cache. Right? So so why do we have a whole bunch of caches? Why do why do we have caches in general?

Yeah. I mean, there's, like, this is some information, you know, you'll access again. You can get it, like, quicker, and then it's like a hierarchy. Yeah. If you keep this from the one, you go to the cycle, but it takes longer, but you try to make sure that you're optimizing, like, where you're putting the closest cash, I guess.

Yeah. Yeah. Yeah. Right? So so a lot of the code that you write is a lot of reuse.

Right? You don't just use the data once. Right? You make use of it multiple times where you write to it and read and write and update and so on. Right?

So so, typically, you know, if you don't have any of these caches, right, every time I do a notice store instruction, right, I'm gonna be going all the way to my memory to fetch it. Right? I operate on it, and then I store it, and it goes all the way to memory. Right? So so this could take, you know, thousands of cycles, right, or even tens of thousands of cycles depending on how slow it is.

Right? Mainly because, you know, if you think of it, when you build a computer, where's your memory in relation to your CPU? Like, on the motherboard. Right? You you have those DIN cards that you put in to the slots.

Right? It goes through all those copper wires on your motherboard. Right? And then it goes into your CPU. Right?

That's that's really far away. It's really slow. Right? Copper copper wires, all these EDR buses and everything. They're they're super slow and super narrow.

So so that's you know, it takes a long time to to load and store your your instructions. Right? And, also, you know, this is in order. Right? So if there's a load instruction here and I'm installing for a thousand cycles, you know, everything backs up.

Right? The whole pipeline stalls. I can't do anything. So so, you know, if you wanna improve our performance and throughput, right, we wanna make sure that the data that I want is is close to where I am. Right?

So we have all these layers of caches where, you know, if I access something, you know, it'll be cache in these levels of caches. So if I need something, hopefully, it's my l one, which could be, like, you know, like, five cycles instead, hypothetically. Right? So so waiting five cycles is much faster than, you know, thousands of cycles. Right?

And then if I miss an l one, you know, my l two may be, you know, in order of, like, 50, you know, cycles, and this may be, like, you know, a 100. It's it's about an order of magnitude every time or something like that. Right? So so there's a lot of research being done on, you know, how you could improve the caching to make it better. If you ever look at a at a at a dye picture of a of a processor, right, you'll notice that it's like, all of this is cache, and then your your CPU core is, like, just this tiny part here.

Right? Most of your chip is is actually just a cache nowadays just because the the memory move base is the main bottleneck. Right? So so this is the more or less, you know, the modern memory hierarchy. Right?

Like, every time you access a different level of cache, it's about an order language. It's slower. And then if it doesn't fit in your cache, it goes to main memory. And then if it's not in your main memory, you start swapping out to to disk. Right?

So that that's what the swap does. Right? It goes to your to your disk. So in a sense, this is your your main memory hierarchy and then your whole computer system. Right.

Any questions? No. Okay. Great. So, hopefully, that's all refreshed in your head.

So now we could go to the GPU side of things. Oh, it's lagging. Okay. Alright. So intro to CUDA c.

Right? So CUDA is the the programming language from NVIDIA, for you to program your your GPUs. Right? If you have other GPUs like AMD, right, the language is called HIP. Everything that we talked about in class translate on a one on almost one to one to to a and d HIP.

Right? It's it's very, it's a very similar programming model. Even OpenCL as well. Right? So the main difference between a CPU and a GPU, if you wanna think about it, is that CPUs are optimized for latency.

Basically, there's a lot of things that goes into the core to to make that, you know, one instruction, one thread run as fast as possible. Right? So so, you know, you learn about caches, in in one six one, right, to to make the memory access faster so we don't install the pipeline. In the pipeline, you can make this faster with, you know, forwarding paths and stuff like that. Right?

In c s two zero three at the graduate level, we can learn about other stuff like out of order execution, branch prediction, and other more advanced techniques. So so there's a lot of stuff that goes, you know, into this control part of the chip to to make that single branch faster. Right? GPUs, on the other hand, their goal is not to run things as fast as possible. Their goal is to just do as much work as possible.

Right? So rather than optimize for latency, you know, make one one thread as fast as possible, their goal is to optimize for throughput. So how many threads can I process at the same time as much as possible? Right? So so you could kinda think of it like, you know, if I have, you know, a lot of cargo on the other side of the room, right, I could just run really fast back and Right?

I can do it faster by running faster, or I could just carry the whole box over. Right? If I'm strong enough. You can take over that way. Right?

So so, you know, that that's that's basically, like, the the whole notion behind, the CPU and the GPU. Right? So so we essentially dedicate a lot more threads I mean, hardware space to the SIMD units. These are basically the execution units. Right?

So if you look at a GPU, most of the core is actually, ALUs. It's actually very little cache. Your register file is actually larger than your your cache in a GPU. Right? You may be thinking, hey.

But accessing memory is slow. Right? Why why do I not need caches? So so the main philosophy behind GPUs is that if any one of my threads are basically stalled waiting on accessing memory, that's fine. I don't care if that thread stalls.

Right? On the CPU, you know, that that's horrible. My whole processor stalled. On the GPU, if that thread stalls, I have thousands of other threads that I could execute. Right?

So so, you know, on on a GPU, you know, you you may be able to support, like, 20,000 threads hypothetically. Right? But the code that you write will have more than that. So even if any of those threads stalls, right, the other threads have to contact switch in and and process while while the other threads are waiting on memory. Right?

So it's a very different philosophy. So because of that, we could get away with, you know, dedicating more more of a chip space for ALUs instead of caches. And then we also rely on just simple in order course. Right? So that also makes the the ALUs more space efficient, essentially.

Right? So so we need powerful ALUs and CPUs to make things fast. Right? And and once in 01/28, right, yeah, I think you guys did, like, fast multipliers and all these other stuff that you implemented. Right?

So you can actually make even more powerful AOUs beyond what you guys learned in class. Right? You you could trade off area space. Right? It's just gonna be faster.

And then you you have your large caches because you don't want that single threat to stall. Right? And then a whole bunch of sophisticated control for branch prediction, data forwarding, and all this other stuff. Now nowadays, right, your your batch predictors, especially in the new AMD, CPUs, they're actually neural network based. That's pretty interesting.

Right? So it doesn't have to have neural batch predictors nowadays. So so there's a lot of machine learning now on your processors. Right? So the CPU g fuse, right, looks completely different.

Most of it is for your course. Right? So I don't need simple you know, I don't need a lot of caches because I can just context switch threads now to to tolerate any solve. Laggy. I don't need any fancy data forwarding or branch prediction, so my control is very simple.

We'll see later on how how this also improve things as well with the stuff. And then my AI used to be very energy efficient. Right? It doesn't have to be fast. It could just be very simple.

Like, the stuff you built in 01/28 would be perfectly fine. Right? It's slow, but I make up with, you know, for that with just a whole bunch of them. Right? If you ever looked at NVIDIA's marketing, you heard of the notion of a CUDA tour.

Right? They're like, well, we have a 100,000 CUDA cores or something like that. Right? It sounds very impressive, but a CUDA core is just an ALU. Right?

It's not equivalent to a CPU core. Right? So we'll talk about what's more equivalent to that. So right? That's just marketing.

Right? NVIDIA is really good at marketing. Right? Right? So so all of these AIUs are are like like NVIDIA likes to call it.

Right? Are are basically what's what's allowing us to have multiple threads. Right? We just have a whole bunch of them. So then, you know, the the flip side of things is that now we just have to have a whole bunch of threads.

That means we need to store a whole bunch of states. Right? Every thread needs to register file. So now my register file is actually gigantic. Typically, like, almost ten ten times the size of a cache, for example.

Right? So you would think, great. GPUs are fast because I could do a whole bunch of things at once. Right? So that means I could throw all of my total GPUs and that'd be faster.

Right? But that's not necessarily true. So your your code actually have a whole bunch of, you know, sequential parts and and parallel parts. The code that you write sometimes just don't work well on on a GPU. Right?

So for example, things that have a lot of data dependencies or are very irregular, like like graph processing, right, where you you're constantly jumping to different nodes, and those nodes are, like, have different memory addresses, very irregular workloads. Those perform very poorly on GPUs. They actually run faster on GPU CPUs. Right? So so any code that that, you know, are sequential, right, you you typically wanna run them on a CPU.

If there are patterns in your computation that are inherently very parallel, then those map very well to GPUs. Right? So in our class, we'll look at various algorithms and problems that map really well to GPUs and how you can implement those on on the GPUs to make it, you know, very efficient. Right? So so when you're running sequential code, typically, it's 10 times faster than a than a than a GPU.

Right? So so don't be surprised if the code that you write is 10 times slower than a CPU sometimes. Right? It just happens. It just doesn't matter very well to to that GPU.

Right? Now on the other hand, right, the the stuff that you run on the GPU could be much faster than the CPU. 10 times or even a 100 times nowadays just because GPUs could be, like, huge. It it's so big. It can melt power cables now.

Right? Alright. So so, you know, one one thing I have to say often in this class is that, you know, you can write parallel code. You know, parallelizing code is easy, but making that code fast, that's the hard part. So we're gonna look at different techniques on how you can make that parallel code faster.

And and that's really the whole key point of this class. Right? Any questions? No? Okay.

Alright. So so we're gonna learn how to essentially code and program, GPUs, essentially. Basically, there there are various ways to program your GPUs. You could program things using, you know, various layers. Right?

So you could use application frameworks, libraries, compilers, or you could use any program language itself, right, which is what we're doing in this class. So just to give you an idea of how you could program GPUs. You know, most of you probably ran machine learning stuff before. Right? Followed this tutorial on running a neural network maybe in TorchForge.

I don't know. Whatever. So so all of those are are high level languages, like, Terra's types of TensorFlow, PyTorch, even MATLAB for EE students. Right? All of those run could run on GPUs natively now.

Right? So you could literally just code in nothing but PyTorch for MATLAB, enable some flag, and then, you know, it uses a GPU. And all those code gets offloaded to your GPUs magically. Right? So so you can use GPUs in that way without even knowing anything about how the hardware works.

Right? A bit lower down, right, you have libraries. So so a lot of you guys build applications by just including a whole bunch of libraries, calling APIs, and then just stitching them together to make it work. Right? If you most of you use Python, I assume.

Right? So you have stuff like NumPy and Pandas and all these other stuff. So there are there are GPU accelerated versions of those as well. Right? So so you could take your Python code that already used Numba or Pandas or other, Scikit learn and other stuff like that.

OpenCV as well. Right? You could just change your, you know, import to use the GPU version, and then all of a sudden your Python code is gonna use GPUs magically. Okay. So so you could just use libraries that makes use of GPUs, and and basically use your GPUs.

So there's a whole bunch of libraries. You know, linear algebra stuff like cuBLAS, magma, numerical analysis, math stuff for statistics. There's something called, which just generates random numbers. I don't know why you need to do that really quickly, but, apparently, there's a need for it. And then, you know, you know, in in C plus plus, you you have STD algorithms.

Right? We have for us and c plus plus standards nowadays allow you to have parallel algorithms, natively that that use the GPUs now. Right? It's built into the c plus plus standard now. And, you know, the the main reason for GPUs in the place, you know, image and video editing, they're just tremendous and libraries for that as well.

So so the algorithms that we're implementing class, essentially, is, like, us kinda building a library, right, for for GPUs. So we're gonna build matrix multiply specifically, which is like a mini version of cuBLAS. We're gonna implement reduction, histogram. I think all of those are are function of plus. Right?

So so we'll have experience in in how these libraries are made, not just using it, but basically get an idea of how these libraries are made. Right? So then you can also do compilers. Right? Compilers are like magic.

They could take your regular c code, put some pragma in front of it. Like, in this case, it's open ACC, or open empty nowadays. You do pragma ACC parallel loop. There's some copy and copy out. We'll talk about what those means.

We're basically telling the GPU what data to move to the GPU, what data to move back. And then the compiler will recognize that, oh, this is a very parallel friendly output. There's a lot of independent work. There's no loop care dependencies. Right?

Nothing in I equals to one is dependent on the result I equal to zero, for example. So I could just split this up magically, and the compiler could figure out this this type of pattern and automatically generate parallel GPU code. Right? So so this is, you know, another low overhead way of, you know, changing your CPU projects to a GPU project by just adding pragmas everywhere, essentially. And then and it helped the compiler figure it out.

Hence, there's a lot of interest in compilers nowadays. Right? There's a lot of money if you wanna be a compiler engineer. Very niche. Alright?

But for the purpose of this class, at least the half of of class, it is the the programming language, of things. Right? So how do you as a programmer write these low level codes, low level still still in c. Right? C plus plus CUDA, that best make use of the GPU.

Right? So all of these libraries are programming in CUDA, essentially. And we'll see how how we can focus on that, in this class. There's a lot of ways to write code in in your GPU. Fortran, which is still used a lot in the high performance computing space.

CUDA c, c plus plus is another standard that NVIDIA officially supports. A modern c plus plus nowadays, like I mentioned, we'll do a lecture on this later on, natively supports GPUs nowadays as well. Right? You don't need any compilers. Just a standard c g GDCC, g plus plus can actually compile code for your GPU.

Well, in the c plus plus standard. Python hasn't officially been supported historically. Right? So so in the older days, when I started teaching this class, right, the only thing we have for Python was something called PyCUDA. It was it was a Python code, and then in, like, triple double quotes, you just write CUDA c.

Right? And it is just a wrapper around CUDA c. And then about five or six years ago, a project came out. You can write things in a very Pythonic way, and then there's a just in time compiler that automatically compiles that for CUDA. Right?

So so that still relies on, like, compiler magic. And the API is is very similar to, to CUDA. So so the assignments that we do in class, you can write it a very methodical way of number. Almost there's almost a one to one mapping between between all of the APIs. And then just recently, like, last week, we're we're in the yeah.

Last week, I guess, when GTC was, in in CUDA 12.8, they released something called CUDA Python. So so CUDA officially supports Python as a language now as of last week. Right? So if if you wanna play with it, that might be something that you can play with for your final project. Right?

See what the limitations are and what you can do a bit. Most of the time, it's still kinda experimental code. It you know, it's very there's a lot of limitations on what you could do. I know, like, when when Numba came out, one of the final projects, the students were trying to run code on Numba. And it was, like, just so horribly broken.

It was so slow. Right? And then every year, a different group would try Numba. And, like, over time, it was kinda cool because I kinda saw Numbot got better, and then all of a sudden, students don't have issues, and then it got faster. And now Numbot works, like, almost natively, right, for programming CUDA in Python.

So yeah. So CUDA Python is another official project map that just started that just got released from NVIDIA last week. I have no idea how it works. I I don't know anything about it. This documentation online, and that's about it.

Like, if you could go for it, there's nothing out there. This chat GPT proof. If you're waiting. So if you want a challenge for your final project, you can try to go to Python. I think any GPU works.

This is the software runtime support. Right? And then other languages as well. I don't know what f sharp is, but this this slide came from NVIDIA. I never heard of f sharp before.

So okay. Still good? Alright. Okay. Right.

So how we program GPUs. Right? So we're gonna start diving into, you know, how GPU programs work, and specifically looking at memory allocation and data movement APIs Right? So most of the GPUs you have nowadays are are discrete GPUs. Right?

So, you know, when you when you're building a desktop computer, right, your your GPU sits on the motherboard on, what's it, PCIe slot four, five? What is it? I think it's five. Five now. Right?

Yeah. Right. So if you say five so then your your GPU has its own separate memory, right, on the on the on the GPU card. Right? And then your CPU has its own separate memory.

Right? So so there's something that we'll have to do essentially. Right? If you kinda wanna visualize it, right, you have your CPU. You have your DRAM.

You're connected to your GPU over PCIe. Right? And then your your GPU also has memory as well. GPU memory is, GDDR six. Hi?

Six? I don't know. I don't know what standard they have now. Maybe six. Yes.

I see some. Okay. Right. They they have a slightly different memory technology. It's still very similar to to your, CPU DVR.

It's just higher bandwidth. Right? So so, essentially, this is how your your memory looks like, right, between your your systems. So so, you know, if you have a a c program and you're writing code as a bit of refresher. Right?

How do you allocate memory in in your c code? Yeah. Like, memalloc or something? Yeah. Metlock.

Yeah. In c plus plus, how do you allocate memory with c plus plus? New. New. New.

Right? Right. So so you allocate memory with stuff like, you know, malloc or new. Can you do that in Java? No.

No. Right? It's kinda like a virtual machine that gets managed. Right? And in Python I don't know if you have allocation in Python.

Not really. You know? Yeah. Right? Yeah.

So, you know, CCS looks like it's more than a low level code. Right? So malloc and deal would essentially kind of create this memory allocation. Right? So, essentially, you know, I allocate some memory on my CPU, and then, you know, I I do my processing.

Right? Whatever my variables are, it it, you know, it gets kinda processed in in this regional memory that I allocate it. Okay? So so if I write a a a CUDA program, right, I'm gonna be running a whole bunch of code on my GPU itself. Oh, no.

What happened? One second. Yeah. What happened? Nope.

That's not it. They crashed. Nope. There it goes. Right?

So so you're gonna be running a whole bunch of code on on your GPU itself. Wow. It does not like me touching that section. I can't draw on that section. Okay.

Right. So I'm gonna have to essentially move the data that I want to process onto the GPU so that when I write my CUDA code, it could process on the GPU, you know, on my GPU memory. Right? The reason why I have to do this is because my my GPU is a discrete GPU. It has its own separate processor.

It has its own separate memory. Right? If you have a integrated GPU, right, you have a GPU that's on the same die as your CPU. Right? For example, with with AMD.

Right? We have something called APU. They share the same physical memory, so you don't have to physically move data back and Right? But for the case of most NVIDIA devices, right, we have to physically move the data and allocate the data back and in order for us to process it. That that's kinda like what you saw with with this compiler fragment here.

Right? We had to tell the compiler what data I'm moving back and between the CPU and GPU. So how do we do that in in CUDA, essentially? Right? Alright.

So we're gonna use a simple example in this class, vector add. So so, you know, vector add algorithm just takes two vectors. In this case, we have a, a and b. Right? And then we wanna add every element to each other.

You know? So a of zero plus b of zero equals c of zero. You know? Or one, two, three, and so on and so on. Right?

So we're gonna use this as a running example to kinda introduce, you know, CUDA, and and how we could program it and and, you know, just how threading and and and indexing works. Right? So if you were to implement this ad with, in c or c plus plus, how would you write a code that performs this? Anyone? Yeah.

Just a four loop. Four loops. Yeah. Yeah. Right.

For every element in a, you know, whatever the size is. Right? I will have it iterating of I equals zero one two three four. It's just a of I plus b of I plus c of I. I mean, the other way around.

Right? It it looks something like this. Right? You would have a for loop, as long as I'm less than n, which is the size of my array. It's just a of I plus b of I equals c of I.

K. So so you notice that there's some, annotations. Right? Some some some methods here. Right?

So everything here has a h underscore. This is just telling us that this is a whole set of variable. Right? Right? So right now, when we get started introducing CPU GPU code, right, we're gonna introduce variables that have d of a.

Right? Because because you have a copy of a on your CPU and a copy of a on your GPU. You don't wanna get it confused. So so we call it whole side and device side, h and d. Okay?

So so h of a, b of a, you know, it's on the whole side. The h of c is on the on the whole side as well. But, essentially, this is what your your vector will look look like. Right? You just kinda loop it around it, and then add it, and then you get your final results.

Okay? So for a CUDA program, if I wanna process it on a GPU, right, I'm gonna have to move the data over, process it, and then move the results back. Right? So if you take any GPU program and you squint hard enough, you're gonna see that these three phases no matter what. And just just because that's just how you have a program with discrete device.

Right? So the part one is that you're gonna have to allocate memory for a, b, and c, right, and then move the data over. Right? So now that the data is is on the GPU side. Right?

The part two is then I can actually launch the kernel code that performs the actual vector add operation. Right? I perform the operation. And then at the very last part, I I get the data back. Right?

I I don't have a data memory copy, essentially. Okay. So so this, you know, these three parts, in your assignments, your main file, you're gonna see these these three parts more or less. Okay. So there's various memory spaces in a GPU.

On GPU, the the memory included terminology. They like to call it global memory. So this is just essentially the same as your DRAM on your CPU. Okay? Or your GDDR six, whatever.

So so we call that global memory. So whenever the the host communicates with the GPU and writes data, it goes directly to your global memory. That's that's your main memory of the GPU. So besides that, we also have, you know, your registers. And this is a very simplified view of it.

Okay? And and your and your threads. And a little bit, we'll talk about what the concept of the grid and the block is. But, basically, you know, you have a whole bunch of threads. It's very hard to manage it, so they break it up into hierarchies.

Right? So a grid is a whole kernel. Your grid consists of blocks. It blocks us into threads. It's very simplified.

Later on, we're gonna see that there's also shared memory and constant memory, and then there's a notion of a warp that organized threads and the 32 group feed out threads and so on. So so we're gonna slowly build up on this. Right? But this is a very simplified view of things. Right?

So, essentially, we have the global memory, which every thread could read, and that's what the host writes into. And the register, which are private to your thread. Right? Which and we discussed why why it's private to a thread. Alright.

So so we're gonna have to allocate data into here. Right? So similar to c, we have malloc and free. Right? So so the API is just CUDA malloc and CUDA free.

K. So so that's the API we use to allocate data, into your GPU. Similar to malloc, might be a refresher. Right? I think in malloc, you basically specify a size of the allocation.

Right? So so let's say I have int a of 32. Right? So if I wanna allocate enough space for, 32 integers, what would I put as the size of malloc if you remember? Yeah.

32 times the size of the Yeah. Right. That'd be 32 times the size of the n. Right? So so the malloc takes things in the size of, bytes.

Right? Number of bytes, that that you can allocate. So similarly, include a malloc is also the size of bytes. This is probably the most common, mistake in the assignment because a lot of people, you know, forget to multiply it by four. Right?

It it it is 32 bits, so it's it's four bytes. Right? Yeah. Is the code literally just, like, for functions, you add CUDA in front of the Oh, do you just add CUDA in front of it? Yeah.

No. Not really. Like, for the functions? No? No.

Probably. No. But but if you go from CUDA to AMD, you just replace CUDA with HIP. With what HIP? Yeah.

Yeah. They literally copied the API. Yeah. Okay. Right.

So so there are there are some some things are CUDA plus some c API, but not all c APIs have a CUDA equivalent. Like, you can't just this cost, for example, stuff like that. Alright. So so could have just takes a bite. Right?

Like I was mentioning, this is the most common mistake. A lot of students will put 32 instead of 32 times four. Right? And then they're like, oh, my code stops working after you know, if I'm doing a vector add of ten twenty four. Right?

Oh, I only get the the correct results for the 256 elements. That's that's because you're allocating one four for the space that you needed. Right? So so keep this in mind. Right?

This is, in terms of bytes. So it'd be, like, 32 times size of bits. Right? If you do size of, size of int. Right?

That's that's a function that you need to call. It will return for it. It just it tells you how many bytes that the instruction is. Okay? And then, you know, you have two to three as well.

Yeah. Like, three is optional when you guys are in school. Right? But in industry, you should. I I never use three in my class.

I'm using control c and stop the program. Right? Okay. Any other questions? Alright.

So that that's how you allocate. Okay. So then to move the data over, there's an API type CUDA mem copy. It's pretty self descriptive. It copies memory.

So so CUDA mem copy essentially copies a memory from your CPU to GPU or GPU to CPU. You know, we we pass two pointers to it. Right? A pointer on the GPU and on CPU side and then a direction, essentially, and then the the size and bytes as well. And then the runtime will automatically transfer this.

After midterms, we'll we'll talk about, you know, how could a mem copy works and how you can make that memory copy faster and how you could tolerate that as well. But for now, you know, this is just how you you copy memory back and Alright. So so how that looks like in terms of your CUDA code, is that you need to you know, set up the space on the GPU and move the data over so so we could process it for vector app. Right? So so, essentially, you know, in your vector app code, you know, you're you're pressing a, b, and c from the host.

You know? This is already preallocated before with malloc, for example. And then you you allocate a, b, and c on your GPU side. Right? So so then you get the address, a, b, and c that's that's returned on the on the GPU side and the and the size.

Right? And then you do a CUDA mem copy as well. So the CUDA mem copy takes two pointers, you know, a device pointer and a host pointer, the size, and then the direction. So this is going from host to device, from CPU to GPU. And then when you get the result back, it's just device to host.

And you notice that this is flipped. Right? So so there's a there's a source and a destination, essentially. I think I think there's a source and destination. Yeah.

Okay. And then could have three at the end. Yeah. Any questions with this? Yes.

Oh, why do we also need, like, the type slash direction parameter? Would it, like, would it, like, the destination and source, like, kind of, like, take care of that? Oh, why do we have to tell it the the direction? Yeah. Like, because if, like, like, if we have a destination in source, it's really not, like, which way it's going.

Okay. So so in in this way, in early CUDA, there's not really a way to keep track of it. Later on in class, we'll learn about unified memory, and this is not gonna be done, like, automatically. Right? I'm teaching you the low level stuff before I teach you the easy stuff because then you'll be like, why am I learning this?

I really know the easy way. So so this is the low level stuff Right? So, yeah, we started we have to pull it a direction. Actually, that's a good point, actually. So so for these HFC and whatnot stuff.

Right? I got that from malloc. Yeah. What type of memory address is that? Is that is that a physical address or a virtual address?

Virtual. It's a virtual address. Right? So so this is a virtual address. So it could be address, you know, a thousand, for example.

Okay. Now looking at this KudaMalloc, right, I I get an address back after coming KudaMalloc. What is what type of address is that? That's it's a physical GPU address. Which, coincidentally, could also be a thousand, hypothetically.

Right? So that I'm trying to include a mem copy. I just get two points of a thousand and a thousand. I don't know what is what. Right?

I don't know the address space. Right? You have some aliasing issue with with the addressing. So that that's another reason why we need to do that. With unified memory, instead of calling malloc, there's something called a CUDA manage malloc.

And then the page table is basically managed by the GPU. So then the GPU could figure out which direction automatically. Right? But but at the lowest level, there's no notion of that. Right.

So, yeah, so this is a this is a physical address, on the GPU side. And we're gonna learn why that could be an issue later on. 12? Okay. Alright.

So in your code, right, all of these CUDA APIs could spit out some errors. In theory, your your code should catch these errors. Right? Yeah. In practice, you could not have this and just cross your fingers and hope everything works.

Right? But once in a while, this actually comes in handy because especially with the assignment. I noticed sometimes the GPU crashes because maybe someone allocates all the memory, and then someone else's program comes in, and they could have alloc fails that says out of memory. Right? And I can't figure out what's going on, and it turns out the process is stuck in an infinite loop.

So we have to contact, ENGR to, like, go in and kill that process. Right? So so if anything weird happens, try to catch all the errors and see what that is. If you if you're, like, a 100% sure, it's not my code. Sometimes it's true.

It's not you. It's it's your classmate who who took the server down. Right? It happens quite often. Yeah.

Okay. But just make sure it's not you Right? Okay. So so, you know, I believe in the scheduled code that we have, a lot of these, CUDA calls already wrapped up in in this error checking. So so there's a, you know, CUDA error type that essentially catches these errors and and and tell you what the what the issue is.

Right. Normally, you don't see any issues unless, you know, something weird goes on with the server. Right. So now let me know how we can move data back and between servers. Sorry.

Between the CPU and GPU. How do we actually tell the CPU to actually do the vector add itself? Right? So how do we write the code to actually do the computation? Alright.

So so how do we do this without using any for loops? I mean, yeses. While loop? A while loop? That's still a for loop.

It's in a different form. Okay. Without any loops, not just for loops. You could use a collision instead. So the connection is still.

Okay. Alright. So so we have multiple threads now. Right? Yeah.

We can assign deep thread to pair from a m d, and then each thread in computer, and then we. Kinda like that. Right? Yeah. Right?

So so the whole, you know, mindset now is, like, if you're trying to solve a problem, you know, the the traditional way of serial thinking is I'm gonna loop over every single element and perform an operation. Right? Now if all of a sudden I have, like, thousands of threads, right, the mindset is gonna be, how do I map a thread to a different piece of data in the process? Right? So so GPU specifically, they exploit a form of parallelism called data parallelism where you split up each data into a different thread.

Right? So every thread process a different piece of data. That that's why it worked really well for machine learning workloads, like matrix multiply. So so the mindset here is how do I map a thread to an individual piece of data and index. Right?

And you'll see later on that a lot of the code that we write is really just indexing. For some of the more advanced algorithms, like, the line of code don't change. You just change the indexing, and all of a sudden, the behavior of the code behaves in a very different way that's more, you know, hardware efficient. Right? So so you can't measure productivity in lines of codebook with GPUs.

You you could just basically stare at it until you figure out what the indexing is. Alright. So so we're gonna essentially, you know, do indexing to map to map the threads to your data. Okay? So so the way we we launch a kernel, and specify how many threads we have.

Right? We essentially call functions like this. So, you know, kernel a and then triple angle brackets. You specify the number of blocks and number of threads. Right?

And then your normal arguments that you pass into your function. Right? So this kinda introduces the concept of a block. You know, what is a block. Right?

So, you know, you can manage your, you know, threads individually. Right? Like, for example, in p threads. Right? Every thread you fork, and then you when you fork it, you say to run this function.

Right? It doesn't scale when you have thousands of threads. It it's a it's a pain in the bleed to do that. Right? So instead, what we do is that we essentially organize these threads into the notion of blocks.

Right? So so we have, you know, a kernel that is broken up into blocks. So every every block essentially is uniform in size. So every block, for example, you know, block zero, one, two, dot dot dot dot dot can have, you know, 512 threads, for example. Right?

So then every thread in a block has some kind of index. Right? You have a thread ID in a block. You have a block ID. Based on that, you do some indexing to map it to your data, and then you process it.

Right? So so the the way we specify how many threads you have is by specifying the number of blocks and the number of threads in the block. Right? So, for example, if if num blocks here is four, the number of threads per block is five twelve. How many threads do I do I have in total?

Four. How many? It's four and five. Four what? Four blocks and five twelve.

No. Wait. So no. Wait. Alright.

Oh, I see what you're saying. Okay. Now so so the number of threads is the number of threads per block. More than one of these. Four and five twelve.

So I I have four blocks. Twenty forty eight. Yeah. Twenty forty eight. Right?

Yeah. So you just multiply it together. Right? So that's that's how many threads you have in total. Okay?

So then how do I map it, essentially? Okay. So this is essentially how we do the mapping. So so we use this form, like, I is equal to block ID times sorry. Block ID times block dimension plus thread ID.

And we're gonna see this one over and over again. So I do a a mapping, and then I just call a plus b equals c. Right? So so with this mapping, all of the different, you know, thread IDs and block IDs will map to every single element in in my in my in my array. So so this should kinda walk you through it.

So thread ID, you know, within a thread block tells you the the index of thread is. Right? So so if I have 256 threads in this block, my thread ID goes from, you know, zero to 255. Okay. So if I have another thread block one, my my thread ID also goes from zero to 255.

255. Right? So so that ID is basically local to your thread block. It always starts from zero. Okay.

So then you have a block ID, and every block has a different block ID. Right? So I have block ID zero, block ID one. And then we have another variable in in the icon, block dimension. So, basically, the size of your block.

Right? So when we do this, right, we could essentially map your entire array, you know, in into these step blocks. Right? So so when you're when you're parallelizing code on a GPU, kind of the thing you wanna do is how do I partition my problem, right, into into my different blocks. Right.

So you have a plus b equals to c. And the way I partition it, basically, like, let's say if my array is ten twenty four, right, and my my block size is 256. Right? What's a logical way of splitting up my ten twenty four arrays into my blocks? What's a logical way of splitting that problem up?

Yeah. Each ray might get a integrated block. Yeah. Right? So it's each it's the other way around.

Each block is a contiguous array. Yeah. Yeah. Right? So so we could partition it so that, you know, this is block zero, one, two, and three.

We'll perform index zero to two fifty five. Right? Index two fifty six to five eleven will be this block and so on and so on. Right? So, basically, you want every single one of these threads to have a one to one mapping to every single one of your index arrays.

Right? The the the arrays give the index. Right? So it's that formula block ID says block dimension, but it's still the eight essentially gives you that. Right?

So when when block is zero, this is all zero. So your index I here will be zero to two fifty five. Right? If your block ID is one, everything is just offset by whatever that block dimension is. Right?

Two fifty six. Right? So so everything is offset by two fifty six, so you get two fifty six to five five eleven. And And then I block IDs too. It offsets by five twelve, you know, which gives you this range and so on and so on.

Right? So that that's that's what that, index against. Right? And we're gonna see that form over and over again. It's so common, in Lambda in Python, that's just an API call nowadays.

Right? I think it's just, like, grid ID or something like that type of thing or global ID or something. Right? So so, essentially, every every thread here has an index that it indexes into. And this is just kinda what I drew out.

Right? So what you notice now is that if I if I do my index incorrectly, I essentially write the same line of code. Right? The same two line of code. All the threads will run it.

It will map to a different piece of data, and then I I'm able to perform a parallel map to add, you know, without any for loops. Right? The magic is just in how I do the indexing. Alright. So we're gonna next week, right, we're gonna learn how to do this in two d, which is your assignment and so on, and also more complex things, right, later on on on how we could make this work.

Right? So so your block ID and thread ID could also be two d or or three d, and then you can mix it as well. Right? So just depending on on the problem that you're working on. Like, let's say you're doing molecular dynamics.

Right? In Hailey, that problem space is three-dimensional. Right? So you can represent your blocks as three dimensions, three dimension IDs. Right?

You know, and then you partition it that way. Right? So so, CUDA gives you the flexibility to to do your indexing, you know, in one one or two or three d, just however it maps well to your problem space, essentially. Right? Okay.

So so that's the gist of things. The TA tomorrow discussion will go through how you can program it and run it on vendor, right, and then how the assignment works. So so assignment one is is assigned today. It'll be due next Friday. It's super easy.

The whole point of this is just to make sure that you have access to vendor. You could use GitHub classroom, and you know how to run the code on vendor. Right? The TA will basically run through all of this, but we're gonna do, instead of matrix, instead of vector add is a is a matrix addition. Right?

Wait. Not yet. Don't need it. Okay. Oh, I didn't log in.

Dang. Alright. So so the code have you guys used before? Oh, okay then. Okay.

I I don't need to talk about it. Right? It's it it it gives you a refill. You submit it just by by committing, and then that's about it. Right?

There's some questions on the bottom that you have to answer, and you didn't do the meeting. I I saw someone finish their assignment with me. Okay. I didn't even assign it. So yeah.

Alright. So I'll see you.


I should record. Okay. Alright. So so this is the basic vector ad code, that you saw in discussion. We saw this last week also as well.

This was, what we left off left off on. And, basically, you know, instead of having a for loop, the whole notion of data parallel programming. Right? It's it's the state of parallelism that that could exploit. The magic here is how do we essentially map a thread to a piece of data that we're processing, and everyone basically performs the same operation.

Right? So we saw this form, you know, inside, which is the index, is thread ID, and the x dimension, is thread index in the x dimension plus block dimension in the x dimension and block ID in the in the x dimension. Right? So this this form basically gives you, like, a a global index based on the local index of your thread and and your block ID. And then every thread will run if I is less than n, and then a of I plus b of I equals c of I.

Right? That's your basic vector app right there. Okay. So just to, you know, perform you know, let's say what let's say if I remove this if statement. Right?

What do you think could happen potentially if I remove that if statement? Because, technically, this is also vector add. Right? N? And this is the size of your array.

Right. So so, you know, we have vector a, vector b that we're adding. The size of this array is is n, typically. And then we're partitioning this between the different blocks. Right?

Yes. If you choose a, block an account and block size that multiplied to something that isn't even like four of Mhmm. What your array size is. I mean, telling to have that check could potentially cause you to have a memory. Yeah.

Right. You you might actually have a so so on a on a CPU code. Right? If you make an array of size 10 and you access, like, a of 12, what's gonna happen? Right.

You got a single patient fault. Right? The good news is that in GPUs, that's perfectly valid. It's It's not gonna spit any errors. It's gonna work.

This is no operating system on the GPU, so so you don't get those checks. Right? But but your code will be corrupted potentially. Right? So the so there's there's ways to check that, but not natively.

Right? So so on a CPU, if you actually index out of bounds, at runtime, it's gonna crash. Right? On a GPU, it's fine. It's gonna run perfectly fine.

But, you know, what's unintended behavior. Right? So so we had to put these, basically, boundary condition checks, in order to make sure we don't access anything out of bounds. Right? So, specifically, let's say we have, you know, rep loss of 256.

Right? And my n is equal to a thousand. Right? So that means I'm gonna have more threads than I do the number of data that I have to, process. Right?

So so, basically, in this scenario, if I don't have that, if I is less than n, was it 24 of my threads at the, you know, at the far end is gonna process garbage values probably. Right? So so vector add is actually a very nice code to start with because even if you don't include that, it'll be perfectly fine. You're just writing to, like, c of ten twenty four, which doesn't make a list anyways, and we don't copy that back. Right?

But but yeah. So so in this scenario, it's very, very, resilient, but let's let's see later on how other value prediction checks, are more required. Alright. So so now that we, you know, we we have an idea of, you know, this this is a core function that runs on the GPU. Right?

How do we actually partition the data into the different threat blocks that we need to process. Right? And and this is pretty logical. So so, typically, now your hardware consists of a whole bunch of threat blocks. Right?

We organize the threats with a number of threat blocks. So threat blocks could be of various sizes. We'll talk about this later on too. But in this case, hypothetically, let's say each threat block is 256 threats. Right?

So each threat block is 256 threats. So if I have, you know, a thousand elements I need to process, how many reflux would I need to to launch in order to cover all of my data? Have you programmed GPUs before? A little. Yeah.

I could tell. Anyone else? Four. Four? How do you get that?

Yep. It's the data and each for the, like, ten four thread block or, for 124. Yeah. It's like a thousand divided by three fifty six, and then you take the ceiling of it. Right?

Right. So or more generically, just n divided by your side of your thread block ceiling. So so in this case, you know, we we saw this. You know, this is how how we launch a such CUDA kernel. Right?

So inside of these triple angle brackets, we have various parameters that we have to pass, which defines how much parallel we essentially have. Right? So each thread block has 256 threads. Right? So this, thread block size.

And then this is this is the number of thread blocks or or the grid size. Right? So so n divided by two fifty six, ceiling, and that's the number of thread blocks that you have. In this case, you know, if it's if it ends up down, then it ends up being four, for example. Okay?

So this is pretty straightforward. Later on in class, we're gonna introduce more more CUDA features. So then instead of two parameters in there, it's gonna be three or four parameters or even more, that we're gonna use. Okay. Any any questions for this?

No? Okay. Alright. So so besides, you know, just throwing in the number into the grid and block dimension, we could potentially also have, you know, two dimensional and three-dimensional graph blocks as well. Right?

So we see examples in a little bit of of how we partition data into two two dimensions. It's just one dimension, but you just replicate it in the in the y y dimensions. So so CUDA has these in three data structures, that you could define the the three different dimensions. Right? So instead of writing two fifty six, it'd be d m three didn't block two fifty six comma one comma one.

So this is the x y and z dimension. Right? So z dimension is x, y and z dimension is one dimension. That's the. K.

So so this is the the more, generic way of of writing this using, in the. Are you talking to questions? No? Okay. Alright.

So what happens, you know, underneath when we actually call call this. Right? So on the host code, you know, besides your, you know, CUDA malloc and CUDA mem copies and everything else like that, you're also defining the the size of the the parallelism. Right? The the amount of parallelism that you have.

Right? So so you define how many thread blocks you have, the size of each thread block, and then you and then you call the actual kernel itself. Right? So when you when you launch this, vector add kernel off, that that's what physically launches the code onto the GPU. Underneath, we're gonna see what happens in a little bit, you know, I think maybe in, like, three or four weeks.

But there's commands that get sent to the driver, and the driver sends a command to the GPU. And then the GPU essentially just launches these number of threads processing to the function that we we broke. Right? So so all of this is gonna get distributed to the GPU. And then, you know, every thread on the GPU is gonna run, the the the instructions that you wrote here, for that vector.

Right? So we see, stuff like like host and and here. So we'll talk about what those are in a little bit, but they're just basically telling the compiler, you know, where where that code runs, right, and and what to compile it for us. So so every thread in that grid, right, when you launch a kernel, NVIDIA calls it a grid. So a grid consists of blocks.

A block consists of threads. Whereas are grouped into warps. Right? And then recently, NVIDIA, like, two weeks ago, announced there's another hierarchy card cluster. But, yeah, there's a whole hierarchy and stuff.

Right? So so, basically, you know, all of the threads will run that instruction, get their own index, map to their own data to process, and add and so on. Right? So so at a high level, that's that's what's happening. So this is how the the GPU is interfacing.

What we put in post. The protest is not today. Like, it's not Thursday. Oh, you could hang on. There's supposed to be a post check going on.

Okay. So so we we saw before, right, that there's some function declarations. So depending on where you want your code to run, you have to annotate it to tell the compiler what to compile it for. Right? So so, you know, your host function is the one that run on the CPU.

They typically get compiled into x 86. Right? Because that that's what most of your processes are. I'll let you know on that one. That's that's ARM, I think.

Right? Yeah. Okay. Or your phone your phone is simply ARM too. To compile your code to run on GPUs, you have to put either underscore underscore device or global in front of the function.

So the compiler knows to compile that for your GPU. Right? So compile it into PTX, which is, the intermediate language that that CUDA uses. Right. So so you see here that, the function that we have here is called global.

So so a global function is a is a function that runs on the GPU that's called from the host. Right? And then there's a distinction between also a device only function. So this is a function that runs on the GPU, but you can only call it from the GPU. Right?

So these are just, like, functions that you call from functions, right, that you write in your CPU, but but only on the GPU. Right? It's actually valid for you to write code that have, both of these. Right? So you can write a function, like, is even, and then you return, you know, true or false.

Right? That code, technically, the same code could run on a CPU or a GPU. Right? So that type of helper function, you can actually pretend with, device and host, and it and and the compiler automatically compiles it for both, right, for for previous purposes. Right?

So so that's perfectly valid as well. Right? So so, you know, device and host can be used together. By default, you know, everything is host, so you don't really have to put host in front of it. It's optional.

By default, the compiler assumes those are CPU functions. Okay. So so most of the time, you don't see host unless it's hard in both CPU and GPU. Right. So so these are the declarations that we have in CUDA for compiling the code to, you know, CPU or GPU.

Python has something similar as well. We'll we'll talk about this, you know, in a few weeks. Right. So so the way we compile, and you saw this in discussion. Right?

We call NVCC. So NVCC is basically the NVIDIA compiler. When NVCC encounters your CPU code, it actually calls GCC or g plus plus, whatever you you tell it to. Right? There there's a setting to it.

So depending on what the function is to find as, it would either compile it for your CPU code, you know, typically, like, x 86, or for GPUs, we compile it to PTX. Right. So, NVIDIA GPUs are a bit unique in that we don't actually compile into a bind into an assembly language. So so, you know, in, in your CPU code, right, the binary that that you compile directly has x 86 assembly instructions. Right?

So so for GPUs, we have something called GTX. It's an intermediate representation. And If you took compilers, right, you probably heard of that before. Compilers, you you compiles with intermediate representation, you do optimizations, and then you you compile it to assembly. So so GPUs do the same thing.

So when you launch your kernel, that's when it gets just some time compiled into the actual hardware itself. Right? So the reason why NVIDIA did this is because, you know, every new generation of GPU, you have a lot of new features. Right? It takes a lot of time for the compiler to support it.

So rather than, you know, half the hardware always constantly evolving and the compiler not be able to catch up, the compiler folks only need to convert into PTX code. And then there's another layer that converts from PTX to assembly. Right? So it kinda decouples the compiler research from from the hardware, from the hardware additions. Right?

And then that way, they're able to, like, innovate a bit faster. It's a bit more modular. So so I think with AMD GPUs, you see something similar as well. A lot of, GPUs and accelerators in general have some form of intermediate representation. Okay.

Questions? No? Still good? Alright. Okay.

So now it's for more examples. Right? So we saw examples of a one dimensional, problem, like, with vector add. So how do we handle code that are multi dimension? So your assignment one is a matrix edition.

It's a two dimensional, array that we're adding. So, hopefully, you know, you guys use two d indexing and everything else like that. In theory, you can use one d indexing by converting it, but it works too. And then, other assignments, like matrix multiply, we'll also use that two d. But but the whole point of today is to just get that that understanding of how can we partition in a problem and how do we do indexing when it's when it's, more more than one that mentions.

Right? So similar to how we partition that vector into reflux. Right? We also wanna partition a problem space. In this case, an an image that's two d, into our red blocks.

Right? So your red blocks you know, if you have a two d problem, typically, you want your your red blocks to also be two d. Right? It's just a more natural mapping of things. Right?

So so, you know, like I said, if you have, like, molecular dynamics, naturally, those are flowing in three d space. It might make sense for you to have a three d block in that scenario. Right? So you're gonna give it as you break your whole three-dimensional space into open the cubes. Right?

It's like Minecraft type of thing. Right? So so with this two d grid, right, we wanna partition this into my 16 by 16 red block. So so this scenario here, how many red blocks would I need to completely process the the image that I have? Four by five?

Four by five. How do you get the four and five? Sixty two divided by sixty two zero one seventy six divided by 62. Yeah. Right?

So it's just let's say that we did one dimension, but I passed the 12 dimensions. Right? So this is 62 columns. This is 76 columns. You know, this is 16 by 16.

Right? You know, 16 by sixteen, sixteen by 16. And you just kinda overlay the whole thing, essentially. Right? So the total number of red flops you have in the x dimension, right, will be 76 divided by 16 ceiling.

And on this dimension, it's 62 divided by 16 ceiling. Right? Is that but a lagging. Right? It's about, like, four.

Four, four by five. Right? So so in total, we need twenty twenty five blocks to cover this problem space. Okay. Any questions of this?

No? Okay. Still following? Still simple? Okay.

Alright. So, in in c c plus plus, right, the memory that we have in two d is actually stored as one d. Right? If you took I mean, I I think most of you dealt with this, right, when you took c s 10, maybe, or something like that, right, when you work with two d arrays. So so, you know, your your main memory naturally is just a linear address space.

Right? You have physical address zero to, you know, a thousand or infinity or whatever like that. Right? So when you have two d array or three d array, it typically kinda get flattened out, into that one dimensional space. So so c c plus, they they're known as a it's a row major format.

So, basically, you go across different rows, and that's how they essentially get, flattened or or serialized. Right? So so even though we have a two d array of four by four, in memory, they're arranged linearly and contiguously in in this manner, like like you see here. Right? So so, typically, you know, if you want to access a matrix here as a bit of refresher right?

You have two indexings into this this entry. Right? So so in in this one here, is this a row or a column? If you wanna access something and see. There's a bit of refresher.

Row. Is that row? Yeah. Right. It's row major.

Right? So so, typically, this is row, and this is column. So, you know, the row is typically zero one two three. And then the column here is, like, zero one two three. Right?

Right. So they wanna access this. This would be m of three 2. Right? So so that's that's how it's arranged.

Okay. So so, you know, most of you know that. So it's a little bit of a recession. So GPS have a weird limitation. So, like, if you were to have this to the array and you were to access it on a CPU, right, you could just directly access with m of something and something.

Right? So so GPUs If you pass this array directly into the GPU and you try to access something like this, you get some kind of weird error because, you can't actually access a two d array in GPU that you transferred over. I don't know why it's a weird limitation. So a lot of the code that you see whenever we deal with the two d or three d array, we always end up using a flattened address into the matrix. Right?

So so what that means is that we have to basically convert all of these indexings into a linear index. So so the generalized form of of converting this, is essentially real terms with plus follow. So, essentially, your index here is now gonna be, like, zero, one, two, three, four, five, six, seven, eight, nine, ten, eleven, and so on and so on. Okay? So so you're gonna see this this form of, linearizing your your two d programming, over and over in a lot of the the GPU codes that you see.

Alright? You you could try accessing this, but the last time I tried, you you get an error. Okay. Any questions? Mhmm.

They're good? Okay. Alright. So we have a simple code here where we have this picture kernel. And sign here.

Okay. So so now that we have row and column, we basically have to map our threads to process a piece of data. Right? So so in this simple scenario, let's say I have a picture. Right?

And every picture is essentially represented by a pixel, hypothetically. Right? So so the goal here is to essentially map one of your threads to a single pixel in order to process it. And the way we do that is that we give each thread a row and a column index. Right?

So this is the same four that we saw before. Right? So so column goes this way. Row goes this way. Right?

So so the x dimension is column, and then the y dimension is row. So your column is essentially, you know, blocking them, like, about 10 block dimension plus ID. The The same format of the song vector app. And then on the y, it's the same format too, but in the y direction. Right?

So that's the what this, direction is. Right? So block them in the y, block them in the s. It's just the the block dimension you report the index in in each of the different dimensions. Okay.

So that's how we do the indexing. We'll see a little bit an example with more complex indexing to see what we could do. And then once we have that, we could process our pixels. Right? So in this case, the simple program basically just double the value of every pixel.

So so we need to do these boundary condition checks in order to make sure, the row and columns within our width and height. Right? Okay. So so in this case, for every pixel that we see, right, we index into the the array using that linear transformation. Right?

So row time surplus column. We double it, and then we write it to to the out out array with the same with the same index. Right? Yeah. So, you know, the GPUs have a bit of, some weird quirks to it.

Right? Yeah. Alright. Any questions? What is the d under the.

Oh, so so the the d convention is that this is a allocation on the device side. So we have an array in, which is the picture in. Yeah. And the right on is picture out. Yeah.

So so that I think that's what that d stands for. Okay. Right. So this is, like, a really simple two d example. Right?

Okay. So we we follow the indexing. Yeah. Okay. Alright.

So now we could try something a little bit more complicated, or not yet. Right? After after we think about how we launch it. Right? So so the way we launch it, again, is is very similar to to how we launched the vector ad.

The only difference now is that we have two dimensions. Right? So so we have a 16 by 16 thread block here. And then before we saw that, you know, your picture has m and the y, n and the x. Right?

So so this is the x I mentioned. So it's it's it's n divided by 16 ceiling. This form is is actually a ceiling function, right, using the properties of the, integer division. Okay. Right?

So so if you take a value, you subtract one divided by 16 plus one. It's it's a ceiling function. Right? Have you seen that before? Yeah?

Okay. Good. Right. You can also call ceiling. You know, you do something to the math library, maybe.

But this is another form of of, just doing a ceiling. So so we see this a lot too as well. Okay? So so, end of it by 16 ceiling and end divided by 16 ceiling. And then that's that's how you define how many red box you have.

Okay. A lot of times, if your code is not working or it only works for, like, part of your problem space, the main issue probably is is the way you're doing, the thread block launching. Right? Maybe you're not defining enough threads, or the thread blocks are defined in a in a way where you're swapping your your x and y dimension by accident. Right?

So those are two, like, very, very common issues, that we see. So so, you know, just just double check to see whether it's a it's an x or y dimension. Right? Yeah. The 16 and either defined.

Right? It is. Any numbers. Yeah. Right.

So that that sixteen and sixteen could be something else. It could be, eight by eight, four by four, thirty two by 32. Anything above 32 by 32 is typically illegal. We'll we'll show why. It's it's a hard way to integrate.

But yeah. In general, you could you could put whatever you want. You notice that it's almost always a power two, and that's because it's just more friendly to the hardware. Once we get a better idea of of what the hardware is doing with these these, threats in a in a little bit. Okay?

Alright. So we'll do another example. This one's a little bit more complicated. It's still an image processing, across two dimensions, but with a little bit of a twist to it because now instead of storing just one pixel value, right, we have to store an image that is, color. Right?

So so in order to represent color, how do you represent color, on on computers? I mean, RGB. Right? Typically. Right?

Yeah. So so to represent it's basically need a RG and d value. Yep. So how much data space do I need to represent in RGB value, typically? Three.

RGB could be how many how many shapes of screen anyhow? 50. Yeah. It's typically two two fifty six. Right?

Yeah. Right. So so images are represented as RGB values. You have other color spaces too, like like c y m something. K?

Wait. No. What's the last one? Yeah. And it's like oh, okay.

That was a guess. And then, like, I see, like, hue saturation something on the HSV. Yeah. Face is is another one. Right?

So so, you know, there's different representations for colors, but RGB is typically the most common one that we see. Right? So so the way we arrange the array, you see, it looks exactly like like that picture that we see. Right? So instead of every element in your array being associated with one of your pixel values, right, now you have, like, continuous groups of three, indexes representing one pixel.

Right? So so, hypothetically, right, if my if my image is, let's say I have an image that's that's 10 by 10. Right? And let's say that's a grayscale image. Right?

How large would my array have to be in order to store that 10 by 10 image? How many how many indexes do I need if if I were to store that 10 by 10 grayscale image? For grayscale? Yeah. For grayscale.

Assuming, like, eight bits per for the Yeah. Assuming eight bits. One byte one byte per pixel. A 100. Right?

Okay. So now if I was storing a a 10 by 10 image in RGB Right. How many you know, what's the size of this array, essentially? 300. Right?

So what's the dimension of this array if it was a 10 by 10 image RGB image? Two d? Two d? I mean, it has two d, but but, well, you know, what's the actual how many rows and columns do I need in in that? 10 rows, 30 columns.

Yeah. I need I need 10 rows and 30 columns. Right? So so it's 10 rows, and then I would need thirty thirty columns. Right.

For RGB. Right? Okay. That make sense? Yes.

Right. So so this so that means, so, you know, if you think back to vector add, matrix addition, the example we saw before. Right? We were mapping a single thread to one piece of data to process. Right?

So this case here, if I were to process an RGB value, how would I approach mapping a thread to the data for for my problem? So how how many pieces of data would each thread have mapped to? Three? Three. Right?

Right. So so I have to index all of my threads, to to an RGMP value. Right? So so, you know, there's probably gonna be some offset on some stride or something like that in order to to get the index. Right?

So so we'll see how we do that in a little bit. You know, that that's just the small tweak to it. Right? Alright. So we're gonna do this breakout conversion.

Right? Scotty. Right? Is that Scotty? Yes.

Yeah. So we're gonna convert Scotty from color image to a gray scale image and, essentially, the way in the. And then what's the image processing? No? Is it an image processing class?

EE? Folks aren't here. Okay. How do you convert RGB to grayscale? Notify by something.

Multiply by some number. I don't know where Yeah. There's some number. Right. I'm sure in color theory to have that number all figured out.

But yeah. But, basically, yeah, there's some weights to it. Right? So the gray scale value is still probably gonna be, you know, up to two fifty six values. Right?

So you essentially kinda do a weighted average type of thing, to to the r g and the p value. And then you get your gray scale image. So, essentially, for every single output, right, every single output pixel, I'm gonna take in three input pixels from from my picture and perform this this operation to it. Right? So so we give r point two one weight, a g point seven one weight, and the blue point o seven weight.

And then at the end of that, we we get the grayscale value. Okay? Alright. So so now, you know, from this, right, you can see that my input array is gonna be three times the size of my output array. Right?

My output array is gonna be 10 by 10, for example. So so then my my thread's gonna map have different mappings to it. Right? So this, the code for it, it's just a small tweak to to the one we saw, a couple of slides back on on the regular, image will be double every value. Right?

So we still have the row and the column. You know, x x is the the column. Y is the row. And then we did a boundary condition check. And then for every single pixel, we need to map to the r g and b value that I need to process.

Right? So when we when we're dealing with indexing, a lot of times, it's just finding a pattern and then writing a index function that that captures that pattern. Right? When you took your SATs, you have series, and you write a function that matches that series. You did that in in high school.

Right? So so that's that's the type of math we're gonna use here. Right? So so, you know, the pixel, right, will handle the you know, three RGB values. Right?

So so what would that index be for that pixel in terms of row column? Zero zero. Zero zero. Right? So so, you know, the so that's my thread zero.

Right? So my thread one will map to the next r g and b value. Right? So so what would that index be for for my next thread? Right.

So, Okay. Do you want me to use x y coordinates or row column coordinates? Row column? Okay. Okay.

Right. So so zero zero with index to zero zero, row column. Okay. Not not x y. Zero three.

Right. So this one will index to zero three is the column. Right? Yeah. It is multiplied by the column.

Yeah. Let let's just do one more just to make sure. It's not exponential. What would this one do? What what what would the row column be here?

Zero six. Okay. Now that looks linear. Alright. So if I am something n right?

What what would this index be if it was, like, thread n? Generalizing it. Assuming my row is still zero. Zero three n? Yeah.

Zero three n, essentially. Right? My row is still that, but my column is three n. Right? And then, you know, anything here is essentially the same the same form.

Right? It'll be essentially row, three, and then you it's essentially a thread ID type of thing. Right? Three times your thread ID type of thing. Right?

So so we see here okay. Remember After we linearize it. Right? We're basically going to, you know, for every thread, we were offset by, like, zero, three, six, nine. That's the one that's aligned so on.

Right? So so we define the channels in this case. The number of channels here is three. Right? RGB in each channels.

So, basically, in this case here, the offset is just your they call this a gray offset, which is just your normal indexing that we saw. Right? So so this is the the one d transformation of your array. Right? So you did a one d transformation of your array times three, and then that's that's your index, essentially.

Right? That's your offset, into into, the RGB value that you need. Okay? So then once I have my my RGB offset, right, I get my r g and b value by just accessing the array of RGB offset plus zero, one, and two. And then I just grab my RGB values, and then I perform that, gray scale conversion operation.

Okay. So so my grayscale image at the output, I have a one to one mapping between my thread and and my my output pixel, essentially. Right? So I don't have to do any fancy, fancy indexing right there. I just use my, you know, the linearized one d array access.

Okay. Any questions with this? So so if you do this from scratch, it's really just it helps to just draw it out and try to find what the indexing is, and, eventually, you find the pattern. And then you just write indexing functions, essentially, to to handle that. Right?

So so instead of doing four loops in c, right, the the mindset of a lot of, CUDA programming specifically, data file and programming in general, right, is how do I find the right indexing to map to map the data, to the thread? Alright. You could probably stare at this a little bit and and then I click. Or paste the code to chat GPT and say explain this. Right?

That that works pretty well too sometime. Okay. And any questions? Yeah. Can you explain what channels is again?

So so so channels, in this case, is the three. And image processing an image has and an image has three channels on a g and p. Right? Okay. Any other questions?

Yeah. Well, like, can you, like, kinda, like, generalize this? Like, any, like, sound like, we're changing, like, an image and, like, generalize that even just, like, video. So, like, each thread is mapped to, like, a block, and it's like, hey. Just change this much at this time to kinda make it, like, a single video.

I don't know if that makes sense, but because, like, great like, making it a great image is the same as, like, updating it in some way. Right? Yeah. So then, like, video processing isn't the same like we have with blocks. Oh oh oh, I see what you're saying.

Or is it, like, more nuance? So a video is a collection of frames. Right? So so you can take a video as a collection of images. Like, the most naive way of storing that is literally you have this image, and they add another dimension time to it.

And then, you know, you have it's kinda like every frame has a whole image. Right? That's the most naive way of of storing an image. But then most images are encoded. Because if you think about it, like, from frame to frame, not a lot of things changes sometimes, like, between frame zero and frame one.

So a lot of times you have, like, a master frame, and then you just have, like like, a dish before the future frame, for example. Right? So then so most video are encoded in that way. Does that does that kinda make sense? Yeah.

I basically I think I, like, I was already assuming, like, there might be, like, a little bit more to consider because, like, yeah, it's like you can save a lot because maybe one pixel doesn't change in between frame. Yeah. Yeah. Yeah. Right?

So so in order to save space, right, there's a lot of video encoding techniques, that tries to do, you know, lossless encoding and those type of things. Right? So, you know, if if you guys wanna do a video processing algorithm for your final project, for example, right, you might have to use OpenCV to read a video file. And then I think OpenCV has to reading a video file. You you know, OpenCV can show, like, frame by frame.

Right. Have have anyone used OpenCV before? Yeah. Right. Some people say, yeah.

Right. So so OpenCV, you have to get an image. Right? You could get a a video file, get image by image, and then you could you could pass it to crew that the processor, for example. Right?

So that that's a common, pipeline that you would implement for for video processing there. Okay. Alright. And then and then Kudos has libraries that work directly on on videos too. They they have video processing primitives and everything else like that.

Okay. Any other questions? Okay. Alright. So so this is basically, you know, a little bit more complicated indexing, right, compared to the the case that we saw saw previously.

Alright. So now a bit more about, the the direct locks and stuff. Right? So, you know, we already had questions like, 16 by 16, is that user defined? Right?

Why do I pick 16 by 16? Can it be something else? You know, in order to kind of answer that, we kinda need to understand what happens with that, you know, underneath, right, in in the hardware. Again, right back to that chakra analogy. Right?

How does it flow into the GPU? Let's see how that how that works. Right? Okay. So so in GPUs, we basically break everything into direct blocks.

So direct blocks is a way of of the hardware essentially do resource allocation. Okay? So so we talked about the notion of a of a SM. Right? So so GPU essentially have a whole bunch of cores that they call it they call it a streaming multiprocessor, or an SM.

Right? Most of the GPUs that you buy as consumers have maybe, like, eight or 16 SMs. But on your laptops, they might have, like, two, maybe four. Not not a large number. Right?

So depending on on your hardware, right, your your GPU have have different numbers of SS. The ones in the data center can have, like, a thousand SS, potentially, or hundreds of SS. However, you know, when I'm writing as a programmer, I shouldn't be thinking of, you know, how many hardware resources I have. So the ways that abstracted to you as a programmer is the notion of thread blocks, essentially. Right?

So so we define thread blocks, and then the hardware will essentially schedule a thread block to an SM. Right? So so this is one of the reasons why we have the notion of a thread block, because then at the at the hardware level, we we do scheduling with it. Right? So so you can think of it like on a CPU, you map a thread to a core.

Right? And then in Linux, you could you could you could pin a thread to a core, right, with some specific command as well and tell it to go to CPU four, or CPU eight or something like that. Right? So so something, in the GPU, like, it is very similar to that. Right?

So instead of mapping a thread to a specific SM, which is the equivalent of the CPU port on the GPU. Right? I map a thread block to an a core or the SM. Right? So so after I I define my thread blocks, the hardware will automatically schedule this to the number of SMs that I have.

Right? So so I could create, you know, like, in the case of, like, let's say, vector add. Right? If I have, you know, a thousand elements plus 256 red blocks. Right?

I have four red blocks in total. Right? But if my hardware only has two SMs, what ends up happening is that the thread blocks gets serialized. Right? I I don't have enough resource to process all four of those SMs at once I mean, thread blocks at once.

So I'm gonna process two at a time, essentially. Right? Mhmm. So so if I'm doing this vector ad, you know, these two thread blocks will will get processed and then the next two thread blocks will get processed afterwards after it gets, it's done. Right?

So so I'm serializing some of the thread blocks, but then I still have some parallelism. Right? With with multiple thread blocks running at the same time. And then if I have, you know, a a larger GPU, that means, like, we run more more than our clocks in parallel. You get more parallel essentially.

K. Does that make sense? Yes. Okay. Alright.

So so this is, how the hardware, you know, handles red blocks. Later on, we'll see how how that gets broken up even more again. Right? So so the streaming multiprocessor, or SM is really what your CPU for is, if you wanna think of it that way. Right?

So block set schedule to SMs, and the SM consists of a whole bunch of different execution units that that does the processing. So for example, you know, different architectures have different configurations of these SMs. So for example, in a Fermi architecture, which is now, like, 15 old or something, it it's still exactly the same right now. Your SM all have, hardware constraints to it. Right?

There's a limit to how many threads it can run at the hardware level and how many blocks that that it could contain. Right? So so SMs, you know, it has a lot of bookkeeping overhead information, right, that keeps track of states. Right? Like like, when you were doing c s 61.

Right? You have you have a stack that kinda keeps track of the state of your program. Right, and a and a register. Right? So, similarly, in a SM, because of hardware limitations and space limitations, there's a limit to the number of, blocks I could handle because of the amount of bookkeeping that I I keep on the hardware.

Right? So so there's different, constraints to SM. So, typically, you may have constraints like SM can only process eight blocks at a time or echo and or. Right? It can only process fifteen thirty six minutes at a time.

Right? So depending on the GPU generation, these numbers can vary. Right? I think the the GPU that we have in vendor has this limitation, and I think the eight is 16 or something like that. Right?

There's a slide I think it has it in a little bit. And then some other generations of GPUs and maybe twenty forty eight threads in 32 blocks, for example. Right? It just depends on the generation of GPUs, and that changes from time to time. Right?

This is also another reason why the compiler kinda is kinda abstracted with with PTX and just type of approach. Okay? But but, you know, the main takeaway from this is that when we run a thread block and hardware SM, there's there's some constraints to it. Okay? So so all of these constraints will essentially determine what size I make my thread block in order to optimize the hardware utilization.

Alright? So so, you know, if if my SMS fifteen thirty six threads, ideally, what I want is to define my thread bucket in a way where I make use of all fifteen thirty six threads. Right? So if I only use, you know, ten twenty four threads, 500 total of my threads are idle, and my and my hardware is very underutilized. Right?

I'm I'm wasting the performance, essentially. Right? So so this means that I can either define, you know, six blocks of 256 threads or three blocks of 512 threads. Right? So both those combinations, give me fifteen thirty six threads I can run on the hardware.

Right? So typically, there's more than one right answer. Right? I don't know. You guys have to be too young.

Have you ever tried running Bitcoin mining on a GPU before? You have. Right? So so one of the thing it does is that it actually profiles and benchmarks the hardware to find the the block size automatically. Right?

So it will run a whole bunch of different block sizes and see which one gives it the most hash per Right? And then it takes that block size. So so so rather than us manually taking it based on our knowledge of the hardware, you can essentially auto tune this parameter, right, if you want. So it's a lot of, a lot of times you you can auto tune it. K.

Or or in libraries, you just write a different function for every different GPU generation. Okay. And so let's let's run through, like, this, example exercise. Right? So so the limitation here is that we have, you know, fifteen thirty six threads per SM and and eight blocks per SM.

Okay? So if we have a two d algorithm, right, we typically wanna keep it a square and a power of two just because it's nice and neat. So that means we're kinda limited to the number of sizes that we could have. Right? So we got eight by eight, sixteen by sixteen, thirty two by 32.

Why can't we have 64 by 64? Yeah. Mhmm. It's too big for it could be larger than fifteen thirty seven. Right.

So 64 by 64 would be, what would that be? Three, six, 78, 6. Or is it four k? Something larger than Yeah. Three.

Yeah. It's lunchtime. I'm still hungry with it. I don't know. It's I still haven't seen chance of this.

Oh, okay. Right. So so, yeah, 64 by 64 is too too large, typically. So, I think the compiler will allow it, but then when you when you run it, you're gonna get a runtime error. Like, the like, the kernel is not gonna launch or something like that.

I don't think the compiler catches that. Alright. So so, yeah, depending on these, different sizes, right, we could kinda see how utilized my hardware will be. Right? So so my block size is eight by eight.

Right. So how many threads do I have in that in that block? 64. So you have 64. Right?

Eight by eight is 64. So that's 16 by 16. How many blocks do I have? Two fifty six. Right.

Two fifty six. And this one would be, wait, ten twenty four. Right? Okay. So so my limitation here is, this.

Right? Fifteen thirty six threads, for SM. Right? Ignoring this for now. Right?

So if I have a drop lock of eight by eight, right, that mean I have 64 threads. So how many drop blocks would I need in order to fill up all fifteen thirty six threads? So how many eight by eight replicas do I need to to fill up all fifteen thirty six threads? How many times does 64 go into fifteen thirty six? Twenty four?

Right? So you you will need, you will need, twenty twenty four blocks in order to maximize your GPU utilization. Right? The maximum number of blocks I can have in the SM is eight. Right?

So, you know, the number of blocks in here would would essentially be, like, max of eight. Oh, sorry. Yeah. Max of eight. No.

Would it be main? Oh, no. Actually, never mind. It would be no more than eight. Right?

So so, physically, what I could run here is eight. So I'm limited to to eight rep loss of eight by eight. Right? How many threads am I actually utilizing? The of the Yeah.

I mean, so so eight threads Okay. Logs of 64 would give me That's cool. Was it? 64 times eight. You have calculators.

Five twelve. Help me. Five twelve. Right. Right.

So so I'm wasting, like, two thirds of my hardware resources, right, if I can finance eight by eight. Eight by eight is perfectly legal. It's functional. It runs. You get the correct results, but it wouldn't be the highest performance, right, that you could you could get.

Right? So for the case of 32 by 32, right, how many of ten twenty four can fit into fifteen thirty six? One. This is right. Oh, dang.

That's not gonna work out. Right? Even though I can fit a, I only have one. So my my limitation here, you know, isn't the hardware's block limitation, but my block size is just too large. Right?

It doesn't have to fit any more than that. So in that case, I have I have ten twenty four threads, processing. Right? So so 1 of my hardware is under utilized in that scenario. Right?

This is a bit exaggerated, but you know? So so looking at 256. Right? How many times can February go into, fifteen thirty six? What for this one?

Five. Yeah. Six. Right? Right.

So six is still less than eight. Right? So I can have all six. So six is perfectly legal. Right?

So I'm happy right here. Right? So so this one gives me the best performance. Right? That that's why we we need to take 60 by sixty minutes in this scenario.

So this is, you know, just based solely on, maximizing the number of threads that are used. Right? But, typically, your performance of a program could be due to many factors. Right? For example, memory access also impacts your performance.

Right? So later on, we'll see that if your workload is very, memory access heavy, right, How will you reuse your caches can actually have more of an impact than than your hardware utilization. So sometimes You know, 32 by 32 may actually be best even though I'm underutilizing this because I'm making better use of my cache. Right? So so this is a, you know, the main solution for every everything.

Right? So so later on, we'll see that there's other considerations we have to do as well. Right? This is kind of why the Bitcoin mining programs, you need to do the auto tuning automatically, right, before they run. There's just a lot of different parameters that you have to account for to find the best performance.

Okay. Questions? Yeah. The modern GPU. No.

So so the modern GPU, Hopper? Right? Is that Hopper architecture now? There's a lot of generations. Okay.

But this, I'll I'll show you in a in a little bit how we could figure this part out. Right? Okay. Right. So this is just, going through everything we just talked about.

Alright. So so what is your actual hardware segmentation? Right? So depending on the GPU that you have, right, any NVIDIA GPU that you guys have, you could run this, NVIDIA SMI even on vendor. And it'll tell you which generation of GPUs you have.

Right? So the the one we have in vendor is a is a Turing processor? So it's a Turing gen generation. Is it g t RTX twenty seventy. Right?

I think the newest one is hopper. Okay. Right. So so the way, NVIDIA kinda keeps track of all the different GPUs and and generations of architecture. It is that included, they encode it as something called compute capability.

Right? So so every NVIDIA GPU has a compute capability version associated with it. So if you go to the websitedeveloper.media.com/cudagpus, you can look up the GPU that you have, and it'll tell you the compute capability. So this is essentially the parameter that you can pass to the compiler to say, hey. Program it towards this architecture generation.

Right? So so here we see that, you know, RCX twenty seventy has compute capability of 7.5. And then when you look at the CUDA documentation, they have support for various versions of compute capabilities. Right? So so there's a lot of depending on the architecture, the hardware parameters are very different.

Right? The the stuff that we talked about. So so this is the, for example, the maximum number of thread blocks on my home GPU device. No. No.

Sorry. Number of grids. So so this number of concurrent programs concurrent currency to run. Right? So so some of it varies from 16 to one twenty eight.

It's not always increasing. It just depends on the generation. You could have, you know, three dimensions of direct blocks. The number of direct blocks you could define is two to 31 minus one. In the past, the number of drip locks that you could have was only, like, 25 k or something.

So it's now, it's, like, significantly more. And the and the old list is much less. But the one specifically that we're interested in, right, is is, 7.5 and maximum number of resident blocks per SSM. So resident blocks is the number of blocks we have, in the SSM. Right?

So in our generation of GPUs on vendor, we could support 16 threat blocks per SM. Previous generations is 32. And, again, you can see that they kinda swap between sixteen and thirty two. I think depending on the hardware constraints and what whatnot, that they make these decisions. K.

The other one is the number of, threads for ASM. So you see that some of it is fifteen thirty six. Some is twenty forty eight. Our GPS specifically is only ten twenty four. Oh, it's a very small one.

Okay. Right? So this might, you know, change your results when you were looking at, looking at this. Right? So so in our GPU on Bender, right, all it does is ten twenty four.

So in this case, I think both of these would be okay. Right? Because they both make make use a lot of threads. Or for the GPU on 10. Right?

For eight by eight, sixteen. Still underutilized. No. It's still underutilized. Right?

Yeah. Yeah. Yeah. That one wouldn't work as well. Right?

And there's all 16 in our GPU. This will be, I think 18. No. Wait. Yeah.

The Oh, no. No. No. That would actually work too. Oh, goodness.

Yeah. Yeah. Yeah. Right? Because yeah.

Actually, that would work too. Oh, that would you it doesn't matter, actually. They all work perfectly fine. If you only consider the the reds, but not the memory aspect. Okay.

So, yeah, this is how you would kinda look it up. Right? So so you have a GPU at home. The performance trends that you see on vendor might not match the performance trends that you see on your own GPU, mainly because of the the hardware's limitation on on on thread blocks and enterprise. Right?

So so you can look at the the CUDA programming guide, and and it tells you all of these different, parameters for that few capabilities. Yeah. Alright. Any other questions? Nope?

Okay. Alright. So so now this is a bit lower level. Do I wanna jump into this? Actually, yeah.

Yeah. I'll introduce it. Right? So so the notion of, I think I mentioned the notion of a warp a few times already. So so that notion of a warp is really gonna be one of the main limiting factors and one of the biggest programming quirks of a GPU.

So when you're programming, like, on a CPU, right, you're thinking in terms of a single thread all the time. And then when you're programming in, GPUs, your mindset should really be, multiple threads processing at the same time. Right? But specifically, a warp. That's because the whole hardware path is all divided into, 32 units, essentially.

Right? So so, you know, this is your typical CPU, a CPU with SIMD units, or are they really vector units. Right? It's what a GPU is. Right?

Everything is 32 wide. Right? So so instead of having one ALU, the hardware is grouped into 32 ALUs. Right? Instead of having one register file to read one operand, my register file is just grouped into, 32 wide bank.

Right? So I can read 32 values at once. Right? So so everything is grouped into notions of 32. If it's a AMD GPU, sometimes it's 64.

Right? But but all of this is is, is what a warp is. Right? So so at the lowest level, if you think about it, like, a five stage pipeline, you're thinking of one instruction flowing through a pipeline. Right?

So for GPU, you should think of 32 threads flowing through that pipeline at once. Okay? So so we schedule things at the unit of warps in the hardware. So a warp is 32 threads. When you schedule a block to an ascent, that block gets broken up into warps.

Right? So if I define my my thread block as 16 by 16, right, all of my threads in here is gonna get broken up into warps. And then these warp instructions is is is what's gonna flow through the pipeline. Okay. So so, hypothetically, right, if my thread block 16 by 16, how many warps do I have in that?

How many warps would it get broken up into? Eight. And so 16 by 16 is is, how many does I have? Two fifty six? Is that right?

32? That would be eight. Right? Right. So so my my five sixteen by 16 gets broken up into, eight warps.

Right? So so, essentially, I have these eight instructions that I could contact Swinich in and schedule for. Right? You could think of it like you know, on a CPU, I have one program counter because I'm constantly executing one thread. Now think of it like I have a locks, and I'm picking from eight program counters that I could I could schedule from, essentially.

Right? So so this is how we did the multithreading in the GPU. Right? So because of that limitation of, of a warp, that's also why we don't really see drop block sizes. You know, like, four by four or in, like, 17 by 17 or something like that.

Right? It just doesn't fit well within the warp. Right? So so, you know, if if I make my thread block four by four, it'll compile. It will run.

Right? But my thread block, the four by four is only using 16 threads. Right? So when it flows through the hardware, only 16 of my 32 AOUs are being used. 16 of my 32 registered files are being used.

Right? So so my hardware half utilized even though, hypothetically, I could use all of the threats. Right? And then you just 17 by 17. I don't think that's the visible by 32.

Right? So so then your indexing gets, like, really weird. And then you have a lot of what we call warped divergence because because one of the the weird things is that with warps, right, they all share a program pattern, so we want all the threads in the warp to do exactly the same thing. That that's how we usually need the most most efficient. Right?

Okay. So so, you know, the main takeaway here is that my my block is broken up into a notion of a warp. Okay? So so, we've literally just said this. Right?

Three blocks of 256. How many works in each block is is two fifty six divided by 32? Okay. Right. And then how many works in HSM?

So so each of these is, you got multiple works. Right? So so, yeah, you can as as you saw before, right, you have multiple blocks scheduled into an SM. And that's also because we have a limit to the number of works we could have, on a on a SM as well. Right?

Going back to that hardware limitation. Right. So maximum number of works per SM is 32 as well. Right? So so this is, that's another hardware limit that we have.

Right? There's a physical number of of works that we could we could have in in the in the SIEM. So since in addition to block size, you have a work limitation as well. Okay. So so in terms of, a thread block, right, we also linearize it as well.

Right? So going back to that 16 by 16 example, right, my warp is 32 threads. Right? So which which one of my threads, you know, mapped to my work of 32 threads. Right?

So so in a 16 by 16 thread block space. Right? Where where is my warp zero located? And so so so we had a group by 16 by 16 threads in the groups of 32 threads. Right?

So what would my group of threads be? Then Then you would just group that. The two. The two rows. Right?

So it's very similar to the row major format that that we have, essentially. Right? So your warp zero would be the two rows. Right? And then if I could use a different color.

Yeah. Right? My next two rows would be warp one dot dot dot dot dot dot. Right? It's not to scale.

Right? So so in total, I have, eight eight warps in this case. Right? So so so we linearize it as well in a, raw major format. It might be like like the way we handle memory.

Alright. So so we linearize it, and then we partition it. And then we're gonna talk about what divergence. This is gonna take some time. K.

So so on Thursday, we're gonna talk about what divergence and what that is, and why that that's such a big deal. So a lot of things in GPU programming is really so important to this. And this is assignment two, I think, as well. So so we'll continue on Thursday. We'll just end right here.

Alright.


Okay. So, yeah, it's 11:02. So we'll get started. So I made that announcement that your matrix multiply assignment is actually due not today, but tomorrow. I'll give you an extra day.

Yeah. Okay. So now you could bug the TA instead of me. Okay. Also, today, we're gonna talk about histogram, which is our last assignment.

So assignment four will also be assigned today. And, also oh, I don't have Discord on here. The GitHub password link for your final project, I I posted it also in that announcement. So, essentially, you you know, it's the same thing as your assignments, except it's it's a it's a group assignment. So you had to create a team and then make sure that all of your teammates join that same team.

Or if yourself, you create a team for yourself. Right? So they'll both join someone else's team, because then it gets really messy where I have to, like, manually move you around and and stuff, and I don't think they have classroom really supports that. But, otherwise, there should be a link right there. Okay.

Alright. Any questions with so far doing okay? Mostly you tried it? Yes? Okay.

Good. Alright. You know, I have office hours at 01:00 today, so if you have issues, you can ask me. But the most common issue so far has been, I think just the the boundary condition checks, especially when you're copying and padding zeros, copying the data to share my name and padding zeros. I think that that's basically more or less the the most issue that's been and and, you know, same threads and other race conditions.

And speaking of race conditions. Right? So today, we're gonna talk about how we handle race conditions, like, true race conditions. So we're gonna use histogram as an example algorithm, because this is one of the one of those simple algorithms where we have a lot of, risk condition data access patterns. Right?

So our few assignments, essentially, we have threads that map to one single output results. Right? In vector add, one thread is responsible for output. In histogram, it's more or less kind of the same. Right?

Every thread has their own index that that's kinda like their home index that they do an in place reduction for. So no one else is writing into it except yourself. Similarly, with matrix multiply, every thread is responsible for one output element that they compute on. Right? But what happens if you have multiple threads that need to write to the same location?

Right? There's a lot of algorithms out there that requires that. And and histogram is is the one that we're looking at as an example, and and we'll see how we could solve that. Right? Y'all probably made a histogram before in the past.

Right? It's a very common plot that you see. But it's also used a lot in in data mining. Right? Just for, like, summarizing data in certain ways.

It's used a lot in fraud detection. Really just a lot of data mining where folks make use of of histograms, databases, and everything else like that. Right? So so a histogram is basically just, like, a bar chart. We have bins, and it's just a count of how often a certain, you know, in this case, letter shows up in that frame.

Right? It's it's a way of summarizing the data. Right? This is one way of measuring it. So in this example, you know, let's take the string programming massively parallel processors, your textbook.

Right? And we're trying to do a count of, you know, the distribution of the letters that exist in in that sentence. Right? So the results would look something like this on the bottom where you have different bins with alphabets. Right?

You can put up your alphabet across, what, seven different bins. So there's, you know, five letters between a and d and five letters between d and h and so on and so on and so on. Right? So if you were to if you were given that string, right, programming massively parallel processors, and you need to come up with data that that shows that result in c or c plus plus or Python or whatever, right, how would you get that result? What would your general approach be?

Dictionary. Dictionary? That's like cheating. Right. You just take every letter through the dictionary, then you get a count at the end.

Right? Okay. That's a very Pythonic way of doing it. Right? What if you have c or c plus plus?

Do you have a dictionary? Oh, you need to get some unordered math. Unordered math. Like, yeah, it's just math. Right?

I swear when my when my students are doing interviews at Google, HashMap is the solution for everything. Right? It seems like it. Okay. But assuming we don't have fancy stuff like maps or dictionaries.

Do we know how many bins we wanna spread it on to? Yeah. You do. Right? So you have 25 it's it's gonna be a programmer input.

Right? You have 20 or 26 alphabets, seven bins. How would you represent the bits? The numerical count. Yeah.

It's a numerical count. Right? If I don't have maps, I wanna represent Vector of ints. Yeah. Vector of ints, essentially.

Right? So so you can represent these results as a vector of ints where where every, element in that vector represents h or d or each h or something like that. Right? So you basically have to just figure out what letter that is and what bins it fall into and then increment the count. Right?

And you can do that serially on c. Right? For every letter in that string, what bin do I fall into and, you know, bin count plus plus, essentially. Right? Yeah.

Makes sense. Okay. You know, so so if you wanna do this in parallel. Right? You know what?

Yeah. Let's just look at this. Right? If you wanna do this in parallel, just like vector add and everything else. Right?

We we have to partition it up. And the general approach would probably be, you know, map a thread to every input element like we do in vector add and figure out how to increment it more or less. Right? It's a very, you know, straightforward way of doing it, and it might look something like this where, you break up your input across four different threads as you're looking at, you know, every single element, you know, in there and try to figure it out. Right?

So for example, you know, let's say we break it up across four different pieces, and every thread is not gonna look at their index. You know, this this is the case where we have more data elements than we have threats. Right? You know, alternatively, you just map one thread to every element like vector app. But in this case here Okay.

We have less threats than data elements or vice versa. Right? So so in the iteration, right, we got p, we got blank, we got p, and we got o. Right? P, p, and o all mapped to the same bin, right, that we had to increment.

Well, it goes nowhere, so I don't have to increment any anything. Right? You you can figure that out. This is all asking. Right?

It's asking you to have to convert it to. You can convert it to a a numerical value if you want over it whatever like that. Right? But, yeah, basically, you know, p, p, and all match the same thing and you increment that. Right?

If we do this in c, right, and we do, like, p blank p and o sequentially, right, your end result you were expected to. But if you do this in parallel, right, what what are potential issues with this type of access pattern? Right after right after right after right. A bit of both a bit of all of those. Right?

But, yeah, you can have a race condition, essentially. Right? So so this is the time we're seeing an access panel where, multiple threads wanna update the same memory location. So so how do you handle that? Most of you have not, you know, seen a race condition yet, right, unless you took CS one sixty.

Is that correct? Yeah. Even in operating system, is it multi threaded? Yeah. Your OS is multi threaded.

No? Pthreads? Use pthreads in OS? Right? You have some form of condition, but not, like, maybe explicit like this in the app.

Right? So yeah. So so, basically, right, we have this issue where we wanna update the same thing, and we're actually gonna have some risk conditions because, we all wanna do the same operation. Right? We'll see later on what the risk condition looks like.

I'm gonna introduce it because most of you never seen it before. Yeah. And then, you know, the next iteration, right, we go to the next element. So this this time, we're gonna this to grab a bin up r and a and c. Right?

And then it goes through their different bins to to increment. And, again, we could have some race condition here specific for for the bin a a to d. Right. How do we partition, right, is another issue here. Right.

So looking at this, there's there's another way we can optimize this. Right? Besides the risk condition issues, if you kinda think of back to our lecture talking about memory coalescing. Right? Would this access pattern be be a coalesced memory axis, or would it not be coalesced in this case here?

If you recall, coalesced memory access means neighboring threads access neighboring addresses at the same time. Right? So do I have neighboring threads accessing neighboring addresses at the same time in this scenario here? No. Right?

We're kinda accessing in a strided way. My my my same thread is accessing neighboring data, you know, at the next iteration, but we want them to access it at this exact same time. So, again, right, we could partition this differently to make it a bit more optimized as well. That's what that's what this slide was saying. Right?

So so when we partition it up, you know, where every thread get n consecutive values, this results in in a strident memory access pattern, right, which is not coalesced. So the way we wanna really partition it is kinda like this interleaved way where, the total number of threats that you have, you know, execute on that amount, and then we kinda stride over the total number of threats that you have. It's kinda like a a grid stride type of access pattern. Right? So this is the way that you should be implementing it.

Right? So then you ask us how to realistically look look more like this if it's more optimized. So in the iteration, we'll look at, you know, a chunk of four where all threats are operating, and then we stride over just on the number of threats that we have. Right? But, again, you know, we still have some race conditions.

So let's see how we Okay. How we're gonna handle that. So right. These are now, you know, introduction to data races. So I I know a lot of you never seen this before, so we're gonna kinda go step by step to see what this look like and how we can address that with atomic operations.

Yeah. So so, typically, you know, data races occur when we have multiple threats. I don't think they really occur in single credit programs. Right? So so re remodified write operations, are really the core reason why we have data races.

Right? If you have a piece of code that that did nothing but reads, right, it doesn't really matter the order in which you read something. If if that data doesn't change, you don't see race conditions. Nothing changes. It's not an issue.

Right? The issue comes up when we specifically have modify modifications that we have to do, and there's other work that's dependent on that modification. Right? So so in this case here, you know, we're incrementing both p and o. There's a data dependency there because, you know, every single day is gonna do, like, pin value plus one.

Right? If you do that in parallel, we'll be done to pin value plus one, but to get the correct results, you know, he should complete the four o or else things are gonna get all messed up. Right? It's gonna get corrupted. So, specifically, right, there's there's a read, modify, write operation here.

Can you identify what the read, modify, and write operation would be? So so what am I so looking at thread zero. Right? What would be the read? What am I reading?

What am I reading? P? Okay. But how about on the histogram? Data structure.

In the structure. I'm reading something there too. Okay. Would would p cause any race conditions, reading p? Yeah.

Right. It it's a read, and nothing changes. Right? What else would thread zero be reading? Yeah.

You're reading the the bin counter? Yeah. The bin counter. Right? So I'm reading the bin counter of m to p, specifically this one right here.

So 22 is the end result. Right? The original value here would be zero. So so that those are my read operations. So what would my modify operation be?

What what's the modification? The what? So the bank counter. Yeah. The bank counter.

Right? What's what what am I doing to the bank counter? Yeah. Increment. Incrementing.

Right? So plus one is a modification, and then the right would be back to the bank counter. Right? So, ideally, thread zero would read zero, add one, throw one back into the pin counter. And thread zero would read one, increment it, and throw two back in there.

But because the threads of EP and o are operating in parallel, it's possible that they could do it at the same time, and they both read zero at the same time. Right? If they both read zero at the same time, they're both gonna do zero plus one, and then they both write one. And one is not too. Right?

That that's not the the correct value you can have. So that's the issue. That that's really the the race where the race conditions come come into play. So so we see, you know, like, we modify right is a very common pattern that exists everywhere. So for example, you know, like, the examples this is in video slides that is based off of, like, other bank teller, when you're depositing money or taking out money.

Those are counters that other people could look at at the same time. There could be multiple operations. Right? Another common one, I think, is when you're booking a seat on a on a flight map. Right?

You can pick the seat that you want. Let's say two people select the same seat. What happens if you both buy the same seat? It happens. I mean, they offer you, like, a thousand dollars to give up your seat sometimes.

Right? Anyone ever took that offer up? No? Yeah. Not me either.

But, yeah, that happens. Right? So airplanes, they overbook seats on oversell seats on purpose for profit or whatever. But, you know, if if that that was never handled, then everyone's, like, buying multiple seats at the same time. That that would cause a lot of issues.

But at the core of it, it's really you have multiple threads operating with a read and a modify and a write operation. Right? So so in this case, you know, we're both reading the same memory location. Let's say we have two threads. This memory location s could be the bin.

We store it to some register value. We increment it the plus one. Then we have some new value, and then we write it back to memory. Right? That's the mem arrow new.

So we're both writing it back to memory. So in this case, you know, we can have race conditions because because my my, ordering of of these instructions can be different just depending on how your operating systems or hardware, you know, contact switch your threads. Right? So so let's do an exercise just to see what could happen more concretely. So let's say, you know, MEM zero is initially MEM of x is initially zero.

Right? Right. And this is a shared variable. It's global in in memory, so both threads can read it. And we can run my threads in different combinations of ordering hypothetically.

Right? So after thread one and thread two completes, you know, what are possible values that this final mem x can be? What would you expect it to be? What's the correct result that you expect it to be? Two.

Right? Zero plus one plus one is equal to two. So two is what I expect. Now what are other values that you can potentially see here? One.

Right? So so risk condition occurs because this depends on the ordering of things. You might get lucky. It might work or it might not work. I don't know if you took operating system and you implemented these things.

Like, sometimes it works one out of 10 times with your page management because of race conditions and stuff. That happened to me. So, like, when I showed my code to my TA, I just prayed that it worked one of those times, and it got lucky. It did. I never figured out what the issue was until, like, a week afterwards.

But, yeah, there were risk conditions, essentially. Right? Alright. So so how can you get one potentially in this scenario? One thread overwrites the other.

One thread overwrites the other. Yeah. So in order for that to happen, you know, one overwrites one. Right? Which means that both of them read zero from them zero.

Right? Essentially, if they both read zero in this scenario, however, I reorder the rest of the instructions. It doesn't matter. I'm gonna get one as a result, which is incorrect. Right?

Alright. So so now looking back at this example again. Right? What are possible values that your old could be here? Just depending on the ordering of things.

Initially, it could be zero. Right? So one value could be zero. Are there other values that I could potentially read? One.

Right? If it's one, that's that's great. That's what I expect it to be. It means the other one finished before. Right?

It's you know, everything is basically not nondeterministic. It just really depends on the ordering of things. Right? So whenever you have this, you know, nonanswer. Right?

It just depends on the ordering. It it's a risk condition. So, you know, just, visually, right, we can reorder these in several ways. If we schedule the thread in this way where, you know, all the read aloud operations are not interrupted, you know, this is the correct result. This is what I expect.

Right? This is when you get lucky and everything works. Alternatively, thread two could finish before thread one. That's possible too just because of how your threads get scheduled. In that scenario, things work out great too.

Yay. I got lucky. But when you start reordering things, that's when things could, you know, become incorrect. Right? So let's say we have this reordering.

So 00 is gonna three one's gonna read mem x So, you know, zero is gonna go here, zero plus one. And then, you know, you're gonna write one to memory. However, thread two, gets scheduled to read memory, you know, before my one is written. So this case here, I'm gonna read zero as well, right, just because of the way my scheduler happens to be. So then I'm gonna do zero plus one, and then I'm gonna write one as well.

So then this is how we essentially get that One of those, you know, race conditions where we end up with the incorrect results. You know, this is just a simple scenario with two threads. You can imagine now if I have, like, a 100 threads, my end result could be any number between zero between one and a 100, essentially. Right? It becomes a lot more, you know, non non deterministic when when you have more threads.

I think once I made I I gave a final exam. If I had four threads, what would happen? And and that was, a mistake. There was too many combinations. Okay.

Right. So then, you know, you can reorder it again, and then, essentially, this will also be incorrect because, both of these are being reading zero, and and that's incorrect. But, so more more or less, right, anytime we have a data race, is is whenever my read, modify, write operations are broken apart. Right? So that's really what an atomic operation is.

An atomic operation, in order to enforce correctness and avoid race conditions, is to enforce atomic operations of. Right? Right? An atom is, you know, the smallest unit you can have. You can't split it.

You can. Right? But in theory, it's the smallest thing you could we check how they work work for, you know, all this stuff. But Adam is the smallest thing that you have. Right?

So that's why they call that atomic operation. So so the atomic operation enforces your remodified right to be executed together without any interruptions. As long as we enforce that, we know that these data races can can be resolved. Right? So that's so that so that's what atomic operations are, and that that's how we solve this.

Right? So let's see how we perform atomic operations in CUDA. Have any of you actually used atomic operations before? Like, in what class? One sec.

Oh, you actually used the atomic operation? I think so. Yeah. Okay. You use that to build locks and and semaphores and Not too bad.

Other stuff? Okay. Oh, gotcha. Right. In the absence of atomic operations, right, you probably handle these critical sections using locks or something like that.

Right? So, you know, you would you know, before jumping into the section, you would you would enclose this in a lock so that only one thread could enter that section, that critical section at a time. Right? And then there's also all other stuff like semaphores, barriers, and everything else. At at the core level, all of these locking mechanisms are are built from atomic operations that support it in the hardware.

Right? Atomic is, like, the lowest level implementation that you have. Right? So we see how this is done in CUDA. Right?

I don't know why they have this. Okay. So so, basically, the the whole concept of an atomic operation, is to ensure that re modify write operations are not split up. What that means is that in the hardware, whenever there's a remodify write operation, the hardware has some unit that would essentially serialize the operations to that memory location. You know, if if, you know, your remote can't be interleaved, right, we have to serialize it.

In the case where, you know, we you saw the example where, you know, multiple threads are right into the same bin. Right? These three operations are are gonna end up being serialized in the hardware itself. There's there's actual hardware unit, ROP unit that that handles this in the hardware. But from you as a programmer point of view, there's gonna be APIs that are exposed to you.

So there are several atomic operations that the API provides. So the most common one is atomic add. This is this is probably what you're gonna use in class for your homework assignment. And the way you use this is you just call atomic add, point it to a memory location, and the value that you're adding to it. And we have to specify the data type because it would tell the hardware how many bytes of of memory that I have to lock down, essentially.

Right? You know, if you have a double, you're locking down eight bytes. If you have an int or a float, you're locking down four bytes. The the hardware unit needs to know, the size that I'm locking it down by. Right?

So this atomic add unit will essentially, you know, go into the memory, reach a value that's at address, incrementify value, and then write that result back. And then this instruction then will return. Right? So this enforces that that remotify write is is complete. So there's other operations too besides add as well.

Right? So you could do atomic operations for subtract, increment, decrement, min, max, like, you know, some kinda compare and swap, right, type of thing. Exchanges, compare and swap. So so, you know, these are all the the primitives that that you can use to create locks. Naturally, there's no mocking APIs.

There's no locks and semaphores by default in CUDA. But using these atomic operations, you you could build your own, essentially. Okay. Right. So so, again, you know, there's different in the API, there's different data types assigned to it, mainly because it tells the hardware, the the type of addition operation you're doing, like, whether it's a integer or floating point or double, sign or unsigned.

The way we handle the carry bits and everything else like that is a little different. So these all get compiled into different assembly instructions. Right? This is this is the CUDA API. But the assembly instruction itself, they're they're gonna be different atomic operation instructions.

So you could kinda look at it in the API to see what operations you have. Okay. Over time, you know, you think atomic operations were pretty basic, but they have actually increased the functionality of it. So because of hardware innovations to the interconnect, like NVLink, you can now do atomic operations across not just your GPU, but across multiple GPUs. Right?

So you have a unified memory address that multiple GPUs are writing to. You can do an atomic for that. It's gonna be super slow, but it's there if you need it. And then, also, you have something called block wide atomics. This is basically for more complex synchronization programs where you need to atomic flip in and block.

Okay. So, we'll see here an example of the histogram kernel that we saw before for the alphabet. Anyway, it's pseudo code. Assignment four is gonna be really easy. Right?

Yes. Assignment four, we're gonna do with numbers. It's a little different than this. This is very static. It's a small tweak to this.

Alright. So so, you know, your assignment for for this Instagram example, you know, it it it starts off basically very much like a lot of the other programs we see. We calculate this global index. We calculate a stride. In in this case, the stride is all of the threads that I have in in in my kernel.

Right? That's given to you by block dimension, times grid dimension. And then, you know, we're gonna do all of my threads. And then at the next iteration, we stride over, we stride over. So you see later on that, we have this for loop.

Well, I says the stride, I plus stride. Right? So this this while loop and this stride increment basically allows us to have all of our threads, you know, keep incrementing through my whole dataset until I I complete it. Right? But then within every single thread, all we had to do is just do one histogram operation.

Like we mentioned before, we just have to figure out what bin it goes into increment that bin, more or less. Right? So so we have, you know, a text string. Right? And, basically, we had, I think, seven seven bins, right, across, 26 letters, which means every every bin had, like, a, b, c, d, e, f, g, h.

So, like, four elements per bit, essentially. Right? So so in this case here, it kinda got clever, because it's an ASCII operation. So all they had to do here was that that they they took the value, and then they subtracted a. That looks really weird, but it actually confused.

Right? Can you figure out what that's doing? Yeah? What's it doing? Oh, it's subtracting.

I asked you to argue a from Right. So now every single element every single letter is not just an offset from, like, zero to 26, assuming it's all lowercase letters. Right? It looks weird because because you're like, why am I subtracting a character? That shouldn't work.

But it's ASCII. Right? So they they played this trick here a little bit, which is possible. So now, basically, your alphabet position will give you anything from, like, zero to 26. So they did a filter, you know, just to make sure that it's a valid alphabet.

So this way, you know, if you get the space or or a semicolon or anything else, right, you just don't add anything into your bits. Right? And then this this shows an example of of, the the end that I go into. Right? So the bin that I go into, they just take alphabet position divided by four.

You know, it's an integer division. So, you know, a, b, c, d, zero, zero, one, two, three divided by four. Sorry. Zero, one, two, and three divided by four is all zero. Right?

It's an integer division. Okay? You should all know that by now. Right? Yes?

Yeah. Okay. Python also does some really weird stuff with division too. Right? You had to do divide or, like, divide divide or something like that for some things.

I thought it makes no sense to me. I need it. And then, you know, if you have, like, four, five, six, seven, you divide both that by four, you get one always. If you do the mod, then you get the remainder, essentially. Right?

So so we just take the alphabet position, divide it by four, do it integer division, and then we get the bin that we go into. So there's the bin calculation. You gotta do, like, histogram of the bin, and then you put the ampersand in front because we had to pass the address of that memory location. If I don't put the ampersand in front of it, right, I'm just gonna pass in the value of that bin. So the value of that bin is, like, zero four.

Right? Atomic guide is gonna try to do an atomic operation on address four. It shouldn't be the the the value. It should be the address that I'm passing in. So so we have to, you know, put this ampersand in front, which is the address of that histogram index location.

Okay? So so I think that's that's one of the the biggest, mistakes I see in in the histogram. A lot of people forget that you have to put the actual address to the memory location that you're writing to there. Right? So then that that's how I got operation will automatically handle that atomic add of one.

Realistically, you can do atomic increment, I think, also here too. That works as well. But we just use atomic add. So more or less, that's it. That's your histogram operation.

Any questions? Straightforward? Yeah. Okay. I think in our assignment, right, we're also gonna see how we could optimize you know what?

I'm gonna talk about it now, actually. Right? Alright. So so, you know, this call, you know, is it it works. Sorry.

It's correct. Right? It works. However, it's very slow. Right?

Because we're potentially serializing the operations to my seven bins. Right? So if you wanna think of it, my has only seven bins. If multiple threads are operating on that bin, that's how my it essentially serializes it. Right?

So even though I have thousands of threads, at most, how many operations can I perform in parallel? How many? Seven. Seven. Right?

So so even though thousands of threads are coming atomic tab, at a high level, you know, from my coding point of view, I'm like, oh, yeah. Log on atomic tab. That's working in parallel. The hardware is actually serializing it just to enforce correctness. Right?

So the level of parallelism in the hardware will actually just be seven at a time. So this is actually very, very slow. So what are some ways, potentially, that I could improve this? How could I alleviate that bottleneck? Yeah.

Yeah. How do I make sure every thread has its own set of things? How do I do that? A local one? Yeah.

That means sharing every Right? Seriously, that's the that's the answer for everything here. So, yeah, each each ref block can have their own set of histograms in in shared memory. Right? Because right now, when I when I access atomic add, it's in global memory.

Right? So it's it's really slow. If I have my logo bins, I can put it in global memory too. Right? It's gonna be slow.

But I could also put it in shared memory as well because it's not being shared across different bread blocks now if it's local. So, you know, I can optimize that. And couldn't it just be in, like, registers or, like, just thread local variable instead of both local? Or is there would that, like, have too much space? Well, you would say if it's thread local If you're if you're thread zero and thread one and you're writing to the same register, registers are visible only to individual threads.

Yeah. So how would thread one write to the register of thread zero? I mean, you don't you just, like, call you just have every thread completes the counts for their own assigned letters. Yeah. And then once all the threads are done, you run all their outputs through a reduction kernel, and then you get the results.

Oh, wow. Every thread doesn't count themself Yeah. And then you reduce it? Yeah. That's true.

Did you know you can do reduce of only atomic operations now? Just think of it. You have this whole tree that that gets one result in the end. Right? All you're doing is atomic add up every element to that one location.

But that's not parallel. But that's not parallelized. Right? Yeah. I mean, you can, but that's very complicated what you're what you're suggesting.

Oh. Yeah. Because then in the end, you still have to do that reduction. And then, you know, reduction is actually not not very parallel as well. Right?

Because, every step you lose parallelism, and on average, you're using half of the parallelism. There's better ways to, you know, more better utilize this hardware. Okay. Right. But but at high level, right, the goal here is to essentially have a local copy of a histogram and then eventually reduce the results, essentially.

Right? So shared memory also functions just completely normally with atomic operation? Yeah. Yeah. So you could perform atomics to different memory locations.

It could be global memory or shared memory. I assume distributed share memory nowadays too, but not not in registers because registers are are thread local. Right? So, you know, looking at from a performance perspective. Right?

Every time I do an atomic operation, you know, the the command comes from my SM. It goes all the way to my global memory, which is my DRAM bags. And then it comes back, and then I do my plus one. And then I, you know, and then I write the results back again. Right?

There's a lot of back and for every single time operation. Now imagine that also gets serialized as well. Right? So this is actually very, very slow. So instead of every atomic operation incurring a DRAM latency read, you know, one way we can essentially try to minimize that is to try to see how we could move my atomic operations closer to myself.

So, you know what? It's better for me to just draw this out. Right? Okay. So so let's do an example just to kind of highlight what the next couple of slides are trying to what this I'm trying to make.

So so let's say I have, you know, input of, let's say, a thousand, And then I have a a histo of, let's say, 10 bins. Right? And I have a threat block of size I don't know. Throw me a number. 16.

16. Man. Why is this small number? 64. 64.

Sure. Right? And I have a whole bunch of thread blocks. You know, let's make it easy. Ten twenty four.

How many thread blocks would I have in this case? Two to the four. Two to the four. What is that? I don't know.

It has some number of thread blocks. Right? Actually, no. It's important. How many thread blocks do I have?

16? Yeah. Let's see. It is two to four? I can't do math right now.

I'm hungry. 16 dev blocks. Okay. So let's say I'm doing my histogram kernel. And, you know, one thread is mapping to one element, and we're all writing to this histogram.

Right? So in total, how many atomic operations am I sending to my global memory? 64 or ten twenty four. Yeah. Ten twenty four.

Right? Every input element is gonna result in your atomic add operation, to my histogram kernel. Okay. So so there's gonna be ten ten twenty four operations. Assuming my letters are uniformly distributed.

Right? Every one of my bins would have about a 102, a 103 updates. Right? So so every every element would have, you know, about a 102 or a 103 updates or so. Okay?

So so those a 102, 103 elements are are actually gonna get serialized. Right? So the time this piece of code is gonna take to finish would be more or less, you know, a 103 global memory accesses round trip. Right? Because I had to get the value, update it, and then send it back.

Yeah. So it's about 100, 203 global memory accesses. That that's really slow. Right? So our solution was to what?

What we're gonna do? Share memory. Share memory. Right? How how do you share memory?

I could I could allocate shared memory and not use it. Keep a histogram inside of each thread block. Right. So yeah. So now within each thread block, I would keep a histogram.

Right? So so now within each each thread block, I'm gonna have my own histogram. Let's call it private histo. Also 10 elements. Right?

And and every single one of them will have their own copy of private histo. So so how many how many copies of private histone would I have? 16? 16. I every every stuff like I still own copies.

Okay. Now if I kinda wanna analyze it again from this point of view, how many atomic operations would each thread block generate to their own local histogram? 64. 64. Right?

So assuming a uniform distribution, how many operations would incur for every bin? Six or seven. Six or seven. Right? Let's just say seven.

So so this will take about seven round trip time to global memory? Shared memory? Shared memory. Shared memory. Right?

I can make my private history in global memory. I could do that too. Right? But why? I have shared memory if it fits.

Right? So I could do this in shared memory. So now it'll be about a 102 about seven, you know, shared memory accesses. Right? This is about, like, a 103 global memory access before.

Right? Okay. So for for this rep lock as well, right, it would also incur about the same. This would finish in about seven shared memory accesses in parallel. Right?

Because they're in a completely parallel blocks. Right? So up until this point where I get my local histogram complete, the time elapsed of about seven shared memory access round trips. Okay? Now is my my histogram operation complete at that point in time?

No. No. Right. Now what now what do I have to do? You can reduce the model for that.

I have to reduce it with a reduction, Shariq. Do what? If I want. Not gonna work that much. Yeah.

Right. Let's just do a complex. Why would I write a home reduction stream and I could write one line of code instead? Right? Okay.

So now I have to do a reduction or yeah. I have to do a reduction using atomic operations. Okay? So how would I combine all of my private histogram results into my histogram? This is gonna be what you're gonna implement with it.

For you, like, I did the block. I'm gonna take every, like, for every, like, index one element and atomic add it to the final histogram? Yes. Yeah. Right.

So I'm gonna atomic add every corresponding element of a private histo to my global histo, essentially. Okay? So in that case, how many atomic operations are sent to my my global histogram? 15 for about 16 for each bin. 16 for each bin?

Yeah. Right. So so for bin zero right here. What what's getting atomic added to it? E s o zero for Yeah.

E s o zero. Right? So the element the element, dot dot dot for every single one of my thread blocks. So I have 16 thread blocks. So 16 right?

There's gonna be 16 atomic operations going to here. Similarly, for n, there's gonna be 16. Right? It's just the corresponding, indexes getting atomic added. Right?

So so in total, this global update is gonna take about 16 round trip times to global memory. Right? You know, so so now instead of a 103, right, with this technique, they'll take around 16 global memory access round trip. So what's faster? A 103 global memory access round trips or seven share memory plus 16 global memory accesses?

Unless you have really bad shared memory. Yeah. Unless you have really bad shared memory. Right? Yeah.

The the approach where we put a local histogram is significantly faster. Right? Not only because, we're minimizing the number of access to global memory, but we're a we're able to perform more kilo atomic operations, because of having a localized private histogram, essentially. Right? So in the book and in the slides, they they call this technique that we just drew out privatization, right, because we privatized the histogram, in order to allow parallel atomic operations in general.

Right? So, you know, these are basically the latency, how latency can kinda impact the amount of throughput that you have. That's kinda like what what these slides were talking about. And then to give an example with with grocery checkout store. Basically, instead of running back and and putting something on the checkout counter, right, you have a shopping cart, essentially.

That's kinda like your shared memory. I don't know if it's a good analogy. I don't like I didn't like that one. But yeah. So, you know, one way to improve it is, like, you can have shared latency, shared memory where the l two access latency is actually a bit faster.

Sorry. Not l two. Shared memory. Right? You can also have atomic operations in l two, so that's faster than global memory.

Believe it or not. Okay. In the early days of GPUs, atomic operations were performed in global memory and SMs. After such a generation, they repurpose the raster operation part of the graphics part of a GPU that sits in l two to perform the atomic operations now. So I don't know.

I found this fascinating because I like reading handsets sometimes for fun. Yeah. What? I know. I mean, I I I like what happens internally, and there's no documentation.

So sometimes, yeah, you just have to read the contents. So so in reality, nowadays, atomic operations actually goes like this in global memory, because it's just like a it's like an ARU inside of your daemon controller. That's because that's where your l two sits. And there's a graphics component called, like, rasterizing operation unit or something like that, ROP or something, that handles the rasterizing part of your graphics pipeline. So they repurpose this for atomic operations.

So yeah. So so modern day GPUs, instead of going to global memory, you're actually act incurring l two access latencies. But in our case, we're gonna do it in shared memory because shared memory is really fast, and we get more parallelism. So that so that's just kinda how the hardware is optimizing atomic operations. If if you want, you can read patents.

They're fun. If you go in industry, I don't know if you heard You shouldn't read patents. No. You should write patents. Oh, but you shouldn't read patents if you're Oh, yeah.

You shouldn't read patents. Right? Yeah. You shouldn't read patents. I I do some IP litigation consulting.

It's actually really funny to learn about this stuff. So some engineering companies, they actively tell employees not to read patents because if you do something that infringes, you know, you're like, oh, I never read that. So, you know, you can't get sued. And then there are some companies that are basically, like, just patent mills. Like, IBM is one of them.

When I was at Samsung, they basically rounded up all the interns and told us, hey. I want all of you to come up with an idea to patent. It doesn't have to be good. But if you patented it, we'll give you money. Right?

So, you know, they just didn't wanna increase their patent portfolio. Right? Do they cost money for them to file patents? Yeah. But there's value in having patents because it protects them, and in the future, they could sue other people.

Right? The the goal is to write patents in a way where it's very general but still specific enough that you could sue other people a bit. I need to make patents. Yeah. It is.

Right? It is. Yeah. All companies do that. And then you have patent trolls that take and buy these patents to sue random other big companies.

You hear it in the news sometimes. Right? Right? Like, some known name company with some patents from, like, thirty years ago are suing Intell for something billion dollars that they actually won. Yeah.

If you don't like engineering or you like law, going into IT, going to law school after your degree is very lucrative. Right? So even just right out of law school, you'll make more than most engineers already. It's it's yeah. It is.

Like, as a associate lawyer, you make, like, $3,400 k. Yeah. They make so much money. So no. Seriously.

If you wanna go into IT log because I'm saying this because because the job industry sucks right now. Right? So now's a viable path for you guys. I have a friend who went who, did EE with me at USC as undergrad, and then she went to law school. You make so much if you could actually read and write, you know, patents and all this stuff.

Right? It's a very viable career path. Yeah. And and it's fun because you're not, like, stuck on the same project all the time. Right?

If you have different lawsuits going on, you you constantly have different projects that you can contact search across. Okay. But, you know, anyways, back to this topic. Yeah. So so, basically, right, the whole idea behind privatization is to okay.

If you if you actually go and become a lawyer in the future, like, ten years from now, and you wanna say thank you, you can find me at the house afterward. Oh, it's alright. Okay. Great. So so yeah.

You you know, the whole goal of privatization is to, you know, reduce this, bottleneck into your global memory, by making a copy into your your local shared memory. Right? And we kinda already did the analysis before of, how much less contention that we would see and and also from a kinda complexity latency point of view. Right? So instead of, you know, like, a 103 global memory access, you instead have, like, seven shared memory and, you know, 16 global memory accesses or something like that.

Right? So so we did that analysis before, and and clearly, it's significantly faster. So so, you know, doing these techniques by barely, you know, stopping shared memory and everything and doing it locally, has a significant improvement on things. But, you know, not everything is free because now there is a bit of a overhead. You have to, you know, create and initialize this private copy.

It's a little more effort in your coding. You have to make sure it fits in the shared memory. Right? So one issue potentially could be that, you know, you have a very large histogram of a lot of bins, in which case the amount of shared memory that it might require can limit your parallelism, and it there might not actually be a a benefit, especially with the newer GPUs where you had atomic operations that are done in l two. Sometimes your global atomic operations are actually faster than your shared memory atomic just because of that hardware optimization nowadays.

Right? So if you actually benchmark it in your life, the results might not actually match what you expect in theory just because of what the hardware is doing on optimizing. But but, anyway so these are all the different considerations. So so more or less, you know, if you do shared memory atomics, it's typically a lot faster, about an order of magnitude faster than l two or a 100 times faster than DRAM. And now here's some pseudo code.

Again, this is very static because we'll have seven bins. I think the challenge with your homework assignment is that the number of bins that you have isn't determined at compile time. So you have to dynamically allocate this. I'll I'll make a post on how to do dynamic stream memory allocation. The brute force approach is just to make a giant histogram bin of, like, 5,000 something or whatever could fit.

That gets around it, or is it not being the best performance? The same normal way of dynamic validation? It's a little different. There's some weird keywords that you have to do, and then when you do your kernel launch Oh, really? Yeah.

You know, you have your your thread block size, your grid, number of threads, and your thread block. And then this value here is just shared memory, and then this is streams. Okay. Right? So this one, you have to specify where every thread block would have, you know, like oh, no.

My pen is running out about it. Four k, for example. Right? So so there's a there's a way to do dynamic shape and nail enclosure. I'll I'll make a post on how to do that.

So so you're hoping to sign it. You know, ideally, you would you would do the shape and nail enclosure, dynamic point. But but in this case, you know, it's it's static, so it's seven. And then that creates a a histo. Right?

So thing we had to do is is we actually have to clear it. Because by default, if you just allocate a space in shared memory, it could be garbage values there from something. I don't know what. So what this piece of code is, is doing is that we're clearing that histogram in in parallel. Right?

So if my thread ID is less than seven, because that's only seven bins, I would write zero to it. The naive way of doing this is a four loop of I from zero to seven and have every thread write zero to it. It works. It's not efficient. Right?

But it works. And then the challenging here would be What difference? I mean, yeah, you can't really avoid this. Yeah. And you have what there it is.

But the other issue here now would be what if you have more things than you have threads. Right? Then you have to do that Stride access pattern to clear this as well. And that's just something to keep in mind when you do your homework assignment. I think that might be one of the one of the pieces, I think, that we put in.

I forgot. But, anyways, right, the whole goal here is to is to clear it, and then we we sync threads afterwards. Right? Because everything else afterwards assumes that my privacy has been cleared, so I wanna make sure that, that that is enforced. Right?

And then the rest of it is basically the same. The only difference now here is you replace system with private system, compared to the alphabet example that we saw before. The only difference now is, this is now a shared memory pointer. Right? So your atomic add, the compiler will automatically know what address space that is.

That's private history of the shared memory variable, so it knows that, in in assembly instructions, every load and store has, you know, global shared, local constant, whatever. I don't know if it's memory space allocated to it. So you don't have to actually say, like, atomic add shared or atomic add l two. Right? It's just a a memory address that you pass in.

Exactly the same. And then this is the code where you combine everything together. So for every single bread, you know, again, less than seven, right, because I have seven bins. I will basically take the value of one of my bins and then add it to the corresponding value or index. Right?

So so these indexes are are corresponding indexes, and I take the address of that. This is not the address. This is the actual value itself. Right? Another common mistake I see is that you put an ampersand here.

If you do that, what what are you gonna what's gonna happen with that? Right? Yeah. I understand. Yeah.

Right. You're adding an address to that value. So you're gonna add, like, 0XFFSomethingSomething480. I don't know what. Some giant number to it.

And then your results are gonna look all messed up. So so, yeah, this is the extra additional step that you have to do. This is the part that that combines everything together. And, again, if you have more bids and threats, you just set out an account for that too, in your home at the summit. So now at the end of this point, I should have everything that is complete.

Right? So now I have my final history back. An alternative to this two line of code is your reduction tree that you did for assignment two. Right? So you could throw in your you could throw in your reduction tree here.

It it kinda works. Actually, it should be a separate hurdle, but, yeah, it kinda works that way. Alright. So, this whole privatization technique is is very commonly used in a lot of parallel algorithms. It's very it's very powerful technique mainly because we just avoid memory access, and we avoid contention, to to a specific memory location.

So so we see this a lot in different places. One consideration is that, you know, your histogram should be small. It should fit into shared memory. If your histogram is very large, you know, there's some tricks that you're gonna have to play. For example, you could do a partial histogram of, you know, like, sections of your histogram and then partition that as well.

So one year for one of the final projects in one of my grad courses, I think they were, like, PhD students in databases. So they were looking at how to do a very large scale parallel, scalable, histogram operation on, like, terabytes of database data. So, you know, the histogram would would be very large and don't fit in even global memory shared memory at all or cache. So they were looking at ways on how to partition that across multiple GPUs and and do a Instagram operation that's paralyzed. You know, so in in the real world, like, working on real systems, this technique that we learned is is very basic, but, you know, you could extend this to multiple GPUs and and databases and other data mining frameworks and and stuff like that.

Like, I think RAPIDS has something like this too. I mean, it's just caching. Right? It's just caching, but you gotta get really clever with caching. You know?

So so you say it's just caching, but caching has been, like, a a major topic of research in in the architecture space since the seventies. And even now, there's a lot of papers on, like, how you optimize caching, you know, actively at systems conferences, microarchitecture conferences, you know, like, in the cloud. Have you heard of a Memcache server? Yeah. Like Redis.

Yeah. Redis and stuff like that. Right? There's a lot of research still on, like, how do you make use most make effective use of caching in cloud systems and everything else like that. Right?

Especially with how microservices work and serverless computing works. Right? And and so on. But, yeah, caching is is a it keeps you gainfully employed forever, more or less. Until, you know, quantum computing There's only two hard things in computer science, naming things and caching knowledge.

Naming things and caching dates? Caching knowledge. But we can't name things. Everything is engineers can't name things. I don't know.

AMD definitely has issues with that. Okay. Yeah. So that's all I have for histogram. Any questions?

No? Okay. So your your homework assignment three is due on Friday, And then your histogram assignment is also assigned. Do you want me to pull it up? You can find it.

Right. It's the same format nowadays. You understand? You understand how it is. Okay.


Sure. This is recording. Alright. So, hopefully, there's no issues with the the recording from from today. Alright.

So there were some announcements that I made last week. I guess it it wasn't captured. Just so you guys remember, there's no class this Thursday. Right? Yeah.

A lot of you didn't hear that, apparently. Yeah. So so we have, was it, reviews for our graduate program. So my whole day is filled up with meetings and stuff like that, and it conflicts with teaching. So we skipped lecture on Thursday, but we were actually running ahead of schedule anyway.

So we're not actually falling behind at all. Right? So so you guys are, like, smarter than last year because I'm going faster than last year. Right? Good job.

This means I could probably squeeze an extra assignment. Yeah? No? No. Come on.

Some of my teaching reviews, they actually say I wish we had more assignments. So okay. Alright. Yeah. So, there's no, class this Thursday.

Right? Okay. So assignment two, we're gonna talk about the reduction algorithm today. On two last week. Right?

Last Thursday, we talked about, you know, warp divergence, how warp divergence is handled in the hardware. And, really, that's that's why we have the the issue of, you know, underutilization and making sure all the threads in a warp do the same thing. Right? So we're gonna see that specifically with the reduction algorithm. We kinda introduced it a bit last week.

So we'll go more into detail about it, today and kinda analyze the behavior of the reduction algorithm and how we can write, a a code that's more friendly for work that are written with the reduction algorithm. Right? So that that'd be today. And then, assignment two is also assigned. So we'll talk about this in a little bit after we, introduce reduction.

Right? So there'll be two implementations, naive and optimized, and it'll be a way to also record your work diagram. So see see how your code actually behaves. Right? So we'll talk about that assignment in a little bit.

So we'll just go over the the algorithm that that that we were starting to introduce last week. Okay. Actually, before I jump in, do we have any questions? Logistics? No?

Looks good? Okay. Alright. So I I I'm not sure how how much of this is captured last week. So just to quickly go over, you know, what we we talked about very briefly, last week.

Right? So the reduction algorithm makes use of sort of cost shared memory. Basically, it's a user defined cache. Right? So every assignment that we introduce essentially makes use of another hardware work, right, or hardware feature.

So reduction, specifically, we're gonna see how we could deal with shared memory and and also something called work divergence that we we talked about last week. And then for the next assignment, assignment three for matrix multiply, we'll we'll see how we can make use of shared memory even even more so. Right? So this one just is a gentle introduction to shared memory, so we're just gonna use it very quickly. But we're not gonna optimize the code for shared memory yet.

That that's assignment three. Right? So so this is shared memory like we mentioned last last week. Right? And the shared memory is essentially a user defined l one cache.

Physically in the hardware, it is an l one cache. You can you can dynamically partition the SRAM between, you know, like, 256 k shared memory and 512 k, l one or the other way around. Right? There's a setting for that in in most GPUs. So the shared memory is physically l l one that you can control.

The way you move data in and out of l one is that you just create shared memory variables. This is basically allocating memory into shared memory. So you just have underscore underscore shared, in front of, you know, your variable int, for example, and then, essentially allocates, a shared memory variable. Right? So we're gonna use this essentially to to create the buffer or the cache that we're we're self managing.

Alright. So we'll see how this is gonna be used, for the reduction algorithm, in in a couple of slides. Alright. So so, yeah, this this is one example of it. We'll see more examples of this later on on how we, how we use this.

Right? So so, you know, right now, there's a lot of different memory spaces. Right? So so one of the things that is very different between a CPU and a GPU is just the fact that a GPU has a lot of hierarchies. Right?

Instead of a process in the thread, we have a process that launches a grid of thread blocks, and the thread blocks consists of threads, and the threads are organized into warps. Right? There's a whole hierarchy of it. And nowadays, with newer NVIDIA GPUs, now those could be organized into clusters of thread blocks. Right?

There's another hierarchy that they introduced as well. Right? Similarly with memory, we have global memory, shared memory. You have registered, constant memory. I don't I'm not showing here, but you also have texture memory, local memory, and everything else like that.

So so, you know, how do I know where do I allocate my memory for, you know, for if I wanna use some variable? Right? So, typically, if if it's a data that that's moving between the CPU and the GPU, like, so your like, your inputs that you need to process or the outputs that you, you know, you want your results back, those typically go into global memory because that's visible to the host. Right? Other stuff that's kind of like, you know, just intermediate variables, working variables that you need for computation, but not the final result of, those can get are typically in, you know, register, right, if it fits, or in this case, shared memory where we micromanage, and, you know, move in and out.

Right? So so that's kinda like the rule of thumb. Right? In general, most of your stuff will be global memory, except for a buffer that you self manage, and and that would typically be a shared memory. Alright.

So so, we quickly over with, you know, what what shared memory was. It's essentially in in your mind, just think of it as a user defined cache that you control, and and then that's it. Right? You don't have to worry too much about it. It's the cache that you manage yourself.

We call those type of memory also scratch pad memory. You might see those terminology somewhere in other classes as well. So so shared memory is a type of scratch pad memory, if you ever see that terminology. Right? So we went over this example, where, you know, sometimes you have a workload or a memory access pattern that has a lot of reuse and locality.

Right? So in this case here, we see our threads both access the same array, and we process the same data that we input. Right? So there's spatial locality because if I'm accessing one data, I'm accessing the next data next to it, and and temporal locality because I mean, the next thread is accessing it. And temporal locality because, you know, some some other direct block might access it later on in time.

Right? So so so the whole idea here is to, you know, move the data in the shared memory once from global memory, which, you know, takes a long time to access. And then everyone basically reuses that data into shared memory until until we're done using it. Right? So so we'll see examples of this, in the reduction algorithm, in a couple of slides.

And with matrix multiply, we'll see this, heavily emphasized. Right? So we'll see how we can, you know, change the algorithm fundamentally to to make use of of this, cache, essentially. Alright. So so that that's, you know, that's basically what the shared memory was.

Any questions with that? No? Still good? So there's shared memory among each chip, but also shared memory along the entire GPU or just on your chip? Okay.

It's it's on chip. Right? So so on chip Means it's local to the work. On chip means it's, the GPU chip. Right?

The shiny white square thing. Right? So so on chip, you have your SMs. Right? And then you also have your, you know, l twos as well outside of the SMs.

Right? And then there's some interconnect, right, that connects them all. And then the l twos are connected to something, you know, that's off chip. Right? So off chip you have off chip memory.

This is your GDDR and, HBM, right, for the data center fast GPUs. Right? So, typically, these are all on your your PCIe card, if you wanna think of it that way. And then this is connected over to your CPU, right, over your PCIe. And then your PCIe is connected to DRAM.

Okay. Right? So, specifically, inside of this SM, that's where your shared memory is. Right? So so within every SM, there's a there's a shared memory.

So the shared memory is a physical structure that's shared by everyone in, other threads in the SM, but the data is only visible to the scope is only visible within, your your thread block. Right? So so you have, for example, two thread blocks on the SM. Each of them have their own copy of a shared memory variable, but they can't view each other's copy. Right?

So so we see examples of that code. We'll talk about that in a little bit. Right. So this is, I guess, the mental model of the memory and the GPU and where they are with in relation to each other. Yeah.

Confused? Yes. Confused? Okay. I had a professor, at USC.

He he was always he would always ask if things are, like, clear as mud. Right? So is this clear as mud? Yes. And he has a habit of repeating everything three times because he said if he repeats everything three times, he finally get on with their time.

Yeah. I don't know if it's true or not. Alright. So so we talked about the reduction algorithm. We introduced it last week already, but I'm just gonna go to this picture because I think that's kinda where we left off.

Right? So, you know, the reduction algorithm is basically you know, you have an array, and you wanna perform some kind of reduction operation on it. So the reduction operation typically has to be associative and commutative. Right? So reduction operations can be min, max, add, write sum, these type of stuff.

Right. If you were to write this in c, right, the normal way of processing this sequentially would just be to iterate over every single element and add it together. Right? That's that's essentially a reduction operation, but it's serial. Right?

So so if you you just add every element one by one, you get a o of n algorithm, right, from an algorithmic complexity point of view. So it's not very fast. So our goal is to see how we can implement this in a parallel way. Right? And we talked about it, how, basically, all the operations can be done in parallel.

It doesn't matter the order in which you add or max in this case. Right? That the array, you will still get the correct result at the in the end. Right? So we can essentially perform reduction in the form of a reduction tree where we do a max or sum or whatever of two elements at a time, and then we do the sum or max of those two elements, the results of that.

And then we keep doing that for every result until we finally get one result that current you know, that that, is the reduction of of the whole array, essentially. Right? So we kinda see this here where we have eight elements. Right? So in total here, you know, we we have eight elements, that that we're trying to reduce.

The number of steps here would be what is it, log base two of eight, which is three. Right? So we have three steps. Right? So the step, performs, you know, these reduction.

The step performed this reduction. The last step performs the final reduction. So then this will be, a log of o log of the n algorithm. Right? So so, clearly, it's much faster.

Right? O of n is is much slower than o log n. So parallelism basically affords us this algorithmic complexity gain that that that we're able to to get from this. Okay? So so the whole point of today is to look at how we could implement this on a GPU and analyze it from the point of view of what diversions to see how efficient that would utilize the hardware.

Okay? So so we kind of, left off on this, I think, last week. This is where we stopped. Right? Yeah.

No one paid attention. Okay. Don't worry. Your midterm's in two weeks, I think. Right?

Is it? I think so. Yeah. You'll pay attention soon. So okay.

Alright. So, so, you know, how can we go about, you know, analyzing this and and seeing how well this works and how do we implement this. Right? So there's various ways in which you could kind of, analyze this. Right?

So so we'll see later on how we can analyze it from the point of view of, warped divergence, But these parallel algorithms also has things besides performance that you can analyze. Right? So so one of the things that you can analyze a parallel algorithm for is something called work efficiency. So work efficiency is basically the amount of work that needs to be done. So so with with parallel operations, right, it's oftentimes that, yes, we have less I don't know if it was the case with this algorithm.

Right? So, yes, we have less, steps, but sometimes we have more operations to do. Right? So so those type of algorithms may be less work efficient, but you you gain parallelism. Right?

So so in this case here, right, we could do something called work efficiency analysis. So work efficiency is a metric that you could use to analyze these algorithms as well besides just, you know, o of something, right, how many steps you could take. So work efficiency analysis is basically the amount of computation that that an algorithm has to perform. Right? So so for reductions for you, right, the amount of operations you have performed is basically n minus one.

You could you could basically kinda compute that or count that. Right? Graphically, it's just you know, if if I have eight elements here, right, my step is gonna do how many operations? Two. How many?

Four. Right? So so every every thread operates on two two inputs. So then, you know, there's four operations here. And then for every step afterwards, how many operations do you perform?

Half it. You half it. Right? Yeah. This is some kind of series.

Right? The number of operations is I forgot calculus or whatever. You do a series and then you add it. But, yeah, you end up with n minus one. Right?

Operations. So this is n minus one operations. Right? And then in terms of number of steps, right, we we saw it was a log of n. Right?

So something like, a million input, which is the default value, I think, in your assignment, would take, hypothetically, 20 steps. Right? So so given the number of steps and number of operations, you could kinda calculate the average parallelism. Right? So so you just take the operations, divide up a number of steps.

That's how many operations you do per step on average. So in this case, on average, the number of steps or average parallelism that we have is is, you know, 50,000 operations. However, when I allocate the number of threads that I need, right, I have to allocate the number of threads based on this step. Right? I I I can't allocate based on this step because then I just have one thread.

Right? I need enough threads to handle my step. So so for for the case of a million, right, I'm gonna need initially, like, half a million threads even though my average parallelism is only 50,000. Right? So so on average, my reduction tree is actually only using 10% of the hardware at a time.

Right? Very inefficient. But I gained, you know, o log of n. Right? So so there's a trade off here.

Right? So it's not a very resource efficient or very work efficient algorithm, but I have less steps. So so I guess, you know, I'm happy in the end. Right? So it's not not resource efficient.

But in terms of work efficient, right, there's another definition that, NVIDIA likes to call it work efficient. So it's not efficient in terms of payloads up, but it's sufficient in terms of work efficiency because the number of operations is exactly the same as a serial operation. Right? If you do that serial implementation is still n operations n adds. Right?

Later on, we'll see other algorithms like scan and and and other stuff where, for example, the amount of work could be like you could do 10 x amount of work, and it's still faster. Right? So that's a very inefficient, algorithm where, for example, instead of computing results and caching it, I could just keep re recomputing the results because computation is cheaper than memory access, for example. Right? So so there's a lot of different trade offs between not just the number of steps or the execution time, but resource efficiency, parallelism, and work efficiency.

Right? So it's all of these we we kinda balance. So throughout the course of this class, right, we'll we'll see different algorithms of how we could optimize it. So you could kinda see, this different trade off that that we do between, between basically managing all these trade offs and and doing algorithm parallel algorithm designs. So so when I joined, we actually didn't have a lot of algorithm courses.

And now I noticed that even at the undergrad level. Right? You have, like, algorithm in engineering as a course for tech collective. Right? And then I think at the grad level, you have parallel algorithms.

Right? So there's a ton of algorithm coursework now at UCR. So if you're, like, really interested in more advanced algorithm design, there's so many courseworks for that here nowadays. Do you like algorithm courses, typically? Some people do.

Yeah. That was, like, my only c in grads in undergrad, I think. I had a a go into the finals, and then I bombed my finals so bad. I got the I think I got a d. And then because of it and this is, like, my my last semester, so I thought I wasn't gonna graduate.

And luckily, there was a curve, so then I got a c, and I graduated. Yeah. Senioritis. Right? Don't do that in this in this class.

I don't wanna fail you. Okay. Oh, so so you know RSA, the algorithm for for encryption? My my algorithm professor was the a in RSA. Oh.

Yeah. Yeah. So he would tell us stories about how he came up with RSA. It was like, he's Jewish, so he had too much wine to drink during one of the holidays, and then they just had an idea, and that's how they got it. That's how they came out with the RSA algorithm.

Yeah. Okay. Yeah. So so we're gonna see how we do at this parallel sum reduction algorithm. Okay?

So, basically, you know, at a high level, right, we need to have enough threads to handle the step. Right? So that's how many threads we allocate if you wanna think of it that way. Right? And then in terms of the number of steps that we have to do is is log of n.

Right? So so, basically, if you kind of have to implement this in a parallel way, right, you get ready to kinda see that the number of thread blocks that I need will have to be based on the total number of elements I have to perform divided by two. That's how many threads I need, and then you split it into thread blocks. And then I have to do multiple steps. Right?

So, therefore, I'll probably need a four loop that iterates and and, you know, perform each step at each iteration. And then within each of that step, I have to perform the reduction algorithm, the operation for each element, but only some of the threads have to perform work. Right? So there'd be a if statement telling me which thread is active at a high level. Right?

Yeah. Obvious. No? Okay. Right.

So, yeah, well, we'll go over the code, right, and walk you kinda through it and and and and the intuition for this. Right? So yeah. So so, you know, you can kinda break down this problem. Like, oh, man.

How am I gonna implement this tree? How do you have you implemented reduction trees before or binary trees? Yes. Right? I I'm sure if you took algorithms, you implemented something like this, kind of.

Right? You've also done, was it, DFS, BFS. Right? It's kinda like in reverse, kind of. Right?

So so, you know, if you took algorithms, right, you can kinda squint and see how you would kind of approach this problem. Right? So so we'll see specifically how we can implement it. Right? And, also, not only that, but, also, how do we map the data, And how do we map a thread to a specific piece of data?

Right? So this is the the naive version that that we're gonna look at, and, you're gonna implement this in your assignment and also analyze the performance, using the code that you write. This is pretty simple to implement for your assignment because you're literally just copying and pasting what's on this slide. Okay. Actually, what's on the next couple of slides for the naive implementation.

For the optimized implementation, you're expected to do it yourself. Okay? Be very careful if you know, with that because, this is the assignment where I catch chat GPT code the most. Right? Because optimized reduction is such a generic terminology.

They don't know what optimization we're talking about. So students submit CHAT g p t code for reduction, and I'm like, why is it doing that optimization? We never learned about it in class. Right? So please try this yourself or just be critical and analyze what CHAT g p t is doing and realize it's leading you down the wrong path.

Right? So, yeah, so we're gonna go over this this naive implementation Right? So, just to take a step back. Right? How do we you know?

So this is what happens inside of a thread block, but how do we actually split up the work Right? So let's say we we have, you know, this very large array that I have to process. And and we talked about this last week, but I don't think it was recorded. So let's say it's of a very large size. I don't know.

Someone give me a number. Throw me a a number here. 2012. Something bigger, please. 2000000000.

2000000000? Wait. That wouldn't even fit in memory. Smaller. 1000000.

1000000. This is your assignment. Fine. I'll do a million. No.

No. No. That might give away some answers. 2,000,000. You never guess the answer now.

Right? Okay. So that that's why we have 2,000,000 elements. Right? And then your threat block size is of some size.

What's a valid threat block size that I could throw here? I mean, you could pick any thread block size. Right? The assignment is set, but, yeah. So what what's the size that you could pick here?

Why would I use two d on a three d on a one d problem? Right? You could just You could I mean, I guess you can. No. No.

Wait. This is invalid. Right? Wait. 16 times 16 is two fifty six times 16 would be two k.

4,009 For the yeah. That's too many threads that wouldn't fit on an SM. On on vendor, you can only fit a thousand thread. 1,000? Yeah.

1,000 threads. Right. Yeah. So 16 by 16 by 16 would be invalid on on on vendor. What?

Python 12. Yeah. No. Python 12 is your homework assignment. Two fifty six, actually.

Yeah. You can't generalize this answer. Okay. Right. So two fifty six.

Right? So let's say each thread block size is two fifty six. Okay? So so your thread block is of size two fifty six. The total elements I'm reducing on is, you know, 2,000,000.

Right? So I guess this is my problem size n. Okay. So how would I, you know, break out this problem across my different thread blocks? Am I am I performing a reduction over this whole, array?

Can I do that? Can I launch enough threads to handle n of 2,000,000? No. No. Right?

I have a limited size of number of threads that I need. Right? So so we're gonna have to partition up this problem and just subtrees sub rejection trees and then combine the results afterwards. Right? So so how would I partition up this 2,000,000 elements that cost me a couple of blocks?

How many threads do you have in total? How many threats do we have in total? Because the threat block, you know, kinda gets scheduled to an SM, and then we schedule more threat blocks than SM when it finishes. The amount of threads you're allowed to launch, in theory, it's almost infinite. So you're not limited to any number of threats or threat blocks that you can launch in this case.

Okay? It's just that the limit is that you have 256 threads per thread block, because you can launch any total number of threads or thread blocks that you want. So how would I partition this up? How many threat blocks would I need? Wait.

3,907. Wait. Hold on. Someone's telling me the formula instead. Right?

2,000 2,000,000 divided by what? Divide by 60. Two two million divided by 60. Divide by two. Divide by two.

Yeah. Right. You should divide by two. Okay. So so let's take a look at this one.

Right? So here we have four threads. Right? How many elements are these four threads processing? Four times two.

We're we're processing eight elements. Right? So so for my 256 threads in my thread block, how many elements are they able to reduce? 512. That minimum.

Five twelve. Right? So so we're missing one one operation in that formula that you're giving. Right? So so each red block is able to process 512 elements.

Right? We're able to reduce five twelve elements. So the total number of red block that I need would be my total size divided by five twelve, Right? And then the ceiling of that. Right?

You you guys follow why it's $5.12 and not 256? Yeah. Right? We're we're partitioning the problem. Right?

Not not necessarily the threads. I I mean, in vector add, you know, it was a one to one mapping between what we're processing and the threads. Right? But in this case, it's not a one to one mapping. So so, really, it's, how many direct blocks can can, you know, process how many elements, and then we divide that element by the total elements.

Right? So so in this case, it's 2,000,000 divided by by 12, and that that's how many threat blocks we're gonna have. And then this is valid nowadays in in modern GPUs. Like, ten, fifteen years ago, that number of threat blocks, the hardware wouldn't support. I think there was some driver limitations or whatever.

But nowadays, you can launch almost infinite number of threat blocks. Right? They just get scheduled and context switched out in the hardware one by one. It gets serialized. Okay?

So so that's what we're gonna do. Right? So out of this, you know, giant 2,000,000 array, we're gonna partition it into, you know, this number of thread blocks. And then within each thread block, we're gonna perform a reduction tree. Right?

So this is how your assignment is is as well. And then at the end of this, thread block, right, we have something like a I think we call it a partial sum. So we want the full reduction. We just reduce all of those partial sums again afterwards. Right?

So we just take this, and then we run another reduction algorithm on it. But for your assignment, this is essentially where we stop. Right? We don't do that final reduction step, okay, for your assignment. Right?

In real life, if you were to implement a library that implements reduction, you would you would reduce it again in real life. Okay? Alright. So so at a high level, that's how we're we're partitioning the problem. Okay?

Any questions? No? Yes? Alright. Do I have to worry about boundary conditions in this case or in general when I'm partitioning?

Well, there's an odd number then. Well, I guess so. Yeah. There we go. 2,000,001.

I mean, that's guaranteed to have some kind of Sure. Yeah. So so that mean, like, this this very last one. Right? You might not have a full tree to reduce on.

You know, the simplest solution there would just be to just pad it all with zeros. Right? If it's if it's sum, right, then you just do a reduction tree of zeros, and then the rest of your code could be the same. That that's, like, the simple approach. Right?

If this looks like a a max, you would just pad it with negative infinity or something. Right? So so that's, you know, one simple way to handle these type of things. Alright. So so now okay.

Within each of these thread blocks, this is how your reduction is gonna look like. Okay? So using this example with the four threads again, right, you know, we have this input, you know, n of eight. Right? So so from the for for these eight elements, right, how do we map map a thread to that eight elements?

So the way or the approach we take is, essentially, every thread maps to an element that they're responsible for, and then they add another element that they're, you know, reducing. Right? So so in this case, right, the the element that each of these are responsible for, would be you know, thread zero is responsible for end of zero. It's kinda like their home index. You wanna if you wanna think about it.

Right? This is n of two. Right? This is n of four. This is n of six.

Right? So so n one, n three, n five is here. Right? And then n seven is here. I'm lazy.

So so that's kinda how we do the mapping. Right? And then what we are doing is, essentially, we're doing a in place reduction. So so n of zero is gonna be added to n of one, and then we're gonna write it back to n of zero. So this is essentially overriding this location.

Okay? So so this operation is is in place. Right? We're not making a new variable here to store the next step's result. Right?

We're not doing, like, a double buffering, for example. Right? You know? So so we're basically doing a reduction in place in the same array, in the same end that I'm that I'm processing. Okay?

So, essentially, right, the way we're gonna, do this reduction tree is that for, you know, at every single step, you know, we we do a reduction operation. And then for the next step, we do another reduction operation, in this case, of my home location and another element that's a bit further away. Right? And then at every step, you know, that location that I'm adding is is another step that's further away again. Right?

So so we call this a stride. So, you know, this is this is a stride, you know, of one. This is a stride of two. Right? So the stride here would be four.

Right? If this algorithm is larger, right, and I continue, my stride next would be Probably eight. Eight and so on. Right? So so a lot of times with these algorithms, if you're just gonna sketch out how you want your data and threads to map out, right, there's patterns that you can kinda figure out and, you know, how you could code this, essentially.

Right? Alright. So so this is, how how my all my mappings are done. Right? Thread to data mapping and the operation to data mapping.

Do I have to worry about any other issues here? Is there any other potential problems that I have to account for when when I'm implementing this reduction tree? Need to make sure that all those steps previously or all the dependencies are are configured. Right. Yeah.

So so we have a data dependency issue here too. Right? So so when I'm on step two, right, and I'm doing this add right here. Right? So where's the data dependency that I have right there?

Is there is there some kind of readout to write dependency? Right? Yeah. Two two and two three is Right. I'm I'm waiting on, this result and this result.

Right? So so I'm at thread two. Right? So if I'm at thread two and I need this result, do I ever have to worry about that being a data dependency? Do I ever have to enforce that?

No. Why not? Because it's gonna get scheduled after that. Yeah. Yeah.

It's it's my own computation. Right? So so if if I'm if I'm here, I'm guaranteed I already completed this. It's my own thread. Right?

But if if I'm at step two, is it guaranteed that this thread was already complete beforehand? Potentially. Right? If it was in the same warp, yeah, but, you know, my algorithm is spanning a whole thread block. Right?

So, for example, my this thread that I need to get the data from could be in a different warp, and it could be a completely different program counter. So so this one, I'm gonna have to make sure that, you know, add the enforce that this is complete before, before I can move on to this next step. Right? So for those of you who are CS in computer engineering, right, how do you enforce and make sure something was already complete before this line of code? Right?

Yeah. Would it be a a lock? Locks is for, like, a critical section. Right? Right.

So so a lock is, like, when multiple things wants to access the same thing and change it. Oh, condition. Right. So this would this is not really the case. Right?

So this would be, like, a synchronization or various synchronization. Okay. Right. You you learned you learned barriers before. Right?

Yeah. Like barriers, semaphores, locks, and all these stuff. What is that c s that c s 10 c? What what class do you teach then? Which one?

One fifty three OS because, like, you're talking about Okay. Wait. That's when you learn all the stuff? 10, like, blocks. Oh, you don't learn it in any of the programming classes?

Oh, that's, like, only a One sixty? Okay. But that's, like, a tech elective too. Oh, really? You don't learn any of this stuff in the core programming curriculum?

I mentioned, like, maybe, like, a flag or, like, like, a bit. Uh-huh. Oh, interesting. Oh, I did not know that. Okay.

The core curriculum is all, like, synchronous and, like, a single. Right? Yeah. Okay. Okay.

So, like, we started learning Java. So we learned c, when I was a undergrad, right, back in the early days. And then c plus plus, and then we did nothing but Java. And then we took operating system, which was c, which is, like, your senior year. So, you know, we learned all the hard stuff like pointers and then we got done because we forgot what a pointer was with Java.

And then we took operating system, and they were like, oh, no. Pointers again. Right? So we had to relearn it. But but Java, we were we're already implementing, like, multithreaded Java programs and stuff as part of our core programming curriculum.

And then when I when I was a grad student, I TA'd for, like, c s 10, you know, equivalent a. And then we started introducing parallel programming, for freshmen already. Right? So, yeah, I'm surprised that you guys didn't do any of these stuff. But, anyways, it's not like the world is parallel anyways.

Right? So so you can literally go through and then, like, not get exposed to any of this. Alright. One fifty three is on. Okay.

I'll I'll I'll OS is core. Yeah. I guess so. But that's, like, not the best time to learn all those stuff because OS is hard without all of this. Okay.

Anyways. Right? So so you had to do some kind of barrier synchronization. Right? So in between every layer, right, we need to do a synchronization, do a synchronization.

Right? So that's something else that we would have to put in to enforce the data dependency. Okay? Right? So so more or less, right, that's kinda how this algorithm would look.

Right? Somehow, you had to get all these mappings, iterate over number of steps, control the stride that we're adding across, and make sure we synchronize in between. Okay? It's actually not that hard. Believe it or not, it's, like, three lines of code or something like that.

Yeah. So so GPU programming is not something where you could measure productivity by lines of code. It's all in the clever indexing of things. Okay. Alright.

So so we're gonna look at how can we use shared memory and then the actual implementation of this reduction tree? Again, like I mentioned, it's, like, literally a few lines of code. Actually, it's four. Four loop sync if this. And then this could be one line, I guess.

That would compile. So it's three lines of code to implement that that crazy reduction tree. But, again, we're gonna analyze it right, to see the program behavior so we can see how the data flows through this algorithm, right, using your Naruto skills. Did you guys grow up with Naruto? Yes?

No? Okay. This is, like, when you were, like, seven or kids. Right? So I don't know.

Okay. That's when you guys okay. Yeah. Yeah. For me, it was, like, middle school and and high school when that came out.

So okay. Wait. What age do you guys watch Naruto? My my son is five, and I tried to show it to him. And then I didn't realize how graphic it was.

It wasn't when I remembered it to be. Okay. Seven. Go seven. Yeah.

My son's still so young. Talking about how to do. I think so. Yeah. I know.

Right? I'm checking. So Okay. Yeah. So so we're gonna look at how we could do the the caching Again, this is really because of the the in place behavior of this of this, algorithm.

Right? So so let's take a look at, the the shared memory component, how how shared memory can help the the reduction algorithm Right? So so going back just to this picture again. So, you know, this is doing a in place, in place reduction. Right?

So we're gonna, you know, add these two and then, you know, get the output into here. Right? So for every single one of these reduction operations, how many how many read and writes to memory am I performing? Two reads and one write. Two reads and one write.

Okay. So what am I what am I reading? Yeah. The three and the one. Right?

So so these are reads, and this is a write. Okay? So if we if we don't use shared memory, right, and this whole array n of whatever, right, is in your global memory, you know, there's gonna be two global memory reads and one global memory writes that's gonna happen. And then my next step, right, I have to wait for this write to complete, and then I do another two reads from global memory. And then I write again.

Okay? Hopefully, you know, it'll realize that, oh, there's some reuse here. Right? I'm reading and writing to the same location over and over again, that that home location index that I'm responsible for. So, hopefully, my hardware caches that, and then that one don't don't have any penalty.

Right? Memory access penalty. Or I know the memory access pattern. Right? I'm the programmer of this algorithm.

I know more than the compiler and the hardware, so why don't I manually cache this myself? Right? So instead of having two reads and one write to global memory at every iteration, it's much better to have two reads and one write to shared memory. Right? They'll be significantly faster.

Right? So not only reducing the number of steps. Right? I'm also reducing the the time I'm stalling waiting for memory. That's also gonna make your algorithm faster too.

So so we're gonna try to essentially cache all of this data into shared memory and then perform the reduction in shared memory, essentially. Okay? So so from a high level point of view, right, you have this, you know, giant array n. Before we said it was, like, 2,000,000. Right?

So within a a threat block, what ends up happening is that, you know, we're gonna allocate space in shared memory and then we're gonna transfer the data that I'm responsible for over. Right? We're gonna transfer the data that's into shared memory, and then I perform a reduction in in shared memory. Right? So that's essentially, what what I'm trying to do Right?

So the way I allocate this shared memory buffer essentially is going to be, the sign of code right here. Right? So I put this underscore shared in front. This is a floating point. Right?

It could be int or whatever your your data is. We call this the partial sum, and the size is two times block size. Right? So so why is the size two times block size in this case? If each thread in the block operates on two elements.

Yeah. Right? So before, the example we showed was that this is 256 threads. So they're gonna operate on, like, 512 elements. Right?

So 512 elements is gonna be operated on. So the size of the buffer is essentially double the number of threads that I have. Right? Every thread operates on two pieces of data. Right?

So it's two times block size. You know, and block size is basically just blocked in. Right? I guess block size is the finest five twelve maybe statically in this. Right?

Or you could just do block dip. That works too. Okay? So so the other thing is right now now that I allocate it, you know, 512 in my shared memory here, right, I have to essentially now actually move the data. Right?

I have to perform that caching operation, right, if you want to think of it that way. So so I'm gonna have to move 512 elements into that buffer. So how would you perform that operation? How many copies, you know, does each thread have to perform? So so how many how many pieces of data do I have to cache in this scenario?

I have 512 data that I have to cache. How many threads do I have? Two. Right? So each thread is responsible for caching two pieces of data.

Okay. So how do you actually move data from shared from global memory to shared memory? Mem copy? Probably global to shared memory. No.

Mem copy is a is a CUDA API on the host. Right? Yeah. Right. You just read from a global memory variable, and then you write it, and then you you, yeah, you write it to a shared memory variable.

It's not gonna look anything fancy. Right? It just looks like you're assigning an element from an array to an array. Right? So so this one is basically doing a data transfer from from global to shared.

Right? So that that actually performs the the data movement. Right? So like I said, you're just gonna see pieces of code, but by the end of this class, right, you're gonna see the data movement flow through this code. Right?

Yeah. Could you, like, clarify, like, the cache movement? Like, we're caching the value of the index. No. What what you're actually the actual values.

Yeah. Right. Yeah. Yeah. It's because, we do in place reduction.

Right? So so actual values, I'm gonna read, modify, and then write it back to global memory. And then I do that every every time. Right? So if I'm doing that in global memory, that means every reduction operation is waiting on thousands of cycles to read and write, which is very slow.

It was like if you wanna call into it, you're gonna tell them to pass it. Yeah. Right? Yeah. Essentially.

Right? So so if I know I'm gonna reuse it, it's really more that I know I'm gonna reuse it in the future. So I could just move all the data into shared memory, and then I do, you know, two reads and one writes to shared memory, which is, like, five cycles. Right? So so that's gonna be much faster than thousands of cycles.

Right? So so we're gonna, you know, move all of this 512 elements into this 512 shared buffer. Right? So this is my shared, buffer of 512. And the way we we do that transfer from global to shared is it it just looks like a regular piece of C code.

But underneath the hood, this is actually some data movement that's going on between two different memory spaces. Right? You know? So so input is coming from, I guess, you know, the that's that's the function parameter. Right?

It's something that you you previously could have mem copied from from the host. Right? So that that's what this, this global data is. And then this partial sum that I I declared right here is in shared memory. So this is an this is an assignment, essentially.

Right? So this is gonna read something from global memory and store it into the shared memory. And then I need to do it twice because I have 256 threads, and I need to meet and I I need to need to move five into 12 elements. Right? So each thread moves two elements.

Okay? So so this is how we do the two business. Right? There's a lot of ways in how you could perform this operation. Right?

The way we do this indexing, we won't talk about for a couple of weeks, but but this is actually a lot more efficient to the to the GPU memory system. Right? But, hypothetically, you could also implement this in different ways. Right? So, for example, if you wanna think of it so let's say, you know, this is my input.

Right? And then this is my partial sum. Okay. Right. So so we have a whole bunch of different elements in here.

Right. Right. And each thread is responsible for removing two pieces of data. Right? So if I'm just using different colors, you know, thread zero, which two pieces of data should I copy hypothetically?

Contiguous ones. Right? Even the same amount of data is actually. Something like this? Right.

Okay. We're gonna run with this example. Right? So thread zero would be responsible for these two. Right?

It's not wrong. You could actually implement it this way. So thread one will be responsible for mapping this. Right? And then let's say thread three is responsible for mapping this, and I have, let's say, thread n.

Right? It's responsible for mapping. I am not trying this to scale. Right? Mapping this.

Right? So so let's say this is the memory access pattern. Right? This is just an exercise. Let's say this is the memory access pattern that I have.

Okay? Yeah. So this is my memory access pattern. Right? I have two pieces of code that does two data movements.

Right? So how would I write this code that performs the data movement for each of these threads? These are So what about the indexing? So so so a huge part of GPU programming is just making sure you have the indexing correct. But then not this way, you have.

No. It's not. It's not. Do it? Yeah.

Just just to be clear, this is not what this access pattern is. Okay? I'm glad you mentioned it. So now we could do something different than what's on here. Yeah.

Wouldn't it just be like, your offset would be two times two and two times two plus one. Right. Yeah. So so every thread would start from a times two. Right?

So my thread ID times two would be, my partial sum, I guess. Right? So I guess it would look something like partial sum two two times t? Yeah. Yeah.

Okay. Sorry. I'm gonna have to erase this part here. Right. Equals to input of two t.

Right. Yeah. Okay. So this is for the rep lock. We have to generalize it for different rep locks later on.

Right? Okay. So I can do that. Alright. So the next instruction would be so what would this copy be?

2TPlus1. Right? So same thing, but 2TPlus1. Oh, sorry. That's a one.

Okay. So so that that would be true for for this rep block. Right? So for the next rep block Would would this indexing still work for the next rep block? We need to offset it by We need to offset it.

Right? Because because for this if I use this piece of code, right, threat block zero would be caching this piece of data. Threat block one would still be caching this piece of data again. Right? Even though, technically, I should be caching this piece of data right here.

So how would I make this code generic for different thread blocks? block, I don't have five twelve by block there. We we shift it over by five twelve. But Yeah. I had to shift it over by by what?

Because, the the block side is going to be safe. Right? Opening 512. Yeah. Shift over 500 or one.

Exactly. Right. Yeah. So for every every block ID, you have to shift over by 512. Right?

So that's that's what the start index variable is. Right? So two times block dimension gives you the five twelve, and I shift it over by whatever my my block ID is. Right? So this start gives you that that start offset.

Right? So this start offset, kinda gives you that the offsets of of where I start copying from. Right? So so how do I modify how do I modify these two physical to incorporate that offset? Do I have to add the start into my partial sum index?

No. Right? Because because partial sum is local to our thread block, I don't have to offset that. So I have to offset my my input. Right?

That's plus one. Plus start. Right. So then that would offset it, essentially. Right?

Okay. So so then this would actually work. Right? So this would perform this behavior right here. Right?

So if you write your code like this, this is perfectly fine. It's gonna work functionally, and your code will pass on on GitHub and everything else like that. Right. But what? I don't know what I what what what the bleep is this?

Right? Okay. So you you you mentioned that, we should move contiguous pieces of memory. Right? So this is kinda hinting at, other optimizations that we could do later on.

Right? So, ideally, just like with warped divergence, right, we want neighboring threads to do the same thing. In the memory subsystem for GPU, we want neighboring threads to move neighboring pieces of data. Right? So with this access pattern, you know, is my neighboring thread actually, the the thread ID is wrong.

Sorry. Right here. That should be two. Right. Okay.

Are my neighboring threads moving neighboring pieces of data? What's the neighboring? Like, index zero one two three. Contiguous pieces of memory. Oh, yeah.

I guess not. I guess not. Right? Because because we're not we're not performing each of these thread at the same time. Right?

So so when when when I run the instruction, I'm I'm performing, you know, every other memory location. Right? And then my next instruction runs every other memory location. So this is actually very unfriendly to the GPU's memory. Right?

So so what would a more friendly memory access pattern look like if I want my threads to move a neighboring piece of data? Contiguous neighboring pieces of data. Zero thread, we get the element. The thread, we get the element, and so until you Right. So this element of this Then you get to the halfway point, and then you start with.

Yeah. Dot dot dot dot dot. And then I have my n. Assuming this is my halfway point. Right?

So then and then I and then I repeat, halfway. Right? So then this would be purple. This would be orange, and then whatever the last one is is in black. Oh, I was off by one.

Oh, well. So on like that. Right? So so then just say when when all the threads run that instruction, they're moving into your pieces of memory. And then when I do the next one, I do another continuous piece of memory.

Right? So that's what this this piece of code is doing. Right? It it's functioning at the end. You still get the same result.

Right? It's just we're changing the ordering in which we're accessing it and and touching a memory. So this is a lot more memory friendly. So that's what this piece of code is doing because it's a more GPU friendly way of doing things. Right?

So so your t so that means, you know, for this step here, right, it's just basically a one to one mapping of t to you know? It's just a mapping of t to speed. Right? Of course, with that with that start offset for a different block ID. And then for this next one, everything just shifted over by your block size.

Right? So all this index is shifted over by block size. So block dimension plus c, and then we just add block dimensions as well. So so that's that's what that's performing. Right?

So, see, how does this offset block dimension and block dimension? Everything else is the same. Questions? No? Right?

Okay. Yeah. We'll talk about it after your midterm. There's, oh, that's it's, yes, the memory coalescing. Right?

It's the way the DRAM works, and there's a hardware unit that does coalescing. They call it coalescing. It it merges neighboring data transfer into one, data transfer to to save bandwidth, essentially. So only within a block or only within a warp? It's within a warp.

Right? And they size it in a way where each coalesced access fits into, like, a line in DRAM, essentially. Right? So so it goes it goes deep into the memory architecture a little bit. So so we'll talk about that too.

We'll we'll see transistors and capacitors circuits for that lecture. You you people are like, yeah. Awesome. I could use my one six eight knowledge for that. Right?

You took one six eight? Anyone? Took no? No one liked that class? That's that's the VSI class.

Right? No one took it. No one no one built, like, the register before or something. Okay. Whatever.

Yeah. So so that's that's essentially how we do the the data movement here. Okay. Alright. Now that I cached everything, now I could do the actual reduction.

Right? So it was just like two lines of code, but there's a lot going on behind those two lines of code. Right? Okay. And now this whole reduction tree is really just three lines of code.

Okay. So so, you know, we we mentioned before, right, how we would approach this problem. Right? There's gonna be, you know, some kind of loop that goes through each of these steps. Right?

There's gonna be something that does this indexing. I shouldn't use black. That does this indexing, right, between this and this, this and this. Right? So there's a stride that's gonna double at every step.

There's gonna be a synchronization in between every step. Right? And not every thread is active, so I'm gonna have to control which thread is active at every step as well. Right? So if I'm if I were to implement that, this code should have some notion of of capturing that type of behavior.

Okay? So so, you know, the steps number of steps is clearly a for loop. Right? So this for loop iterates, you know, over every single step. This code kinda gets clever because it also encapsulates the stride along with it.

Right? You could do for example, you could do, like, four log of n steps. Right? And then you can handle the stride later independently. You know, that's valid code.

That works too. Right? So this is just, one way of running it where you kinda encapsulate that stride behavior in in your for loop. Okay? So so in terms of the stride, right, the stride starts off at one.

We saw the next step is two. Then the next step is four. It doubles every time. Right? So if I initialize my strat at at one and I double it every time, this code essentially terminates when my strat is at the maximum.

Right? So the strat is at the maximum basically when when my my thread is when my strat is the the block size dimension. Right? If you wanna think of it, right, I have ten twenty four elements and and oh, wait. Hold up.

The example we were using is we performed 512 elements on 256 threads. Right? So so out of this five twelve right? I have two fifty six, two fifty six. Eventually, my last step will be like this, right, across wait.

I'm trying to draw it to scale. It'll be this element and, like, this element. Right? My very last step will be between those two. Right?

The the halfway point and and and the point. Right? And that that's really the the total number of threads that I have. Right? If you do have reductions for you.

No. You get confused. Yeah. So I have a question. So why do we write, like, directly downwards instead of, like, writing, like like, for example, thread one writes to, like, the same It it writes in place to itself.

Yeah. Instead of, like, writing to something that would be next to the one that's left and the next one. Like, why does the seven go there instead of one slot to go left in the Like, is seven here? Yeah. Like the other one.

This way? Yeah. Why wouldn't I write it there? Oh, oh, oh, like okay. So so you're saying why would I not write it like this?

Yeah. Yeah. Okay. Graphic. Would that be right after right after loop?

Yeah. Because because this is being read, and this is being written to in the same steps. But if it's in the same warp, then doesn't it mean that all the reads are synchronized? But there there's no guarantee that when that this is always within the same warp. Right?

Because this is running across the whole dev block. Yeah. You you yeah. You get the general sense of things. We'll see later on how we could play with the indexing to kinda do that.

Okay. But this is the naive implementation. It's really to demonstrate how how poorly it is in terms of work divergence. Right? Alright.

So so, you know, this this kinda handles the stride and the number of steps. In between every step, we had to synchronize. So we have this various synchronization API called sync threads. So this synchronizes all the threads in your thread block. Right?

So by the time we run this, we make sure that all of the threads finish from the previous step. And then the reduction operation, I think we saw before, is essentially, you know, two t and two t plus stride. Right? So this is index zero, two, four, six. So each of them has their own home location, which is two times t, the thread ID.

And then the you know, you add a stride, you know, at every single step. Right? So this is that stride offset. Right? And and, specifically, the one the line that controls which thread is active on that step, you can kinda clearly see it.

That's probably gonna cause the work divergence. Right? That's that's a control instruction. So so in this example, right, in the step, which threads are active? In the step, which threads are active?

So many I have four threads. Right? Four. So all threads, like, zero, one, two, three. In the next step, which threads are active?

Zero zero and two. Yeah. In this case, it's zero and two. And the and then in this one, it's zero. Right?

If we make this bigger, right, let's say I have 32 threads. Right? We might need to write this a bit longer just to see the pattern. Right? If I have only 32 threads actually, you know what?

Let's just keep taking this example. I have 256 threads. Right? Step in the step. Right?

Which which threads are active? 001. Right. 02255. Right?

Is is any of the threads inactive in the step? No. Right? We're we're all doing something. And in the step, which threads are active?

The the even ones. Right? Okay. How about now? In the step, which threads are active?

If there's a moment, I'll call it. Yeah. Right? So so, yeah, we're kinda, like, removing the ones in between. Right?

So, this would be mod four. Right? And then the rest the rest are idle. Right? And then if this is the step, which ones are?

Mod eight. Right? Every every thread. So I could generalize this pattern, essentially. Right?

So, essentially, it's essentially the stride. Right? Mod one, mod two, mod four. So the the the threats are active is this thread ID mod stride is equal to zero. So this will give us this this behavior right here, essentially.

Okay. So now that we know which threads are active, right, we could analyze this to see how this behaves from a warped divergence point of view. Right? So in the step, do we have any warped divergence? No.

All the threads are happy. They're all doing the same thing. In the next step, do we see any warped divergence? Yes. Right?

Contiguous 32 threads are not doing the same thing. Right? We don't see that. How about this one? Yeah.

We don't see any they're all warped divergent. Right? So so that means, like, from from, you know, after the step on, my my warps are becoming less and less utilized, right, to the point where, you know, you're gonna end up with a warp of only one thread running. Right? Even though I have lots of work to do.

Right? So so, clearly, this is gonna have a lot of warp divergence issues. Right? So in the in your assignment, you're gonna implement this naive algorithm. It's more or less copy and paste.

You You had to inject some instrumentation code that we have, for the assignment, and then you could kinda see the the distribution of of warped divergence. Right? And then we could analyze it in in assignment two. Alright. So we're gonna see how we could probably just play with the indexing now to kinda kinda make this a bit more efficient.

Right? Oh, yeah. So so, you know, we we introduced synchronization. Right? Synchronization basically just make sure all the threads wait for each other.

Right? If you this is a graphical view of things. Right? They call sync, sync, sync. We can't move on to the code afterwards until, like, the last one finishes.

Right? That that's essentially the concept of synchronization. Right? And then we already talked about partial sum. Right?

Everyone writes to a partial sum, and then we don't we don't reduce again. This is just kind of reiterating what we talked about before. So this is this is what our assignment is. Right? And then the the final reduction algorithm.

Right? I'm just gonna go to right, so so we already did this analysis. We just talked about it in the last slide. Right? So in order to basically avoid divergence, right, we want to play with the indexing.

So a lot of these slides I already covered just from my doodling, right? Right? We wanna basically clear the indexing in order to just avoid, that that Webpack mergence. Right? So so this is, I guess, what you were trying to imagine.

Right? Why can't we just write it to a different location? You were thinking from a memory point of view, but, really, it should be the thread activity that you're thinking of. Right? So So so instead of, like, every other thread becoming inactive, right, at every single step, ideally, what we want is to have my active threads be contiguous next to each other.

So that means we have to play with the indexing of things. Right? So something like this, memory access behavior and thread mapping behavior, would actually allow us to avoid warped divergence. Right? So if you think of it, right, in this step, every operation is still, you know, in place.

And I'm still I still have a home location. Right? So this is my home location, right, for for each of the threads. So in this case, right, my home location would be, like, n of zero, n of one, n of two, dot dot dot dot. It's essentially n of T, my thread ID.

Right? My my my operation for reduction, right, is now adding also a stride. Right? But in this case, the stride is different. Right?

So so in this case, what would step, what would my stride be? Four. Four. Or, generically, if this was n number of threads for the reflock, right, what would that generically be? I know what it would be.

The the stride? I'm sorry. N. N. Right?

Not no. Number of threads in my thread block, essentially. Right? Okay. So so the total number of elements I'm processing is two times rep lock size.

Right? Okay. Yeah. I had to keep reiterating this, though. Right?

So the total number of elements that process is two times rep lock size. So half of this is rep lock size. Right? Right. So this is blocked in.

Okay. So my stride would be what? Blocked in. Blocked in or or number of thread blocks, threads in my thread block. Right?

So so we're still adding, you know my home location plus stride which is thread block size right the next step I'm still adding home location plus the stride but my stride is changing again like before so so my stride now would be what in this case One size Divided by two. Right? And then in my next step, my stride is Half again. Half again. Right?

So so now you can see my my stride here is actually instead of going from, like, one to wrap lock size, it's going the other way now. Right? So so my my stride in this case is going from, block dim all the way down to one. Right? So so, you know, you could kinda see how you could change the for loop to do that.

Right? It'll still be the same number of steps. I'm just going in reverse directions, and it'll handle my stride computation. My reduction operation just has a different home location. Right?

But I'm still adding stride. Right? I still have that sync threads, but now the difference here is the if statement that controls which thread is active. Right? So so let's say we have that 256 thread example.

Right? So in step one, you know, all all my threads are active. In step two, which threads would be active with this type of behavior? 100 the 120. Yeah.

Zero zero one twenty. Right. So so at every single step, I set up half of the threads becoming inactive. Right? But now think of it like I'm just, like, left shifting all of the active threads.

Okay? So so if there's two fifty six, right, the threads are active with zero to one twenty seven. Right? And then in the step, which threads would be active? 0263.

0231. 0215. 272322, and then just thread zero. I'm sorry. That'd be one.

The one. Thread 0. Right. So 56789. Okay.

Right. So we can also analyze this from a warped divergence point of view. Right? Do I have any warped divergence in the step? No.

How about the step? No. It's a nice multiple of 32 still. Right? Still no?

No. No? Yes. Yes? Right?

Oh, no. I have both divergence. Still intact. Sad. Right?

But at that point, I only have 16 threads active, but I have avoided it anyways. No. Right? The the last the last five steps, you basically can't avoid. Right?

So this is probably the most efficient in terms of work averages that we can, right, that we can't do. Right? So then, you know, how do I make an if statement that controls the thread activity with this pattern? Right? So in this case, your thread ID you know, that if statement would be if thread ID would be something Less than I.

Less than what? Less than Red block size divided by two to the power. Or can it just be stride? Can I use my stride? Right?

So so you look at it from I mean, you can make a different variable, but, right, we could get clever and just reuse these variables. Right? So so my stride in the one's two fifty six. Right? My stride here would be one twenty eight.

My stride here is sixty four thirty two. It's just less than Yeah. Yeah. It's just less than stride. Right?

There. So that that's your solution for your optimized reduction. Okay. Yeah. Tell your friends not to look at this lecture.

There's too many answers in here. Tell them to use ChatGPT so I could file more paperwork to the university. I love paperwork. No. I'm just kidding.

Right. Yeah. So so, you know, just just by looking at this, right, it's just tweaking the way we do indexing from that previous for loop that we saw, and you have a very different behavior. Right? So we're gonna implement this in assignment two.

You know, with that work divergent instrumentation code, and then we'll be able to see how that works. Right? So I have the TA kinda walk you through the instrumentation code, in discussion section and how that works and everything. But, you know, more or less, you already have the tools necessary to to implement assignment two at this point. Okay.

Hopefully, just record it. Assignment one, all we had to do is, upload into the GitHub and the engine check bar alone. Yeah. That's it. Yeah.

Yeah. Yeah. No.


Okay. Oh, well, I guess the bell tower is late today. So okay. So we just got started. It's eleven anyways.

A lot of announcements today. Alright. So your midterm exam is next week? Yay. Okay.

Yeah. So we'll make use of the CS test center. I used it earlier this year for my grad staff, and it worked out nice. So I will continue using it for this time as well. So, possibly, you used the CS test center before.

Right? You used it for what? Yes, ma'am. Or what what classes do you use it for? There's a lot of Oh, a lot of classes using this.

Okay. So you're very familiar with this app. That'd be great. Yeah. So, the exam will be open next week, Monday through Wednesday.

Okay? So, obviously, it'll be open last next Tuesday. That that'll give you time to study. And you go on that link to make the reservation. I'll make an announcement after this class.

I'll post it as well. Right? So the reservation starts opening on Thursday, Thursday morning. Right? So you could reserve for any time Monday through Wednesday during regular business hours, and the room where some is, Winston Churchill Hall at 01:33.

Right? So the exam is eighty minutes. If you have accommodations or you plan on using, you know, the SDRC, send me an email so I know I need to get IP address from them so I can open it up on the exam. Other than that, you know, that's that's the main logistics of things. Right?

So it'll be closed closed closed notes. You can have, you know, scratch paper if you want, maybe with a calculator. But, otherwise, you know, you you just have to rely on what's in your head. Right? Right.

We didn't learn much in four weeks only, so you don't have to study. Just go in. Right? Alright. Any questions?

No? Yes. Give me, like, any coding questions. Like, coding questions? No.

I wish. If I could scale out how to grade that, I would fit in, like, a lot of coding questions, but I can't. Yeah. It's mostly kinda conceptual stuff. Right?

So so on Thursday's lecture this week, there'll be a mention review, and I did posted some sample problems. So if you go to page the plasma page, and you click on sample midterms oh, no. I didn't log in. Hold up. Oh, hey.

Happy to do this. Don't look at that. It's a password. Right? So, there's a sample midterm problem, so you can kinda get a few of the type of problems that we have.

And, also, for, you know, Thursday. Right? If you guys have questions or or concepts or topics that you want me to reiterate, you could post a question in in Discord that I know to cover it. If not, I'll just go over the sample exam on on Thursday just to kinda reiterate over some of the concepts. Right?

So so, you know, this kinda gives you a feel of some of the questions that you may have. Right? So, you know, there might be some questions. Yeah. Oh, shit.

Don't look at that. You may just have a. Right. And this causes it all reduction and whatnot. Right?

Actually, no. That's fine. Actually, no. Don't look at that. Hey.

You guys actually saw this before the homework is due. A lot of people will be watching this on Sunday probably. So Okay. Right. And, you know, there's there's other questions.

Some of these are maybe, you know, a little bit of coding, you know, fill in the line for indexing. Right? That's the extent of the coding that that you may have. Right? Okay.

And then other questions, conceptual, calculating number of blocks, whatever. Right? But, you know, as long as you understand the notion of, the the threading structure of CUDA, thread blocks, what a warp is, what warp divergence is, how how that occurs and why. And then, you know, the if you did your homework assignment with reduction and why not avoid, you know, warp divergence and so on, I think you're in pretty good shape. Right?

Yeah. All basic stuff. Right? Yes. Okay.

Right. So so we'll go over this more in detail on Thursday. Right? If there's more specific concepts you want me to cover, just let me know, and I'll cover it on Thursday. Otherwise, we'll just run through this and just kinda touches on a lot of the concepts that we talked about anyways.

Mhmm. Questions? No? Okay. Alright.

So that that's, Thursday. Okay. But, again, no no lecture next next Tuesday. So So most of you, hopefully, will will be able to study and concentrate on your midterm. I assume you have a couple of midterms next week.

Right? Yes. Right. So that'll be busy. Yeah.

Other announcements. Assignment two is due this Thursday. Okay? So, hopefully, both of you will complete on time. And then, you know, if you're allowed to read slip days so the latest you can turn this in if you wanna use it all right now, which I don't suggest, is Friday, Saturday, Sunday.

And then by Sunday night, Monday, I I I release all the homework assignment, you know, for us to generate, the answers. Right? So this way you can kinda check and and, get some feedback and whatnot. So that's Tuesday. And then, also, I'm gonna sign a final project right now as well.

Okay. So so it's like a bit about the final projects. And, specifically, the lecture from today and the lecture next week, we're gonna kinda jump up a bit and talk about more modern ways of programming CUDA. And and just the other, that's how the CUDA environment really is nowadays. Right?

So so a lot of the the homework assignments that we're doing in class, we're going at a low level and really learning how the GPU works and how we write kernels to to make the kernels more efficient. Right? So so your homework assignments, you know, one, two, three, four, will teach you all of these little hardware support that's necessary to write parallel CUDA code. But nowadays, in order to program GPUs, there's a lot more ways of programming GPUs and a lot of other modern features that are very, domain specific. Right?

Like, especially for machine learning work wealth and so on, and other optimizations besides in in the kernel. Right? So so we'll learn about different ways of on how to optimize GPUs and how to program GPUs. So that can also hopefully kinda give you an idea of, what other things you could do for your final project. Right?

So the whole whole goal of the final project really is for you to, you know, use the GPU for things that are more than what we learned in class for you to kind of get hands on experience on on actually applying it to more modern problems or more modern features of CUDA or, you know, a whole assignment or a project, your side project that you want to make faster, etcetera. Right? So so just to explore more on on the whole GPU programming environment and so on. Alright. So the final project is already posted online.

So if you go to the home page and click on the final project, this is basically the guidelines for the final project. I'll walk through it a bit right now. So it it's for teams of one to three students. Right? If you have a good idea of what you wanna do and you don't wanna work with anyone else, you know, you could do your own project if you want.

Right? Otherwise, you could team up with a team of two or three. Anything larger than three, I find it doesn't work very well. Right? All of you had group projects up until this point already.

I'm sure you had some horror stories about that too. Right? That prepares you for the real world, right, when you have horrible teammates in industry. So, yeah, you can have up to a group of three students. And, hopefully, by next Thursday, most of you would form a group and kinda converge on a idea that you wanna propose to me.

Right? So so by next Thursday, hopefully, you will have, you know, a team and an idea of what you wanna do, and specifically, just to keep communication tidy for me. Like, on Discord, there's a there's a forum discussion, so you just make a new post right there with the team members, the team name, your proposed idea. Sometimes we need new libraries or frameworks that we have to install on the server. Like, in the past, you know, we had to install PyTorch or OpenCV or other stuff like that.

Right? Those those should be installed right now. And other potential issues you may see, like, recent challenges. Right? Because sometimes some of the projects that students wants to do, like, for example, you know, the Rust language.

Right? It's kinda new, emerging. As one of the students wants to explore, what was the how would this, you know, support for programming GPUs in Rust. Right? There were a lot of risk and challenges on that.

That's it might just not work at all. Right? And that was kinda the case. It was it was, a lot of open source project that kinda got abandoned halfway and that they worked. Right?

But that in itself was a learning experience. Right? So so, you know, you could do risky stuff like that too. Right? And then, you know, kinda like a plan or outline of how you plan to achieve, achieve the goal of finishing a project.

So so once you have this, I'll make a private channel and a Discord with your teammates and and I and and TA so we can have, you know, private discussions. Because a lot of times, you might have technical support issues that you might wanna ask us or you realize, oh, we actually can't do what we propose on doing. What now? You know? Or my team member's not doing anything.

He disappeared. You know? Something that type of thing. Right. So so we keep it private.

Right? But but this this model now, you can make a post deck. Questions? No? Alright.

So so that's by next Thursday. So hopefully, you could propose a team, and converge an idea, by next Thursday. Right? In terms of deliverables, everything is due on finals week. So So our final exam is on week 10.

The last lecture of week 10. Oh, or the CS test testing center. Right? On week 10. And the project deliverables, the final project will be due on finals week.

So it'll be due on Tuesday, so this gives me a week to grade everything. So you have a a final project report, right, that describes, you know, what you did, the results. If you're doing, like, performance analysis stuff, right, maybe there'll be a lot of graphs on comparing the performance of CPU versus multi threaded CPU and GPU or your AMOL GPU if you wanna try that at home or Intel. And and there's there's no minimum length for it. Right?

Just however long it needs to be. You know? You don't need to make it 30 pages and just fill it up with random words that I have to read. That's wasting my time. Right?

I'll grade this by putting in a chat GPT and asking it for a grade. No. I'm just kidding. Yeah. So the the final report, right, should ideally have you some of these sections, you know, your overview, your project idea, how the GPU is used.

Right? So, for example, if you're doing a specific algorithm, how you pair, like, the algorithm, how you're partitioning it, how you're mapping the threads. If it's your own code that you're you're trying to accelerate, for example. Right? You know, same thing.

Right? You might have to decide on a lot of function that's paralyzable, how whether it's paralyzable or not, how you can partition it, and so on like that. And some details on the implementation. You know, all of this will also push your code to to get help people. Right?

The BMP will have classroom that output it as well. Documentation on how to run your code. You know, some evaluation of results. Hopefully, it works. And, hopefully, it's faster than the CPU.

You know, some problems that you face, you know, in in all cases, there will be some issues that you run into. Right? This is something new that you're exploring. You're expecting to run into issues. And also a table of if you have a team.

Right? A table of contributions from your teammates. Right? This way, it avoids students coming to me and saying, hey. So and so did nothing.

If so, hopefully, I wish you guys see, so that's why it's like that. Right? So Tommy and George and then everything didn't do anything for this group. Right? Yeah.

Slackers. Yeah. And then I'll I'll post a GitHub repo, because I'll just have to upload your final project when when I have it ready. So when this class was smaller, when I started it, right, this class was, like, 20 or 30 students. The whole would just be presentations.

That was really fun, but I can't do that anymore. We have 70 students. It doesn't sit out. So it, we'll basically just record a presentation and a demo, and then you upload it. I think you had to do this for, like, one twenty in the past too as well.

Right? Yeah. So we did something similar to that. This this goes pretty well. It allows me to see whether your demo works or not.

Right? I have people try to cheat and to to show that it works. It's, like, random cuts in the video, and then all of a sudden, it works. Try not to do that. Please be honest.

We'll catch that too. Right? You can upload that to some Chat two PT and analyze the video too. Right? Sooner or later, everything will be automated by Chat two PT.

I won't I won't even be here anymore. Yeah. So then yeah. Yeah, hopefully, just a presentation on what you did, how you implemented it, some results, maybe a demo of showing your code working here and so on. Okay.

Any questions? And that's also on on Zoom final suite as well. Okay. So so in terms of project ideas, right, there's a lot of different project ideas you could do. It's completely open ended.

I find under guys are a lot more creative of this, so you could throw out any stupid idea, and it will converge on something that's feasible. Okay? Right, but but some of the more basic things would be, you know, you take this about back propagation neural network, and you try to implement that and parallelize that from scratch. Right? Well, there there's there's serial code from, like, the nineties or eighties that you could serialize and and run, and paralyze.

Right? I don't know if I'll allow that anymore just because, like, Chat Sheet does a good job at that nowadays. And there's just so much sample code of that nowadays. But other stuff that's more unique would be, like for example, last year, our group made a molecular dynamic simulation, like, a three body problem type of thing. And then they were also taking graphics at the same time, I think.

So then they also did a open GL project that that visualized it. So if you're taking another class, it also has an open ended final project. Right? Some students use this class as another project, you know, making it and combining it. Right?

Like like, last year, we had one with graphics and and also with with the edge computing class as well. Right? So so depending on the the high class is it and what what your final projects are, it's possible to to, make it synergistic. Right? And then, you know, we could we could discuss that too.

What do you e folks? Right? You do a lot of fast forward air transport and signal processing. CUDA and GPUs work very well for that. So we have accelerated libraries like Ku FFT or or even FFT that you could call yourself or even MATLAB.

So, you know, you could accelerate, various signal processing applications that that you do. Right? So for example, one year, you you know auto tuning? Right? It's just a whole bunch of filters on signals.

Right? So they were able to accelerate that with CUDA and then basically make it real more real time, or something like that. So so FOT or, image processing pipeline. Right? We have an image processing course in in in EE, I think, right, where you learn various image processing technique.

And and sometimes they they use that and then parallelize that as well. Right? There's also a lot of other programming paradigms, and that's kind of what we'll touch on a little bit today. Right? So besides CUDA, there's there's other ways of programming GPU.

So before CUDA, OpenCL was kinda like the original way of programming GPUs for for, NVIDIA, Intel, AMD, everything. And now recently, there's things like OpenMP, which some of you have done for CPU. Right? OpenMP offloading works also for GPUs. Namba, c plus plus, which we'll talk about a little bit today.

Hip is is Andy's version of CUDA. And CUDA Python is is that recently released feature from from CUDA, like, NVIDIA, like, three weeks ago. Right? So NVIDIA released a native, iconic way of programming GPUs about three weeks ago using something called, like, tiles and crew tiles. I don't know what it is.

Right? But but that's something that's interesting to explore if you want to. Right? It's it's the very new feature set that that came out of, NVIDIA for programming natively in Python. We might have to install a newer version of CUDA but on the system, but, you know, that that's suitable as well.

Right? If you want something a bit more cutting edge, risky. Right? So some things from prior years, EE some EE folks, you know, in in MATLAB, you can actually write functions in CUDA and then call it in MATLAB. You know, there's some tricks to do that.

It wasn't very straightforward. Right? But you can write CUDA accelerated custom kernels and use it in MATLAB. Right? So something you folks at that, the time I thought this was actually.

Right? So that was a very interesting project to see. You can implement neural networks, so, you know, very basic neural networks. These are two high level, but you can use CUDA libraries, like, and actually, like, use that as Lego blocks to build the neural network from scraps, like, every layer by layer. Right?

Okay. So if you want to actually touch these this is old. Right? We don't use this anymore. We use PyTorch then and Jack's and other stuff.

This is actually interesting. This was the one in the project my time I was teaching it. They basically had three students, and they wanted to see what the programming effort and performance would be for programming neural networks. So, like, one one student had to code it and and put it in and what it generates to one in TensorFlow. This guy wrote, like, four lines of code.

This guy wrote, like, a 100 lines of code. And that was still faster. I felt bad for that guy. So, I give him extra credit for it. Yeah.

I don't know why you agreed to it. But, you know, one one interesting thing would be to, you know, explore different programming paradigms and do a performance comparison between it. Right? So, you know, at at the core of what we do in this class, right, it's all about performance and and balancing program productivity with accelerating your code. Right?

So so there's a lot of paradigms that you could explore. So, you know, other people did particle simulation or put an open GI. I have that already up there. Some people had a chessboard and did custom computer vision modules, on there. Right?

There's a lot of challenges in running these type of stuff, on the JSON board because it's so constrained. Right? So so, you know, it's kinda interesting to see because over time, this actually became easier. Like, in the very beginning, it was so packaged to do this and things are always crashing and whatnot. But the the support is much better now.

Right? So you could do things with different languages. One of I mentioned was that someone did a game project for the senior design project, and they did a procedure generation of maze, and that that that took, like, twenty minutes. So they accelerated that GPU, and it does it in, like, twenty seconds now or something like that. So that was actually, like, a very useful project for them.

And and you can port various units on projects to their GPUs too. Right? So someone last year had a group had a very heavy c plus plus project, and they're reporting it to GPUs using unified memory and native c plus plus. And, you know, you run into a whole lot of issues where stuff you do in c plus plus isn't natively supported on GPUs. And, you know, there's a lot of those technical challenges, but, that's a real world experience of applying, you know, GPUs on a real problem.

Right? That that gives you that experience. Right? And then I need to get a job because of that project or something like that. I forgot.

But yeah. So so these are some of the just sample projects. Right? But it's open ended. Right?

Just just throw out anything that you want, and we'll converge. And then we'll set up the server however you need, hopefully. Or if you have your own computer, you can use your own computer for the final project as well. It might be easier for you to, install any libraries you need. Right?

If your computer crashes, I can help you. You're CS majors. You should all know how to fix your computer. Right? Okay.

At least that's what my my parents my grandparents did. Okay. Questions? Yes. Yes, sir.

It really hurts my feelings that you constantly forget that you Why? You can fix the computer. But you keep on forgetting it. No. Wait.

How is this gonna be? This is about. But you can only mention CS changes. Yeah. You can fix a computer?

You break that hard. Did you give them actually credit for unity? No. Seriously. This this class started as a EE class.

Believe it or not. Right? So so the GPU course were were EE courses, and they got processed with CS. And, yeah, the EE folks have a much harder time because you guys don't know you don't program as much, then you guys ramped up a lot. But yeah.

Oh, the degree of difficulty, right, I would grade with that in mind. Right? I would grade a team of three more than a team of one. Okay. Just like, you know, it scales with the group.

And if you're e e upgrades you easier, more leaning away. Right? Yeah. Okay. Any questions?

Right. So so, hopefully, that's, that's by next Thursday. You could form a team and and conversion an idea. Okay. So the topic for today, we're just gonna talk about various more modern ways of programming GPUs and and what's out there and the features that exist.

You know, the the basics and the foundation that that what we learned. Right? So so all your assignments basically cover the foundations and basics of GPUs. From that, like, related, you have the foundations to understand and learn what these features are, essentially. Like, there's new features released every year.

I could literally teach another course of this just on advanced features, but, you know, I don't I don't have time for that. So we'll go over, three things specifically. So some of the new features in CUDA. So, specifically, I talked about CUDA 10 because I feel like that's when new features starts coming out that you guys can't understand. Like, if I just jump into CUDA 12 and start talking about things, nothing really makes sense, from what we wanna.

Right. So codec 10, I think, is is a round up number a bit. So how do you program GPUs in Python and how that looks like, and and also modern c plus plus. Right? In the basic c plus plus language nowadays, the standard supports GPUs.

Right? So that's a huge change from when I started teaching this course about, like, seven years ago. Right? So a lot of the slides from today, I'm just grabbing from various sources. So, you know, the the slides I uploaded is the full step, but we're not gonna cover everything.

Right? So this is just resources and materials for you, in case you wanna look into these, features. Alright. So is, CUDA 10. Okay.

Alright. Oops. How do I do full screen? Alright. So so CUDA 10 was released about six, seven years ago.

But, again, this was, like, around the time when machine learning exploded, and, like, every new feature set after that was very machine learning specific. But they all build off of a lot of these basic features. Right? So this was, like, a nice, bridge that I feel. The CUDA the features that got released in CUDA 10.

A lot of the new features is basically refinements of these. So around this time, right, when you guys were in high school? Right? Yeah. So this was, like, the state of art back then when you were in high school for CUDA.

Right? So So this was when, the new terrier architecture was released. That's when we started having ten six cores specifically for GPUs. A lot of interconnects were also designed mainly for machine learning training as well. And the various features, like, for graphs, also very machine learning driven and so on like that.

Right? So so we learn about what these features are. I believe our GPU support all of these features. Yeah. They do.

Right? So, you know, if you wanna program tensor cores, for example, right, that's something you could do. If you want to play with new features like like CUDA graphs, and whatnot, you know, those are stuff that that that's available on our system as well. Right? So we'll see what what all of these are and and different ways of programming.

So, you know, we'll talk about different frameworks and libraries, specifically the languages, you know, and then the rest of today's class. Oh, they're freeze. Okay. Alright. So so we have various libraries, just to kinda expose you again to the different type of things we have.

Right? So CuFFT, they have libraries for fast forwarding transforms. EE folks, you know, do a lot of fast forwarding transforms into applications of these. So around this time, you know, we started having multi GPU systems. So then the libraries got, developed into multi GPU.

Right? So so our vendor system has, I think, four GPUs. Right? So if you want to write a program that uses multiple GPUs, like, right now you could parallelize your parallelized code, right, parallelized code across different GPUs. Right?

You could do that as well. Right? You can see how you program multiple GPUs. So so libraries can support that natively, or you can do it in CUDA and implement that yourself. Nowadays, in more modern versions of KuFMT, this now scales across different clusters or different, individual server nodes as well.

Right? So the libraries are extended. And now you basically view, like, a whole cluster of of GPU data center. Yeah. Data center is not, like, one whole computer nowadays.

And and NVIDIA basically is trying to basically make an operating system for the whole data center. That's kinda what we're doing. Right? So cuBLAS is used a lot in, scientific computation and machine learning. Right?

This is basically the core machine learning. Everything runs through cuBLAS. That's why tensor score exists. Our assignment three is basically writing matrix multiply, which is your little light version of cuBLAS, essentially. Right?

So what you're doing assignment three is the core of all machine learning workflows. Okay? But this one, specifically, it's written by, like, one guy in the video who writes an assembly from what I heard. So so you can make a lot of money if you know assembly very well. A side story.

So I I I I did an internship. I knew a guy who was at Georgia Tech. He wasn't doing so well in his PhD, but, apparently, he was, like, really, really good at writing assembly code. So he joined this team at, I think, Facebook, the PyTorch team, I think. And, you know, Facebook had a team of, like, 10 engineers trying to optimize this code, and they were getting, like, 10% increments here and there.

Right? And then this guy went in and just wrote an assembly code, make sure everything, like, fits in cash and the register and all the levels of the cash, kinda like what you'll be doing, in assignment three. And then he got, like like, a two x improvement. Right? Everyone you know, this team of 10 professional engineers was getting 10%.

But then after that, like, Google and Facebook started getting a bidding war on him, and and then Harry's starting salary was, like, over 500 k. Yeah. And then he got his PhD after that. Right? So if you if you have a very niche of the bill, it pays a lot.

Right? That's why CS 60 '1 was important. Well, it's too late. '61 is over. You probably gonna buy everything.

But now now is a good time to go learn 61 again. Right? So, you know, just to kind of breeze through a lot of these data libraries. Right? You have solvers for numerical analysis.

If you take, like, scientific computing, maybe you you may do some of these analysis stuff. Right? If you do image processing, right, we have decoders and encoders in in in, Ankuda. Right? So some of those the color space, color space conversion, that code that we saw, like, in in week two, right, that that in terms of image of the RGB.

Right? So those those libraries that accelerate that and then encapsulate that feature set as well. Right? In terms of performance analysis, in our assignment two, right, we have those custom code that we have to insert to read, like, the special register that tracks the work divergence, right, that you guys have to do in your assignment two. I see some lost faces.

You haven't done assignment two yet. If you went to discussion, you saw it in discussion. Right? Yeah. Okay.

It's a in assignment two, trust me. You copy and paste that somewhere to profile the work divergence. Right? There's a software way of doing that. There are also tools to do that as well.

Right? So so we have, like, insight system, which profiles a whole bunch of data and gives you, like, this waveform diagram. Right? EE folks would would get, like, saw all your waveforms from, like, circuit analysis and and stuff. Right?

Ugh. Fun stuff. Right? So you could do that too with your GPU system. Right?

So you can see all the activity that goes on all the data transfers and stuff like that. And you could analyze your your final project if you want, right, to see where you can improve things or where the bottlenecks are and whatnot. So there's a lot of tools that exist on our system, like, inside compute and whatnot. That tells you a lot of this information. Right?

Like, inside compute would tell you specifically, you know, for every line of code, potentially whether it is a work that reaches issue or whether you're accessing memory in in an inefficient way and so on like that. So some of the profiling code that we're doing in our assignment two already existed here, as a as a profiling tool, essentially. Right. And there's a lot of other things that they could collect here as well. Right?

So so these are some of the things that you could do. If you don't have if you try to run this on vendor and you don't have permission, I think we just have to add you guys to a specific group, and then you have permission to do this. Right? I I think, it's a security issue, so there's there's a specific permission that you might have to use for profiling. You you know professor Abu Ghazaleh, the guy who teaches operating system?

Right? So these tools used to be completely open, and anyone could use it. And, you know, he does research on security. So so a couple of years ago, his PhD student, they found out that you can exploit these counters to do side channel attacks, and then they report it to NVIDIA. And then because of that, specifically, they clamped down on all these counters.

So now no one could use it. You need special information to use it. Right? So so that's that's research having a real world impact. And so we used to use profiles in my class, but because of him, I can't.

Right. It it became too messy to give us, you know, all these permissions. Alright. So they're programming models. Right?

Besides CUDA, right, and and the CMT threading model, there's other ways of thinking of how you could program a CUDA program. Right? So you could think of CUDA also in terms of task graphs, because a lot of the programs that you write, you know, consists of a lot of different functions. Right? And not all of these functions necessarily run well on a GPU.

Right? So most of your most of the time in the in the real world, your code consists of CPU components and GPU components and data transfer movements and stuff like that. Right? Or if you have multiple different GPUs or different CPUs. Right?

You can actually program your your workload as as a graph of tasks. So this is a very popular programming model nowadays in in the high performance computing space. But so CUDAGrass allows this and, you know, Rogers and Coco's other stuff as well. And other other improvements here. Right?

So specifically with CUDA 10, this is one of the biggest features that they created. Right? A lot of functions like deep neural network training, machine learning workloads, HPC simulations, linear algebra. You could decompose them into a whole bunch of functions, a graph of functions calling each other. Right?

It's a car graph. So in terms of GPU, right, you can actually partition this across different we we didn't talk about streams yet. This is the topic next week, actually, after after finals. Right? But you can create in class next week, we'll see how we can populate different streams.

Streams are, like, queues of operations. Each of these are kernels that you create. Right? So instead of just having matrix multiply, right, you can have, a whole series of CUDA functions that that performs, you know, operations in the pipeline or or in a branch. Right?

If they have dependencies. So this allows you to make your kernels run-in parallel as well. Right? So we'll learn about that. Typically, in order to program this, it's very difficult.

So, you know, they create, CUDA graph as a way to natively express graphs of task. Right? So so a graph can be a kernel launch. It could be a CPU function. It could be any type of memory operation or a graph or a node could be another subgraph, for example.

Right? So if you think about it from, machine learning training. Right? Your neural network is is static. Your neural network architecture doesn't change.

Right? So when you when you do an inference, you're always gonna call that same order of kernels over and over again. Right? So you can represent an inference path of a neural network as a graph, and and you could compose it as a CUDA graph, essentially. Right?

Not only that, if you know you're gonna run that graph over and over and over again, like we do with machine learning training, like, every iteration, we run that graph again. Right? You can just launch one graph instead instead of launching one kernel. Right? So, basically, every time when we launch a kernel, there's a lot of overhead involved in launching a kernel.

So if a if a inference passes thousands of kernels, right, you incur thousands of kernel launch overheads. But if you encapsulate as a kernel graph, as a CUDA graph, you only incur one launch overhead. Right? So that was one of the reasons why CUDA graph was invented besides, from a programmer productivity point of view. It reduced the overhead of kernel launch a lot.

Specifically, nowadays, PyTorch, TensorFlow, JAX, whatever, they all integrate CUDA graph features into, into your machine learning, inference servers, and so on like that. Right? So so, specifically, yeah, like we were mentioning. Right? There's a lot of these these graph kernel launch overhead set that we try to avoid.

So this way, it it frees up your g your CPU for other stuff, and and your GPU becomes a lot more utilized. It's not always waiting on the on the CPU to give it stuff to do. Right? So so how this looks, I'm gonna skip some stuff, right, just because I wanna cover other stuff too. Right?

Yeah. This is a more native way of programming a graph from scratch. Right. And it it is very expressive. Right?

So you do CUDA graph create, CUDA graph add node. And then when you add a node, you specify the dependencies. Right? So in this case, right, we have, you know, kernel a. Right?

B is dependent on a. C is dependent on a. D is dependent on b and c and so on and so on and so on. Right? It's a very expressive way, a natural way of of expressing graphs.

Right? So you can now imagine that if you're coordinating work across different GPUs. Right? For example, you know, you could launch a graph to different GPUs or, you know, you can launch a function on a CPU and a GPU in order to do header change processing, for example, if you want. So and then you can launch the graph many times or or however whatever you do afterwards.

Right? So your graph could also a node could also be a graph, and that's that's what happens a lot in in the machine learning frameworks, because a lot of these subgraphs are very static. Right? Or for a heterogeneous processor as well. Okay.

Besides CUDA graphs, right, another thing that was released I'm not gonna cover this. Right? It is it's, basically, tensor cores. Right? So with the Turing architecture that came out, right, they they have a tensor core and also and this architecture specifically, it also have a ray tracing core.

So if you took graphics, you implement ray tracing? Yeah. Yes. You do. Right?

Right? The algorithm for ray tracing, you can actually offload to a specific ray tracing core. Right? And it and it does hardware accelerated ray tracing computation. Nowadays, right, we could repurpose this ray tracing for it to map, like, various tree based algorithms.

Right? Because ray tracing is kinda like a tree algorithm type of thing. Right? No. Yes.

Yes. It's a yeah. It count that. Right? Yeah.

Yeah. Right? So you can map, like, tree based algorithms onto tensor cores in order to accelerate it from the compute point of view. And, also, you know, we have tensor cores now. Right?

So tensor cores, you know, tensor is a n dimensional matrix. Right? So, basically, if you do matrix multiply, you can map that directly onto a tensor core, and it'll be accelerated and faster. For assignment three, we're not gonna use tensor core. We're just gonna use the full integrated units.

But but nowadays, you know, everything uses a tensor core. In fact, the tensor cores got, like, beefed up like crazy in in the newer GPUs. Newer GPUs are basically tensor cores with a GPU wrapped around it nowadays for for machine learning workloads. Not not the gaming side, but on, like, the data center side for machine learning. Right?

So it's kinda interesting to see how those, DPs kinda diverge. Alright. So, you know, test scores are units that that you had an offload computation off to. So there's, like, specific, formats and sizes of the matrix that you had to break it into in order to submit it to that test score. So so there's there's ways to to basically, program that as well.

We will we will cover that. So if you're interested in that, I can give you a lot more resources on how to program test scores and and guides on that. Right? So so so this, NVIDIA presentation doesn't cover that. Is there anything else I wanna cover?

The oh, cutlass. Right? Yeah. So so a lot of matrix multiplies, intensive course, they they basically realize, oh, you can write a lot of these, linear algebra libraries as primitives of various operations between tiles of data. Right?

So when we do our assignment three and matrix multiple times, we'll see that we we tile up our matrix in order to make it efficient. So they wrote a library where you could basically use APIs of of linear algebra basic blocks to build other algorithms. And this is also kind of the precursor to the whole CU title and title programming paradigms for for CUDA Python. Right? So so in Python, everything is an array.

Right? When when when you're programming Python. Right? So so the analogy to that CUDA, everything is, everything is a tile. Right?

So so the programming paradigm native to Python for CUDA is to think of everything as a tile. This is, this is what was released three weeks ago. So So you manipulate everything in terms of tile. This this is kind of like a precursor to that to that feature. Right?

And these are all, like, a lot more advanced stuff. Okay. Alright. Any questions? Yes.

Do you think the hardware getting this good is bad, but, like, people actually developing things? Because, like, off topic, but, like, instead of, like, optimizing the game, they'll be like, yeah. They'll buy a better GPU. So then they just, you know No. You still have to optimize it for the GPU.

That's the issue. Right? It's not like, like, in the eighties and nineties. Right? You could write crap code and and you'll still be faster because your your CPUs are are still benefiting from Moore's law.

Right? But on a GPU, like, we're learning in class. Right? It's not just an ISA and it just run fast. Right?

There's a lot of features in the hardware that you have to write your algorithm specifically to take advantage of that hardware feature, like work divergence, and then you have for thread program counters and these CUDA graph features. Right? So you still have to even with new GPUs and new features, you still have to write your code tailored to those new features. That's right. You can't just rely on, frequency increasing, essentially.

Yeah. But, you know, yeah, there's a lot of new features that that is kinda hard to keep track of. Right? But, you know, at the end of this class, you have the foundations to jump into any of these that we ever need to, and, hopefully, it makes sense. That's the goal.

That's the end goal for this class. Alright. C plus plus. Who knows C plus plus? Who knows modern C plus plus?

Professional C plus plus? You think you do? Okay. Yeah. I thought I did too.

Right? I learned C plus plus back in When was the college? 1925. 1925? No.

A little past that. Wait. Hold up. The date of It's been so long ago. I forgot.

02/2006. 02/2006. Right? I started college in 02/2006. That's when I learned c plus plus.

My students work in industry now. Whenever they go to internship, they have to learn c plus plus. And then they come back, and then they show me code. And, like, you know how when you see code, you could kinda, like, compile it in your head? I can't do that anymore with c plus plus.

It's like a foreign language to me now. It's so unrecognizable. So you think you know C plus plus. Yeah. I need to know that in about two years, you're gonna be having students who are born when you studied college.

But now why do you why are you doing that sometimes? Hold on. I need to cry. It's kinda weird to me now. Like, I think you're closer in age to my son than you are to me now.

I I'm I'm 37. So it feels weird to me now that I finally have this, like, generational gap. Like, when I started, I was 27. Right? I wasn't too different than you guys.

Right? I was still immature, but I had to act with that shit up back then. Now I have k then, like, whatever. But, yeah, anyways right. Okay.

Yeah. So so there's a lot of ways of programming GPUs. The easiest way is just to rely on compiler magic using pragmas. You guys used pragmas before? Right?

Yes. Fun stuff. No? Wait. What'd you guys use it for?

I just use once because they're not they're to not do the thing. It's not do the if that for whatever. Oh, okay. If they're for stuff, right, I don't know. OpenMP.

OpenMP. Right? You do. Right. Yeah.

Right. So if you get the open MP, right, I'm not gonna talk about that, but these are another way of programming GPUs with very low effort when basically, if there's a loop that don't have loop care dependencies, right, a compiler could figure out how to parallelize automatically, and that's the right thing. We have language extensions. It's kinda what we learned in this class. Right?

And then you have libraries or high level languages. So for us, it's kinda like a, you know, STD algorithms in c plus plus. Right? That's kind of what process. And now I think that's kind of, reworked into a whole different C plus plus project nowadays.

So so there's a there's a bit of huge effort in, NVIDIA on on the C plus plus ecosystem around around CUDA. So they're making a whole bunch of these SAT like algorithms for for CUDA and stuff. Right? So so a lot of the code that you're writing nowadays, for your assignments, like like reduction and histogram and everything else like that, those exist as functions in Rust. Right?

So you wanna think about it. You're building libraries that that's kinda integrated nowadays. Okay. But, specifically, right, C plus plus makes use of, GPUs through parallel algorithms. Right?

So you know algorithms. Right? Y'all used sort before, I assume? Yes. Right?

So you sort an array from c begins to the end. Now you just add STD execution par, which means it's a parallel algorithm, and this will automatically have a GPU implementation of it. Right? So just like that, now you can use GPUs with g c plus plus. Right?

Yes. You're thinking, oh, I could just take my code and manually throw that in. I have a project done. Right? And also, you know, reduce as well.

Right? So, you know, just do regular, c plus plus stuff. No one gets me. Like, I I learned C plus plus before everything was templated. You know?

There's so much templates in C plus plus now. I I can't parse that. Alright. So just to kinda give you an idea of, how the difference programming period is trying to improve programmer productivity. Right?

On the on the far right, you see how we could program something included. Right? So this is a SaxPy algorithm. So, basically, a x plus y, of an array. Right?

So you see, global void Saxby, I equals block ID times locked in plus ID. Right? Everything we do in class, we get a global index. And then if I is less than n, you know, my y is a a times x plus y. Right?

So so it's actually a plus y. And then you do copy, cross actually, you launch the number of kernels and so on. Right? So that's what we learned. So if you have this, we somehow prefer the SAXS algorithm using a transform in modern c plus plus.

You could parallelize that. Right? See, I I can't parallelize that. I don't even know what that means. Can someone help me?

No? Go on. You say you know C plus plus. What happened? I think with C plus plus, they're like, on a scale of one to 10.

He's like at a eight out of added like, he's not that What? Even the guy who made C plus plus? Yeah. And I can really use an interview. It's like, how dare you a C plus plus?

I was like, it's like an eight. Oh, jeez. Something like that. I don't know. Dang.

I'm like the negative four right now or something. I don't know. I think it's, like, we don't like, at a certain point, like, we stopped, like, talking about templates and start talking about. Like, I had to be able to go to, like, basic data structure implementation. Yeah.

Right? Yeah. We we learned we learned the basics of the CS theory stuff, right, in in c plus plus as a language. But C plus plus in practice, like, at a professional level, it's just how can I solve that problem with a combination of algorithm yeah? You see, templates, right, and and C plus plus afterwards.

Right? You're talking about the middle one or the right one? These are both c plus plus. Yeah. The only difference here is that I I I do a open ends oh, open end p.

What do you not get about this? It's just a question what's what I'm doing. You know what that transform is doing? I think you can just, like, here we can turn. But how is this a x plus one?

Oh, man. I also love c plus one before they were Lambda. You just Everything's Lambda nowaday. Yeah. Yeah.

Right. So there's a Lambda. Right? Yeah. You saw Lambda before?

Do you guys use Lambda? Only only in Python. Right? Because c plus was a Lambda. It's basically the same thing.

Right? The browser capture group that captures everything in local scope. Yeah. Yeah. Right?

So so this if you've never seen a lambda before, my my EE folks, a lambda is basically a function that you embed in another function. You can think of it that way. Right. So so, basically, you're transforming and this is a parallel algorithm from x to x of n, and you're transforming that with y and output goes to y, I think. I don't know why there's two y's after that.

But but, yeah, I mean, there's two inputs. Right? So there's two arrays x and y. And then for every element, it's transforming it with this lambda function. Basically, if if if a transformation of stack space basically, you're transforming it.

Right? So, like, vector add can be the transformation as well. Right? It's an addition transformation. So so nowadays, right, you just add par and then it's paralyzed.

In this case here, if you wanna use OpenMP compiler magic, right, you just wrap that around and then we compile it all the time to parallelize that as well. Okay. So so this is back then when I got the slide, it's like coming to a compiler near you. It's it's supported now. The compilers we have in our system fully supports everything.

Just just say as they would. Alright. Some examples. Traveling salesman. Non e e folks.

You implemented this in algorithms? Yes? No? You did it? Oh, isn't this, like, a basics in algorithms?

No? Yes? We didn't see covers, like, graphs, but, like, there's no assignment on it. Wait. There's no assignments?

Yeah. So, like, that that graphs are really covered until, like, CS one. No. No. No.

The for when I took 10 c, we have to do the the bucket problem with with graphs. I remember that. I remember that. I remember that. I remember that.

The traveling salesman never likes it. It was more like a hidden the problem that exists in a prime example. Oh, but you don't actually solve it? No. No.

We didn't prove it. You do, like, search, like, a star and stuff? That's one That's, like, c s one seven, dude. No. A yeah.

That yeah. A is north of one seven. Yeah. Artificial Oh, that's AI. Oh, oh, okay.

That's what happened to AI. Oh, I see. I think I think the specific 10 c graph thing was Dijkstra's algorithm. Oh, yeah. Yeah.

Problem. Uh-huh. Uh-huh. So it's solving, like, a problem with Oh, that that was the suite math. Right?

Was that the sweet math? A little bit of a. Like, End Of Tennessee is not a yes. Oh, I see. Okay.

Well, you know, this is another one of those famous graph problems. Right? Yeah. So here's an example. You know, just just, you know, if you wanted to play with c plus plus, right, for your final project, right, this is all stuff you could do.

So, apparently, this implements traveling salesman. Find best route return, and then that's it. And then they embed a whole bunch of Lambdas in it. Yeah. I can't parse this.

I I can't even imagine how they would come up with this. Right? So we do transform, reduce. We're gonna do reducing, like oh, we do reduce. Right?

Yeah. So this is a reducing something with we're doing a minimum operation. Some passing. There's a lambda that just found me something. I don't know.

Yeah. Like like I said, I I don't know. See if I'm supposed to do it anyway. But you're feel free to try it out if you want a challenge. No?

Okay. Can we just read the lambda and then the rest of the stuff, just parameters for Right. But then you're applying this to every operation or something like how do you wrap your mind around how this works? Okay. So so I I've I've found this slide, like, this slide set for, like, the last five years now, I think.

Only one person knew everything, like, fluently. And and the reason why was because, he was a master's student, and he came back after working in industry for a gaming company for, like, two years. Right? So so modern c plus plus when you go into industry, it's very different than what you learn here. So all of my all of my PhD students, when they go into industry, they end up having to learn like, relearn modern c plus plus when they were there at the same time.

Right? So so when you say you know c plus plus on a job interview, know what you're getting yourself into. I feel like there should be a course just on modern c plus plus. Alright. But okay.

Anyways. Right. This is just to expose you to what modern c plus plus look like. Alright. So, you know, this this is different ways of programming things.

Right? So this one does a performance comparison. You see the speed up of sequential, of of, you know, c plus plus algorithms, OpenMP implementation, sequential. I don't know what these colors mean. But either way, you get some speed up.

Right? So for your final project, if you wanna tackle, like, one specific algorithm of a problem and implement it with different ways of programming and and do a performance comparison. Right? You can make charts like these to kinda quantify programmer productivity, like lines of code, you know, like, programming effort, and and, how performant it is. Right?

Specifically for OpenMP, you can run it up with CPU and GPU. And then you could kinda compare a parallel CPU version as well if you want. Right? So and then just more examples of things here, like like Laplace transform and so on like that. So there are some quirks of using this with, GPUs.

Right? So not everything, you just throw par into it and it's gonna work. So so some of the things specifically that will compile and, you know, if you if you jump into the c plus plus. You know, some some groups tried that, so they don't need to do the issues. Now when you pass a function to do a transformation, you know, for for you out, right, like, for each, right, you can't use a function pointer.

So, yes, your functions can't be a pointer. Have you seen a function pointer before? Yeah. I think yeah. You've used it in some of your classes.

Right? Yeah. Okay. If you have a cc, put close in a while. Right?

You could do that as a function pointer. I think it's because when we try to compile this for GPU, right, we get a pointer to somewhere else, and then the compiler can't figure out what that code is. Right? So so, you know, this is valid in normal C plus plus, but if you wanna offload this to a GPU, right, we have to embed the code specifically in this function. So this whole function gets compiled to the GPU.

Right? So that's why we embed everything at the Lambda here. Right? When we when we use this for a GPU. Right?

So when we pass this whole function including the Lambda into the compile link, they'll know that, oh, okay. I have everything I need to parallelize onto a GPU. Right. So so you can't use these raw pointers. So so all of these are valid ways.

Right? You can have a function object, whatever that means. Lambda is okay. You can even wrap a function within a Lambda. It's in that fact it's a substitute in that way.

So all of these are valid ways. It just can't be important, essentially, for for it for it to target your your GPU. So there's some issues with memory as well. Right? Because your GPUs use different memory than than your CPU.

Your vector has to be in the heap. Right? Because we had to do an m copy, essentially, internally. And and, basically, you know, vectors are okay because vectors are allocated in the heap with mew. But arrays, sometimes they're statically stored.

Like, if you just do, like, an array of a thousand, that's a fixed space. So at compile time, we can allocate that in the stack. Right? So so the stack is is you know, if you know the size, you can allocate it in stack. Things that have a variable dynamic sizing equals in the heap.

So these in a stack, you can't you can't parallelize. So that's that's another gotcha right there. And, again, that's especially because when we do print copies, we we always transfer from heap space, not not stack. A stack is typically protected. Right?

So this this new features, there's a lib CU plus plus. It's it's basically the standard library for c for CUDA now. So a lot of these SCB things are are in here. And they they have various operators, like, and and data types. Right?

Right. So so here's examples of it being used in real world code. Right? So Lulashe is a very in high performance computing space, it's a very famous, scientific workload. So I actually worked on this when I was at Livermore as well.

My name is on one of the papers, actually. So there's a lot of versions of this. So so a lot of times, this had to have a world in the HPC space. Every time they have a new programming language, you take matrix, you take BlueDash, and everyone just ports it over. So, you know, these are real world examples of using parallel c plus plus in in in real world workloads.

Right? A lot of times, HEPC in the scientific workload space, they're the ones who adopt these new features, you know, Right? So you can look into the, national labs and and what they do and supercomputers that they have, to see the the real world use case of of these new features. Right? So they use this in the and a whole bunch of other stuff.

Okay. And, you know, and they see some speed up too. Okay. So so that's the c plus plus side of things. Again, you know, if you wanna play with that, that's perfectly within the scope of things.

And then the last thing we'll talk about is, Python. Alright. This is a new slide set I got. I can't find the original slides. I I have this.

So this is I'll upload it. So, it it's so number is basically, part of the RAPIDS AI project. So, you know, you in Python, you use a lot of libraries for, like, processing data. Right? You have you have pandas and NumPy, you know, these other stuff.

Right? Do you learn that in data mining or, like, data science or something like that? Right? We have courses that teach you a lot of stuff. Right?

Every single one of those library will be in a RAPIDS AI project as a CUDA equivalent of it. Right? So they have a CUDA accelerated version of all of those. And then another part of the rapid projects have become, which is before CUDA Python another way of programming CUDA in Python. Is another way of programming your GPUs as well in a very Pythonic way.

So this didn't come from NVIDIA itself, but it came from the RAPIDS AI project. Right? So so when this came out, some of the final projects wanted to use this. And, you know, it was, like, horribly horribly broken. A lot of things that we try to do in class wouldn't work, and it would just fail, and the performance was horrible.

And every year, someone has done this. So it's it's kinda nice to see. Like, every year, things got less broken and faster and easier to use, and now it just works everywhere. Right? So now this is actually quite fast nowadays.

So this is a guy who works on Namba, and I'm not going over those. So, basically, what is Numba? Right? So so Numba is a is, again, compiling method. Right?

Like, in Python, you don't compile. It just work. It's just it's just entire compilation. Right? You guys interpret it.

So Numba is is a just entire compiler where it's kinda like open and open MP where you annotate a function, and then they compile it with automatic parallelized things. In addition to that, they also have APIs which are very CUDA specific. So everything we do in class that we write in CUDA right? There's a Pythonic, almost a one to one mapping in Numba as well if you wanna write, like, raw CUDA functions in Python. Right?

So there's two ways of using Numba, really. There's a there's a just a time compilation method, or you can write wrong Kudo kernels using NumPy APIs. What's that line for? Right. So so, you know, Python is used everywhere.

Right? NumPy, Pandas, Scikit learn. Most of you probably used that in the past. Right? So in NumPy, you know, you all probably done something like this before.

Right? Everything is an array. You're arranged in a rate of 10. You do some operations to it. And then you could do cosign operations.

What what and I think there's a GPU accelerated version of that as well. One second. Right? Alright. So so let's say we have this example Python function that that performs a.

Right? It's not you see those, like, pictures of Yeah. Yeah. You've seen these pictures before. Right?

Yeah. So I guess this is the function that that implements that. To run this on a GPU in Python, all of it is on on a CPU. Right? You can call it JIT.

Right? So doesn't just targeting, GPUs. Right? It it's actually originally targeted CPUs. And then they go to compile the back end, and then they target the GPUs.

Right? So if if you run this on this, all of a sudden, this code will run significantly faster. Right? It does just the type of compilation of this, and it does a bunch of CPU optimizations. So now your Python code can run faster.

Right? If you wanna make your Python code run faster on a CPU. That's all you have to do. If you wanna run it on a GPU, right, you just target now GPU. Right?

So code it up to device equals true. So this tells it to target this for GPU. Then a compiler back end basically would generate GPU comes from this function. Right? That's it.

It's like open entry. Right? You don't have to worry about data movement because, Pluto will figure it out at the compiler back end. Right? So, yeah, one line of code and you'll be your final function.

Right. How much time I got? Oh, okay. Twelve minutes. Right?

And then on the other end. Right? Sorry. Right. So so you could call it random.

Right? And then you could call it as well. So this is another function that kinda wraps around it. So you can make things all, like, wrapped up as well. Right?

The compiler will figure it out. So then put a dot jit also right here, and you could write this included as well. Right? So so notice that this one is a bit more, like, that we see. Right?

So so this is a function that performs an operation on a single pixel. Right? But if you wanna make this parallelize and work across a whole image, for example, right, we would have to partition it. Okay? So the way we partition it, right, we don't really have x equals block ID plus block then then plus ID because that that convention is used so much, in in NumPy, just grid, bullet dimension, and they have a global x and a y.

Right? We had to before, we had to do, like, row and column equals block ID and the x or y dimension times block then plus ID. Right? So so because we use that so much, we're gonna use that essentially. And so so it's almost a one to one mapping between what we do in in CUDA and and and Python here.

Right? And then when we launch a kernel, you know, we have that triple angle bracket, and we specify number of blocks inside of their plug. This is the Pythonic way of doing it. Right? So we still calculate, the ceiling of the number of blocks that I need in the x and the y dimension.

So you have a config, the block size, dimension, and the breadth dimension. Right? And then when you call a function, it's a triple angle brackets. It's a square bracket that needs to be config. And then that's essentially calling this, like like, we call it CUDA functions.

Right? So there's a pentatonic way of doing it. So so if you wanna treat Numba like the Python equivalent of CUDA, right, this is almost a one on one mapping of things. Right? So this is something that you you know, you know, if you want, you could try it out to see whether your code is faster in CUDA or in Python.

Okay. So so they say it's faster than c Python, But this is cheating because they're not comparing GPUs to GPUs. They're comparing single thread. Right? So notice even on the CPU, if you do just the type of collision, they they said this code actually is 71 times faster.

Not 71% faster, but 71 times faster. Right? So so the compiler back end work that they're doing, they're doing an insane amount of optimization. Because of all these accelerators and machine learning, if you'd like compilers and you do work on compiler optimization and back end stuff, there's a lot of money in that too right, in in the in the world right now for it, for accelerators and machine learning. Okay.

So so that's an example. So just to give you an idea. Right? Basically, what happens in the back end is that we use something called LLVM. Do you use LLVM in compilers?

We use it as in security. In security. Right? You do oh, you do. LLVM.

For what? You had compilers? Or for static analysis? For static Alright. Yeah.

Yeah. Right. So there's a compiler infrastructure. Right? You do static analysis.

It does compilation. The compilers in undergrad, I think you just do the parsing, the front end part. Right? You do parsing. Right?

So the back end part is using LBM, but I guess that's that's the ground level. Right? Or we use a, like, some weird intermediate representation. Oh, not use LBMIR. Oh, you use a oh, it's an educational language.

Right? So so, basically, LOBF basically takes your front end code. After it compiles it, it sends it to intermediate representation, and then all the optimization happens within this intermediate representation. In in the graph version of compilers, two zero one, I think, or two zero two. Yeah.

Which one? You learn how to do a lot of these optimizations. And then because everything is in IR, you can target different back ends. Right? You just have different back end that you made support for x 86, r m a p t x, which is for GPUs.

And this works also for for AMD GPUs as well. Right. So so this is basically the magic that makes everything work in the back end. And then a lot more details if you want to know, how how this works. Right.

So this so memory management as well. Right? Okay. This is the other things. Keep in mind, right, because you do put a mem copies and so on.

You can actually use it without any explicit memory movement, and it works. Right? You don't have to worry about what data stores were. It would just magically move itself. Right?

Cool, Yeah. So next week, we're also gonna learn unified memory. So we'll see how we can do that also on your CPU code, on your C code as well. Right? But, you know, this is this is the implicit.

If you if you would like to move your data yourself, this could enable us to. Right? So could I have to device and, you know, the device array. Right? So so this is basically and copy to host.

Right? So this is the data. This is the encrypted memcpy host device, and this is encrypted memcpy device to host. Oh, wait. That's, like, the other way around.

Right? Two device. Hosted device, device to host. Right? So that's that's the equivalent to.

And that specific array should have to make, array on the on the GPUs on the GPU side. That always start with a. So yeah. So there's ways of making specifically, memory allocations and whatnot on the GPU and the data looping too. So so there's a there's a number number specific way of doing that.

Right? And then this is the free, essentially. Delete. Okay. This is a very long slice set, so I'm never gonna I'm not gonna go over it.

But, you know, all of this is here if you're really interested in in Numba. Okay. So, hopefully, you know, today was just a really high level overview of the different features, the newer feature sets in CUDA. Right? The great ways of programming it.

Besides just the. There's other stuff of different parallel programming paradigms, c plus plus and the number and Python. Right? Just to give you an idea of what to do for you to find a problem. Alright.

So we'll empty today, and we'll do our meeting on, Thursday for the. Not gonna be remote? In person. Yeah.


Okay. We could get started now. So my mic is ringing. Okay. Alright.

Today is the midterm review. I assume no one looked at the sample midterm. No. Right? The last page, you couldn't answer because you didn't come to those topics, and no one complained about it.

So clearly, no one no one started studying it. Right? But that's okay. I didn't make a midterm there either, so we're on the same boat. Yeah.

So, you know, also you used the CS testing center before. Right? So the the link to make the reservation should be active now. So you can start making your reservations now. Right?

If you don't see it or it's not working, let me know. And then, hopefully, we can figure out what the, CS department, to try to iron things out and make the work. Right? So because of that, you know, Tuesday's lecture is canceled. Right?

You guys take your exam Monday, Tuesday, or Wednesday depending on on whenever you you have time. And the exam will be eighty minutes. It'll be a Canvas exam. So, you know, you log in to Canvas, and you should be able to see it. The Canvas exam is has a specific IP address.

So you could probably see it at home, but you won't be able to take it. It's locked by the IP address. And if you're you're using it at the SDRC, you need to let me know because I need to know the IP addresses for it for their computers to enable it as well. The CRC hasn't gotten back to me, so we'll see what happens. And it'd be close close notes, close book, close to Internet.

Hopefully, you don't use Google or Chat. You can see, like, on the this testing site and websites, computers. And I think I don't think you need a calculator, but, hopefully, that's it. Any any questions? Yeah.

So we're waiting on Canvas? Yeah. Yeah. Have you done a Canvas exam before? No.

I'm in test center. No. I'm in test center, but I've been, like, on the. Yeah. Okay.

I I think if you go on onto eLearn right now, there should be, like, a dummy test as well that you could try out, under is it assignments? Right. So this this dummy test is visible to you. So if you wanna see how e eLearn exam look like what can I look like? Right.

You could try out this dummy test and see. Right? Test it out. Right? There'll be, like, multiple choice or short answer.

So you could go in and and try that out just to get familiar a little bit. So when you go into the CS testing center, that midterm exam will be oh, that got imported. That hasn't been updated yet. I haven't got it. But, yeah, that that would show a puzzle as well.

Right? Okay. Great. Alright. Any other questions for logistics?

No? Okay. So, hopefully, you're familiar with the whole process. Alright. Since none of you really started studying, I guess I'll just go over the sample midterm, and then this will kinda just reiterate over various concepts.

Right? So this is a sample midterm that was uploaded. Except for the last page, which covered, unified memory and and streams, Most of these multiple choice isn't relevant to you. So I actually uploaded a where is it? There's there's a midterm sample that I uploaded.

This one. Hold on. Let me refresh the website. Right? There's a there's a sample midterm too, which has more, like, true and false multiple choice questions that's relevant.

So so we'll look at those samples as well. So if you're looking at this last minute, you can kinda ignore this last page. This is, something we'll learn after midterm, so that that'll be on your final exam. Okay. But other than that, we'll just kinda go one by one and hopefully reiterate over these various concepts, and reinforce it.

Right? So if you kinda think about it, we really only covered the basics of, how crude and realism works, like, SIMT model, SMs, and work divergence, and how do you write an algorithm that is more friendly for work divergence, right, which is your your redemption algorithm. But, you know, so so most of these problems kinda test that that conceptual knowledge of, you know, whether you have, you know, a mental model of how the parallelism works and how the blocks are arranged and the threads are arranged and and so on like that. So, hopefully, it's pretty simple. So for this problem here, right, it's it's about your reduction algorithm.

So have you have you done your reduction assignment? Hopefully, you started. Let's do today. Yes? Okay.

Right. This is your naive reduction. Right? The the product actually implements the tree, right, the reduction tree. So so if you know your assignment too, which is gonna be familiar to you.

Right. So given this naive reduction algorithm, right, assuming we have a block size of twenty forty eight, you know, our side is 32, how many of the warps in a block will be divergent when my stride is equal to one, thirty two, or a 128? Right? So so one of the most common mistakes with this problem is that, you had to figure out, you know, the number of, problems. The the problem size that each each red block is is processing.

Right? Not the fact that all of the the whole problem's broken up into blocks. Right? So each each thread block basically handles, attributes. But right?

So so in this case, right, we don't give a total problem size. Right? So so you don't need the total problem size to solve it because the reduction should happen when we're in a block. Right? So for example, on a midterm, I could potentially say, let's say I'm doing a reduction on an array of a thousand or or a million.

Right? So so if I give you a problem like that, you would have to recognize that, oh, it doesn't matter what the problem size is. What what matters is the size of your dev block. Right? That's a big hit right there.

K. So so in this case, right, our our dev block says it's twenty forty eight. Right? So even though I don't have the total input size, it doesn't matter in in this case. Right?

So so in the case, right, where my stride is one, that's basically my step. Right? And then every step that iterates after it, right, my stride doubles in size, because if you recall that reduction tree, every every step, we we basically, you know, add something like this. And then the next step, we add like this. Right?

And then so on like that. Right? So so our stride basically doubles in size at every step. Right? So when my stride is one, it's basically the step.

Right? So my when my stride is one, how many of my works are divergent in this case? It's zero. Right? Because all the threads are active.

Right? To answer the rest of it, right, you kinda need to know how many works I have in total. Right? So so how do you find the number of works that you have in this case? Divided by Right.

Yeah. So so your work size is 32 threads. My thread block size is twenty forty eight. Right? So you just divide those to get the total number of of works.

So twenty forty eight divided by 32, I think, gives you 64. Right? Because 32 by 32 is ten twenty four. That's still 64. So so when my stride is 32, right, the thing you have to recognize is, you know, what's the what divergence pattern when you start is 32.

Right? So so when I start is 32, which threads are active and and perform a a reduction operation? One thread per Yeah. One thread per warp. Right?

You know, so when my thread is one, you know, every thread is doing something. When my thread is two, every even thread is doing something. Right? When my thread is four, every thread is doing something. You can kinda see that with that if statement.

Right? If you don't know the concept, just looking at the code, you can kinda figure it out as well. Right? So in this code, the only line that would cause more divergence would be the if statement. So if thread ID mod stride is equal to zero Right?

Just by looking at that, you could kinda figure it out too. Right? Just looking at that that line of code. Right? So my when my thread is 32 okay.

When my stride is 32. Right? Thread ID stride 32 means, you know, thread zero thirty two sixty four would be active. So so, essentially, at this point, you know, every single warp would have only the thread active and and the rest inactive. Right?

So so that would be considered divergent or or not divergent? Divergent. It's divergent. Right? So so when we don't have 32 consecutive threads doing the same thing, right, that that's when things are divergent.

So in this case, when the thread is 32, every single thread is divergent. Okay? So then the next one's a little trickier. So when my thread is a 128, right, that mean thread zero, one twenty eight, two fifty six, 300 something, five twelve. Right?

Those are the best that are active. Okay? So so in that case, you have warps that behave in two different ways. Right? So some warps are all completely empty, and some warps have the thread active.

Right? So so how many of those two warp types do you have? Right? So so in that case, you know, the the thread is active, and then the next three works are essentially all idle. Right?

Because they'll do nothing. Right? So so in that case, 1 of the warps have one zero zero zero zero. Right? And then the other 3 is all zeros.

Right? So so if it's all zero, is that more considered divergent or non divergent? They're non divergent. It's non divergent. Right?

They're all doing nothing together. Right? As long as they do the same thing together, you know, nothing is the same thing. Right? Those are considered non divergent.

Right? So so in that case, right, you could if you understand, you know, how warped divergence work, right, logically, you could figure out that, only 1 of them actually have divergent. The other 3 of them are doing nothing, which is not divergent. Right? So that's that's how you get 16.

Any questions? Yeah. So on strike two six p, that means every like, half of the credits in before are. Yeah. Yeah.

Correct. Right? Yeah. So, you know, when your web divergence is, when when my strategy is 32, right, my active mass looks something like this. Right?

So when my strategy is two, it will look something like this. Sorry. One zero one zero. One zero one zero. Right?

As long as you don't have 32 consecutive zeros or ones, it's it's considered, divergent. Right? If if you don't have a consecutive one. So so in that case, when it's when it's two, like you were saying, 1 zero one 0 one zero. You don't have consecutive zeros and ones.

Right? So for one twenty eight, you have, like, this and a whole bunch of zeros. Right? So this is a 128 bits. Right?

So the 32 will be 100000, and then the other 90, what, 96 is all zeros. Right? So then those those other 96 would be the 96. Yeah. Would would be, three three consecutive works of all zeros.

Yeah. So so if you, you know, hopefully, you know, you have a general understanding at a high level of how the reduction string works. But if not, just looking at that piece of code. Right? You could you could kinda figure this out and answer it as well.

If you understand what what type of this is, but not reduction. Okay. Any other questions? Okay. Alright.

So this one's more about the indexing of things. I think this question actually came up, when we were doing the lecture on reduction. Someone asked asked a question about this. So we we kind of discussed in detail about this in class, but we just reiterate. Right?

Since it's it's very hard for me to grade programming, you know, for you to code or write write in the code blocks. Right? The instead of coding will be something like this. Right? Index it, essentially.

Right? So so, you know, one of the major components of writing these CUDA codes, right, is because, you know, we have to write one piece of code so every thread process the same thing, but we just index a different data that we have to process. Right? So so indexing is a very important part of running your CUDA code, and this kinda tests the the intuition you have with the whole indexing aspect of of writing CUDA. So in the reduction code, right, we have this partial sum, which is the the shared memory buffer that we use to we catch the data that we're processing.

Right? Well, point of that is to avoid binding global memory with the slope. Right? So if you recall from your assignment, we had to essentially you know, in your very large array, you know, we break this up problem up into different threat blocks. Right?

And then within every threat block, we have a shared memory component. Right? Shared memory that we have to essentially copy your data over. Right? And then we perform the reduction tree in in the shared memory.

Right? So just to, you know, do a bit of a refresher. Right? If your rep lock size, hypothetically, is, let's just say, two fifty six. Right.

How much data am I responsible for when I do a reduction? Five five twelve. Right? Every day I process two pieces of data, so the amount of data that I'm responsible for is two times your block size. So so, you know, this is two times block size or block sorry.

Block dim. Right? So we we have to start offset. You know, for every thread block, the offset is, you know, one block dim, two block dim, three block dim, and so on. So your offset is two times block dim times block ID.

Right? That gives you the that start offset here. But then this question asks, you know, if when I'm doing this copy operation, right, there's there's two different ways I could do my copy operation. The way is basically asking, you know, so this is my two times block them. Block dimension.

And this is my share memory. Right? So the one's asking, you know, for my thread, I moved, you know, two consecutive, two adjacent input elements. Right? Sorry.

That should not be okay. Right. So so my thread is responsible for copying these two. My thread is responsible for copying these two. My thread is responsible for copying these two dot dot dot, and then the, you know, the last.

So it's responsible for copying these two. Right? So each thread is responsible to load two adjacent input elements in this case. Right? So this, the copy you know, my indexing has to have that pattern of of copying.

So so it's the thing we have to figure out is, you you know, this is blank. Right? Where does my t copy to? Right? So so t and t plus one is two adjacent values.

So t has to map to, you know, one of the index in in the in the global memory Right? So if my thread ID is zero, the element I had to copy is zero. Right? If my thread ID is one, the element I had to copy will be what? If every thread is is responsible for two consecutive.

Right? Two. Right? Right? And then if my thread ID two is responsible for copying from, like, four and five.

Right? So but, basically, it's basically offset by two times your thread ID, essentially. Right? So if you if you could figure out the pattern you know, the general approach to all of these indexing thing. Right?

If you could figure out the access pattern, you could write a an an index form for it, essentially. Right? So so it'd be two times the right ID. Right? That's how you get that.

Any questions with how we got two times the right ID? No. Right? Okay. So, you know, the index here is 024DotDotDot, you know, two times the right ID.

Okay? So the element I add is t. I mean, not add. Copy over is t. Right?

And then the element that I copy next is just the adjacent one. Right? Adjacent main is t plus what? T plus one. So the adjacent element is just t plus one.

So then that's when you get t plus one. And then the next problem asks about, you know, copying from a strident. Right? So each thread loads two info elements with a stride of the block dimension. If you did your homework assignment and and saw the lecture, the lecture assignment might have this also as well.

The access pattern is basically you know, the thread copies this, and the next thread copies this element. And the stride here is locked dimension. Right? And then the next step copies this and this. Next step copies this and this.

Dot dot dot copies this and this. Right? So this is the pattern that that we have, in the the slides for the reduction lecture. Okay. So, again, taking the same approach.

Right? What is the index that each thread gets the time it has to copy in this case? Right? The blue thread would initially have to copy index what? Zero.

Right? And then the orange thread initially would copy which index? One. Right? So continuing on, the next one copies two.

Right? So 0123456. What what's the pattern there? Yeah. Just the next number.

It it maps your thread ID, essentially. Right? So then, you know, that that's start index would be just your thread ID. K. So this blue thread, what's the next index that I copy for the blue thread?

Right? It's it's the blue thread copy zero. The next time, it will copy a stride away. Right? So what index would it copy at that point?

Block. Yeah. The the whatever the stride is. Right? In this in this case, the stride is blocked by dimension.

Right. So we didn't give a a block dimension. Right? But we have a variable that represents block dimension. Right?

We're blocked in. So it'll just be, you know, t. And then the next one is just offset by a strand of of blocked in. T plus blocked in. Right?

Because for the blue and the orange and the purple and the green, right, all of these strides are the same. This is block dip. Right? So so if you understand conceptually, just the the absence pattern, right, you could kind of derive, the indexing. Right?

So so this one basically has your understanding of of the whole indexing type of thing. It's just really figuring out what the pattern is and, you know, either a memory access pattern or a compute to data indexing. Right? And just figure out what the index is. Right?

Alright. Questions? And so even if you didn't do your homework, but you you understand the concept of how threads map, you know, to different memory access location just with the indexing. Right? You could figure this problem out.

Okay. Easy so far? Yeah? Right. Everyone's gonna get an a.

Right? Yeah. Okay. Alright. So so problem three, it's a it's another simple problem.

You know, we talked about this in week two. It's basically how how can you partition up a two d problem, essentially across different blocks. Right? And we've done this in assignment one. And you'll do this again for assignment two.

I think two and oh, sorry. Three. Three is a two d problem. So let's say we have an image size of 400 by 900. Yeah.

It's it's 400 by 900. I'll I'll specify which one's the, you know, height or length. Sorry. Yeah. Height or or width.

Right? I I noticed that a lot of people, sometimes swap with the height and width or whatever. But, on an example, I'll specify what dimension is height or width. Okay? So let's say you have this follow of 40 400 by 900, and then my SMs have, you know, certain limit.

Right? So so my hardware and my SM, they can only hold a certain number of threads because of hardware constraints and also a certain number of thread blocks because there's only unlimited amount of bookkeeping, that the hardware keep track of. Right? The number a lot of states that we can hold. We we saw how you can look in the CUDA programming manual and and the CUDA website to figure out what the compute capability is and and look up what your hardware's, and and, threads are.

Right? So assuming in this case, right, the the limit per SM, you have fifteen thirty six threats, and, also, eight threat blocks is your limit. Right? So so given this, right, what would be your ideal threat block stacks? Right?

So at a high level, how do we select the the threat block stacks? Right? Is it in order to make my thread lock as large as possible, as small as possible? Like, what's the goal in selecting a proper thread lock size? Maximize its own number of threads.

Yeah. Right? So so to go here, at least for now, right, is to maximize the amount of hardware threads that that we have. Right? Later on, we'll see, you know, in in assignment three.

Right? The goal for matrix multiplies specifically would be to maximize the amount of reuse and shared memory. Right? Because in that case, you know, if it's a memory down application, memory access is just gonna be determining the the performance. Right?

So I wanna maximize the amount of cache we use essentially. But in this case here, in a simple scenario, right, the goal here is just to maximize the amount of hardware threads that I'm utilizing. Right? So there's only, you know, a a limited amount of valid drop lock sizes. Right?

So even though this is open ended, you could already just eliminate a lot of the the drop lock sizes, for example. Right? So hypothetically, right, can I make a drop lock size of three by three? Would this compile and run? If I drop lock and it's three by three.

Yes. It would compile and run. Would this be ideal? No. Now why not?

If you can only have eight threads. Okay. But what's the other problem with three by three? It's too small? Why is it too small?

Three by three gigs, you have many threads? Probably nine. Nine. Smaller than one. Yeah.

Smaller than a warp. Right? So that means it works and never be fully utilized. So just off the bat, the hardware is gonna be underutilized. Right?

So that's why you you don't really see any dev blocks smaller than 32. Right? Even in the case of a one dimension. Right? Okay.

But, yeah, also, it's too small. Right? So if my dev block limit is eight, right, the most there is I can use is eight by nine, which is significantly less than fifteen thirty seven for me. Okay. So so what's another ref lock size that's that's somewhat valid.

Right? So it should be a power of two, ideally. Right? So four by four by four. That's that's a valid one.

This is so small in the drop off size. So you can kinda eliminate those already. Right? Another power two after four would be eight. Right?

So eight by eight. Is that somewhat 64. Now. Right? 64.

Right? So it's okay. So eight by eight is starting to be in a realm of possibility. Right? After that, another power two after eight would be 16 by 16.

Right? So 16 by 16 is by two fifty six. Right? So it is 25664. Right?

256 still still finished. Right? Okay. So another power two after that would be 32 32. So that would give me, what, ten twenty four.

And prior to after that, 64 by 64. That would give me something what's that? Bigger than $19.36. Yeah. Something 15 yeah.

I don't yeah. The number doesn't matter. It's more than $15.86. It's $40.96? Yeah.

I'm hungry. It's lunchtime, so not. Yeah. Either way, it's it's larger than $15.36. Right?

So you can eliminate that right away. I think this might compile. And then you just get a runtime error, and it won't launch. It's possible. I haven't tried it, because I don't know if the compiler matches that.

But, otherwise, right, you really only have have these three valid thread blocks sizes. Okay? So so right off the bat, it's only three of them that I have to try. Right? And we kinda ran through this exercise in in class a bit when we when we talked about, you know, how do I size my dreadlocks.

Right? So so for eight by eight, you know, I have sixty four dreads. The limit here is I only have eight dreadlocks. Right? So if I have eight thread blocks of eight by eight, am I able to use all of my threads?

Alright. 64 times eight is what is it? Five twelve, I guess. Right? Yeah.

Five twelve is much less than 10 at fifteen thirty six. 32 by 32 is ten twenty four. So how many of my thread blocks of 32 by 32 fits? One. Only one.

Right? So it's it's, underutilized. Right? And in this case, 16 by 16 is two fifty six. How many of my blocks of 16 by 16 could fit in in my s seven in that case?

Six. Right? So six times two fifty six is fifteen thirty six. Well, fifty thirty six divided by 256. Right?

You're dividing it. 5036 divided by 256. 6 is less than eight, so I could fit that. Right? So so in this case, right, 16 by 16 would be, the proper answer.

So, you know, hypothetically, you know, if this was, let's say, 2048 just running through different examples now. Right? If my SM now holds twenty forty eight, which one would be a valid response now? It's matching order. What are we doing?

Yeah. Right. We can have multiple answers. What do we have multiple answers? So so okay.

So we could divide this by two fifty six. This is equal to eight. Great. That's just the limit. I that still fits.

Right? Twenty forty eight divided by ten twenty four. Ten twenty four is two. Right? So, yeah, both of these would be valid responses and accuracy.

Right? Okay. So what would be the tiebreaker here? Just to think critically. Both of these fully utilize the threads in my hardware.

Right? Which one would you prefer? Like, what other considerations would I think of? Or what I have programs that are running other GPU games, you might wanna get blocked for them instead. You don't wanna use all of them.

Okay. Okay. So in general, NVIDIA GPUs, if a program is running, you can't run another program. Yeah. That's that's why, you know, if one of your fast, I might write infinite loop, which hasn't happened this year.

I'm surprised. Right? No one wrote a code that crashed the server, like, on it. Right? It'll happen tonight.

It'll happen tonight or assignment two? Okay. We'll see. How many of you finished assignment two? Oh, good.

Good amount. Okay. Yeah. Okay. Yeah.

So so what other resources do, Dropbox use? Right? So, you know, Dropbox use threads. Right? We use registers.

Right? Because when we compile it, I mean, like, we simply could use registers. What other resources in the SM do do we use? Shared memory. Shared memory.

Right? So shared memory could is also limited. We don't talk about it yet, but, essentially, you know, shared memory could be a consideration as well. Right? So if you just go back to your you know, this this reduction assignment.

Right? In this case Every thread lock would use how much should it be? Right? So okay. Wait.

So so assuming I'm doing a reduction, let's say let's just say, double precision floating point. Right? Let's just say double, hypothetically. Right? Alright.

Good. So so so a double would be how many bytes? Eight. Eight bytes. 64 bits.

Right? Eight bytes. So that would be eight bytes. Okay. How many elements would my share memory have to hold?

For 16 by 16? No. The for reduction in this reduction example. Right? Three times the block dam.

Yeah. Two times the block dam. It's right here. Yeah. Two times the block dam.

Right? So let's say my block dam is, you know, like let's just say you did two considerations. Five twelve or ten twenty four hypothetically. Right? One d.

One d. The one that I mentioned. Okay. So so if my block dim is $5.12, how much should there be space with with that rep lock need? Yeah.

It's it'll be ten twenty four. Right? So two test block is ten twenty four times eight bytes. So eight eight KB. Right?

So so in this case, I would need, eight KB of shared memory space. Right? So if my is ten twenty four, how much shared memory space would I need? 16 k. Right?

16 k. Right. So shared memory has a limitation. Right? So for example, you know, your SM could potentially have a shared memory limitation of eight k.

For example, it can't hold more than that. So that even though ten twenty four fits, right, it doesn't hold the same amount. So then we would have to, you know, you know, eliminate that option. Right? So, you know, that that's just another example.

Right? So, yeah, besides your number of threads, right, shared memory is a lot is another resource that we have to consider as well. And then in in matrix multiply, we'll we'll learn how, the size of the your direct line also impacts the amount of data reuse that you have as well. Right? So so they this is just something to think about.

Right? But in the context of this problem, right, both of those would be valid. Okay? But there could be other considerations as well. I'll make a nice midterm problem.

Right? Yeah. And so 60 by 60 is the better one? In this case, both of them is valid. There's no better or worse yet here.

Well, if, like, we can share shared memory with 60 If I if I give that constraint, right, if there is a constraint for there, then, yeah, you will have to consider that as well. Yeah. Okay. So that's that's how you size things in general. Right?

Considering the the resources we have in our asset. Yeah. So so that's a. Both of those is valid assuming it's 2048. Okay.

So going back to the original problem, right, of 10/24. I mean, 1536. Sorry. It was 1536. Right?

I think '16 is by '16 was the only valid one, essentially. Okay. Right. So now we're we're at this. Right?

Let's see. So assuming assuming we use a block size of 16 by 16 Oh my gosh. There's an Easter egg in this problem. People still got this problem wrong, believe it or not. Right?

Yeah. How many words would experience the right type of bridge or or what type of bridge it's. Right? So so this problem of 16 by, 16 by sixteen and four hundred by 900. Right?

Is, nine hundred and four hundred divisible by sixteen? Four hundred eight. 400 is right. Yeah. I guess they have, like, how much heck of four.

So, yeah, 400 divided by 16 is the whole number. 900 divided by 16 is is not. Right? So so that means, I just did where where am I gonna draw? This is draw bigger.

Right? If if I break up my thread blocks, dot dot dot dot. Right? It's gonna fit perfectly. This is not drawn to scale.

I can't draw. Right? Some of your thread blocks on this side is gonna hang out, right, essentially. Okay. So for this problem, like, how many warps would have warped divergence?

Right? So so looking in, right, zooming in on one of these threat blocks is 16 by 16. The boundary of the problem kinda cuts, like, right down the middle somewhere. I don't know where. Right?

So so when your thread block is 16 by 16 conceptually, right, you should know where one warp, sits in that 16 by 16 direct block. Right? So so in this case, right, the warp would reside on on which element to that 16 by 16 direct block. Or in general, how do I how do I fit my warp in in that block? Like, what what would warp zero one two three?

Every two rows. Every every two rows. Right? It it's like it's like your two d matrix in the same indexing. Right?

It it it just kinda wraps around. Right. So the warp would be the two by 16. The next warp would be the next two by 16, and so on. I'm not trying to get this now.

So how many warps do you have in in in that ref block? Eight. Right? You have eight eight warps. Right?

So so, you know, if if my problem in in in purple, right, ends basically here, right, I I'm cutting through every single one of those works. So so those works clearly have some work type corrections. Right? Half of the threads are are ones. The other half are zeros doing that thing.

And if if this height here is 400, right, and my warp spans two rows. Right? So how many how many works is gonna lie right on that boundary? Just just conceptually thinking about it. Right.

200. Right? 200. So, yeah, this model will be a little trickier if, for example, let's say, you know, 400 is not divisible. Right?

It'd be, like, let's say, 300. You have just not divisible by 16. Right? Right. So this will be a little trickier because then all of a sudden, you know, you have a a warp that kinda sits out here.

Right? Is this trickier? Do you also have, like, one more that kind of link towards both? Oh, that's a good question. Do you have, like, one more?

Hold on. You're you're giving me, like, subtraction Okay. I see what you're saying. The corner one. Right.

Yeah. So then you have some corner ones here. Would assuming the case of 300. Right? Would this lower boundary ever cause any work divergence if it's 300?

It could. It kinda, like, splits to one word. If I know. Okay. I could.

Right? In what scenario would that occur? So, like, I think it would be, like, if the boundary, like, falls within, I think it'd be, like, within the within, like, one column for that for, like, any word. Yeah. Right.

So so, basically, I would have to cut in between that sixteen and sixteen. Right? Okay. So in what scenario, generically, would that occur? Would that lower bandwidth cut through my work?

So if you take the remainder and modulate it by, 16, would that be it? I think we're overthinking it. Yeah. Oh, the odd number? Yeah.

But anytime there's a odd number in the y dimension. Right? Yeah. Because, a warped spans two, two rows. Right?

If if my y axis or height, right, is is ever a multiple two, that bottom boundary is never gonna cause any work that we just right? But, yeah, if it's if it's an odd number, yeah, then it will cause a work that we need. So in this case, you know, the bottom actually don't cause any issue. Right? Because all of these bottom warps are doing that thing together.

All of these top are doing something together. So So it really reduces the same same problem that we have before. Right? We're just looking at that right now. Right?

So if, conceptually, in your head, you have a good understanding of how warps and threat block are arranged and and how your threat block partition of a problem. Right? You can reason through these type of problems. Alright. Any questions?

Okay. Oh, this one's a fun one. So let's say you did a vector add. Back then, we did vector add for assignment one, but we saw this in discussion. Right?

So and we saw the code for vector add also in in the lecture slides. Okay. Is this vector add correct? Will will it produce the correct results? I mean, ignoring the answer right there.

Just just reasoning about it. Right? What's wrong with this? Won't destroy the entire thing. Yeah.

Each thread is doing the entire thing. Right? So I paralyzed it by just throwing my whole serial version in in the CUDA kernel. And I was like, look. I'm using a GPU that's paralyzed.

Awesome. Alright. Would this produce the correct result? It would? Why?

Why is this correct? Because this thread is still doing one the right thing. Each thread is still doing the correct thing. Right? We're just redundantly doing the same thing.

Mhmm. So, you know, the red zero is gonna do a vector out of everything and write it. Red two is essentially gonna override it or, you know, depending on the ordering of things. Right? They're just overriding each other with the same correct value over and over again.

Right? So so it's still functionally correct. Right? Okay. So, yes, this is still functionally correct.

Mainly because, you know, there's no race conditions here. There's no reasperate dependencies. The ordering of things don't matter. Right? But, you know, potentially, there are some problems where this can cause some correctness issues.

But but not with vector add. Right? Vector add is is so resilient to human errors. Alright. So so in this case, right, how many additions are performed in this vector ad compared to the vector ad that you did in socket one?

But you know, the correct way. Right? So looking at at this problem, right, we have a vector of size n, block size of two fifty six. Right? So each block size is n divided by two fifty six ceiling.

Right. That's how many blocks we have. Right? Assuming we have this problem generically. Right?

How many ads have performed? Right? So how many of these these ad operations are performed? You know, ex excluding this ad. Right?

It doesn't let an ad operation in that for loop. Right? So how many specifically vector ads are performed? How many ads do we do? In, like, the correct version, it's, like, 10.

Right? So it's just in the correct version, what we add. Right? But in this version Since every single one is doing it again, it would be n times, n times two fifty six times n minus one over two fifty six. Which is also n?

Right. Is it? Sure. Is it n square? Yeah.

It's a bit more than n. No. N times n blocks and n additions. Well, okay. You know what?

We could derive this. Okay. It's easier if we just derive this. You can see the thought process. Right?

Let's just look at one single thread. Okay? Right? It's easier here if you just look at one single thread and then just generalize it to a whole block and then a whole problem. Right?

So for for a single thread, how many ads am I doing? Ads. Okay. And then how many threads do I have in the block? So how many ads is is my block doing?

Right? So every block is doing two fifty six times n. And how many threat blocks do I have? Yeah. Right?

Or or the number of blocks. In in this case, I just wrote n blocks. Right? So, yeah, this is the total number of thread blocks that I have times number of threads times the number of addition per thread. Right?

So, you know, you can just reason it through. Right? Starting from a single thread, and then you do expand it to the thread block in the whole problem space. Right? Okay.

And then the correct version would be would just be n. Right? Because I only have n threads, like, one thread per input element, and each should double only add one time. Right? So I'm performing a lot more ads.

Okay. The next problem asks how many loads to memory would this incur. Right? Not only am I doing redundant computation, I'm also doing a lot more memory accesses, which is gonna slow it down even more. Right?

So how many loads from memory would occur? Right? So so using the same approach, right, in a single thread, how many loads would there be? Two. Right?

So so where where are the two loads? Screen, whatever's a and whatever's b. Yeah. Right. So so these are reads.

This would read. This would be a write, okay, to memory. So every thread has two loads. Right? And then it's spelling it the same way.

Right? There's 256 threads and then whatever that number of ceiling of number of flat blocks. Right? So so it'll be times two. Right?

So 256 times m blocks have slow times two. Same thing as before, but times two. It's two load operations. And then in the regular vector add, if it's for how many memory access would you have? In the correct version.

Right? So if it's just like this, every data access how many memory? It's two. Right? It'll be two n.

Right? So, you know, you could privacy, and analyze this naive very naive version of the vector. Okay. Would this parallel vector add code up here run faster, or would a serial version on your CPU run faster? Yeah.

Why? Why would your CPU run faster? Is it more specialized for serial operating? Is it more specialized for serial operating? I mean, I'll I guess in this case here.

I mean, this is the serial version. It can be just multiple times anyways. Right? Yeah. So you think that you're passing memory to the.

Exactly. Right. Yeah. Right. So so we have the extra overhead of of passing memory as well.

Spam. Okay. Yeah. You get, like, 10 scans off the days. It's all good.

Yeah. So so in this case, right, from an algorithmic analysis perspective, right, that algorithm would be o of what? O of what? O of n. O of n.

Yeah. It's n, not n squared. Right? Yeah. Because each one, like, serially, you read twice, and then you add a computation for this adding, but that was right.

Oh, yeah. Almost n. So it's. Right? Yeah.

So so each drive does n steps, right, and loop iterations. So this is gonna be an old n algorithm, the same as your CPU, except, like I said, right, we have that overhead of moving the data that the CPU don't have to do. Right? So moving data over PCIe would be much slower than just a memory access. Right?

So, yeah, clearly, in this case, right, the serial version of the CPU would be faster, mainly because we have we do have the f additional overhead of of, data transfer and memory allocation and stuff. Right? It says log and build. I believe it. What did I say?

Oh, why did I say log? My bad. That should be o. I should fix that. Yeah.

You know, of all, I caught that mistake in, like, five years. Yeah. That way yeah. No one ever no one ever looks at my sample mentions. Yeah.

I wasn't there, like, reading and writing in the part. Like, by what I mean, do not have that issue? Because, like, what if the The what? Yeah. Like, what if the read read after something inputted?

Or they just think of a read? Yeah. Yeah. Oh, okay. They're they're they're they're read after reads, essentially.

Right? Yeah. So there's no read after writes here. Right? It's just a write.

It's what we're reading. Exactly. Right? So even with c, it's all writes. Right?

So it's just a write after write, but you're writing the same thing. Mhmm. So there's no issue. Right? Okay.

So so, really, you only have race conditions or true dependencies are are when you have readouts to write dependencies. But there's no readouts right here. If you hypothetically, if this is, like, a and a plus one or something actually, no. No. No.

That's in the future. Like, a minus one. Right? Then then that could cause an issue potentially. Right?

Because then there's there's a readout to write the same and see and go on. I don't know why you would do that. Oh, it's kind of a scamming option. I'm just gonna scan it. Okay.

But but yeah. Right. So if it there's a a and a minus one, then you have we have the right dependency, and then if you implement this full room, there's also a loop carry dependency. So, yeah, there's a whole bunch of, dependency issues here, dimension additions that you introduce. But but like I I I mentioned, right, vector add is a very resilient code for these, you know, to use their errors.

Alright. So then, you know, we can also have some, you know, just short answer type of problems. You know, if a program needs to access global memory every time for every operation, you know, how could that harm performance? Right? In general, accessing global memory is bad.

Why? But we need to access global memory over and over and over again. It looks like a really long cycle. Yeah. Global memory is really, really far away.

It's it's slow to access. You know, that's why we have a whole bunch of caches including your CPUs. Right? So we have caches and GPUs, and and your shared memory is a little bit tiny cache. Right?

So, you know, it's slow. It could stall a lot of computation. The way you would fix this is to cache it. Right? In that case, shared memory is one of the solutions.

So, you know, some of these have multiple correct answers. Right? You can also, like, write your code in a more cache friendly way to make use of caches. That's that's valid too. And then, for example, control divergence.

Right? So so why is or work divergence. Why is work divergence bad for performance? That was kinda like a major theme in the class so far. And the threads doing no ops.

Yeah. Their efficiency are not being done. Does that work? Yeah. All threads does that work must execute the same operation.

Yeah. Right. So so why is that bad if if we if if a thread has to operate different things Then it has to search between them, and it can't fully utilize them. Yeah. Right.

Yeah. There's other utilization. Right? Because the hardware, we saw that CNC stack mechanism. Right?

That that enforces the serialization of of executing the different paths. Right? So so they have that divergence of, like, basic block a and b. I'm gonna have to execute a with half of the threads, for example, and then b for the other half of the threads. So it's less serialization, you know, that that CNC mechanism in the hardware, results in underutilization on your phone.

So what's one way to reduce the impact of shared event, of of workday purchase? So how can we avoid workday purchase? Are you using proper indexing? Right. Yeah.

So we like, you can assign it too. Right? We could do indexing in a different way to avoid web convergence. Yeah. Okay.

This is the multiple choice that you could ignore. Right? So so I uploaded a one. Right? And these more or less kinda hit hit on the same same thing.

Right? So that multiple choice problem, you know, at eight to that box of fifty thirty six, which one of these one d would result in the greatest number of the SM? You know, going through the same logic. Right? You see what fits.

You see what the limitations are and see how you utilize that hardware is. And then that would tell you. So is it and it wouldn't be a case, fifteen thirty. Or 30 to 60 would be the best answer. Right?

In the next problem, you know, it just gives you four different conditions. And, actually, which one of these will cause warped divergence. Right? So warped divergence happens specifically when threats within a work, you know, do different things. Right?

So so looking at this, right, if if my block ID is greater than four, it means it's if statement can you know, based on that, Why would that not cause what type of this this case? Would would that cause what type of this? I mean, you can see the answer. But Because usually that means that, like, everything in that block would be the same thing. Exactly.

Right? Because every day every thread in the block shares the same block ID. Right? So all of your threads in the block will do the same thing. Okay.

How about this? And it's just, like, almost always true. Right? It's all yeah. You'll never you'll never, have a block in the.

But that would be, like, Yeah. I mean, then you throw an error. Right? I don't know. Yeah.

So that's that's almost always true. Right? Okay. And then this one oh, no. My pen stopped working.

Okay. Pen stopped working. Right? Okay. Yeah.

So the one. Right? Well, that will cause issues. Yeah. Right.

Because because now it's dependent on my thread IDs. Right? So so the rule of thumb, you know, if if you have any, like, a code block, if if it's dependent on thread ID, right, specifically, then it's likely it's gonna cause some warp type versions. Right? However, if I say, like, word ID less than 32, right, would that cause more divergence?

No. No. Right? Because because my condition is at the boundary of, like, 32, which is a more size. Right?

But, you know, that that rule of thumb isn't always true. Right? It just depends on what that condition is. Okay. Right.

So which one of these memories have the slowest access speed? Right. So your GPU has a a whole bunch of different memories. Right? You have your registers, your l two, your l one, your shared memory, global memory, local memory, constant memory, texture memory.

There's a lot of stuff we're not talking about. Right? And so on like that. So things, right, the slowest is is global memory. Right?

Which will be the fastest? The register. Right? Which one will be the next fastest? LL1.

L1? Oh, the L1. Register number, you mean. Oh, that that okay. That that was a trick question.

Are they the same? They're the same. Right. So so your l one and your shared memory is physically the same SRAM structure on the hardware. Right?

Yeah. Like like I mentioned, some of the earlier GPUs I don't know if the model ones let you. There's a command you can run that that could partition the size of your l one and shared memory size. Right? So so the total is always a fixed size, but you can partition, like, you know, half of 1 to shared memory and the other part two l one.

But physically, they're the same. Right? And then and then after l one and shared memory, what's the what's the next lowest? L two. L two.

Right. So assuming the memory hierarchy is shared l two l one shared memory in a register. Right? Right? So you have a general understanding of that memory hierarchy.

You'll be fine with these type of questions. Okay. Any questions? Okay. So so later on in the future, we'll learn about unified memory and how we can start accessing you know, a GPU can access the memory of the CPU and another GPU.

Right? So in theory, your memory hierarchy can now also be a remote GPU or a remote CPU. Right? Yeah. And then in in disaggregated servers, you actually have server blades of just all memory.

They design this type of stuff too. Alright. So true and false stuff. Right? You don't know you don't learn about DMA engines for us.

Okay. Ignore ignore 10. We haven't learned about that one there. That keeps thinking in there. Right.

So all words in the 03:00 share the same program pattern. Is that true or false? Now what would make that statement true? What? All threads in a warp share to say for the account.

Right? Yeah. It'll be this one right here. Right? So that one's true.

So this is false. So it's really warp versus thread flock. Right? So so your warp shares all the threads and the warp shares are program counter, and that's why it's so that's why you have warp. Okay.

So works can't finish executing at different times. Yeah. Right? Because because works can run independently from each other. Right?

So so when when the thread flock, right, in the hardware, the thread flock splits up all the threads in the different warps. Right? And then the hardware basically picks one of the warps that execute. Right? So so it's possible that the hardware could, you you know, favor thread zero, and then the lower ID threads kinda stall and run behind.

Right? So so your works can execute, you know, not at block step and finish at different directions. Right? Finish different times. Right?

So so how do we kinda address that? Right? We we saw that issue in in production where your threads could run at the your works that could run at different speed. Right? But our reduction, we wanna ensure that, you know, we have dependencies, right, between the different steps.

Right? Sync zone. Right? We had to enforce we yeah. Because of this, right, we had to enforce that for sync threats.

Right? So so if we don't have that issue, we don't need sync threats. Yeah. But we do. So so that's why we have sync.

Yes. The next one says, works consists of consists of multiple thread blocks. Right. So what would make that true? Inspector.

I can swap. You can stop it. Word. Threads consists wait. Threads consists of breath loss.

Growth. Multiple works. Right? But I'm sorry for me to do that. Yeah.

Or work consists of multiple threads would also be true. Yeah. These are basics. Right? We saw one fourteen was 16.

The maximum number of threads and blocks that can reside in the s seven varies by the hardware architecture. Right? We saw that in in one of the lecture slides where you look in the CUDA manual and inside that that table. Right? And you can see the different hardware have different hardware, constraints.

Some of them can hold 16 thread blocks. Some of them can hold eight. Some of them can hold twenty forty eight, ten twenty four, fifteen thirty six threads. Right? It just depends on the the hardware generation that the vendors create.

And then variable stored in shared memory are visible to all threads in inferno. False. Right? It's visible to all threads in a board. Block.

Right? So for shared memory, it's private to a thread plot. So So all the threads in the in the thread block sees the same thing. Right? But thread blocks in different wait.

Shared memory variables in different thread blocks. So, you know, on that note, also, just to go over another thing I just remembered. So going back to that reduction example here. Right? I think that might happen on the assignment.

But either way so so let's say I have, you know, 100 thread blocks that's running this reduction output now. Right? And every thread block has has a partial sum because it's private to a thread block. Right? So when I launch this kernel, how many partial sums are actually created in this case?

How many blocks are in? Yeah. How many how many number of blocks there are. Right? That'd be a 100 a 100 copies of partial sum.

So so, you know, if you understand that that shared memory variables are prior to a thread block, right, You you realize that there's multiple copies of it. Okay. Alright. I think that's all the sample. Oh, wait.

No. No. There's more. No. It's not.

Yeah. That's all the sample problems I have. Any other questions or? Any thread and also for the block. I don't it's like all threads in the block.

Yeah. Yeah. Yeah. Same threads would be, threads in the block. No.

No. No. No. No. Threat no.

All works. Yeah. All threads in the block, essentially. Yeah. Yeah.

And then if you wanna sync all thread blocks, you will literally just stop the kernel and launch it again. Yeah. That that's how you synchronize all the threads in the kernel in the old days. And now there's something called, like, cooperative threads that it's another API that NVIDIA made. It's a little complicated, but you could do that too now.

Alright. We're good? From my terms? And, yeah, questions? Were you facing the stuff you want on three series?

What time? Like the libraries? On the libraries and stuff? No. That was more for your final project just to get ideas of it.

Right? Yeah. It's like one through five. Has it only been five lectures? Well, like, there's been slides.

Wow. There's, like, no material. There's no term at all. Yeah. Essentially.

Right? Yeah. Everything from last week and before. Right? Yeah.

Everything out on Tuesday was just for the purpose of, final project just to get good ideas. Right? On that note, right, hopefully, a week from today, most of you would form our group and come up with ideas on your final project, then, you'd start messaging me and we could iterate on things. Right? Most of you probably procrastinate until Thursday, I assume.

Okay. Alright. So if there's no more questions, good luck. I have office hours today. Okay.

PA has it on Monday, and we have discussion on Friday also as well. You know, if you wanna have. Uh-huh.


Alright. Okay. Okay. What time is this? Eleven.

Okay. We can start. One sec. Okay. Alright.

So, hopefully you know, I haven't seen you guys in a week. Right? So, hopefully, you enjoyed your break from last week. Yes? You had a tournament turn?

No? Was it too long? It was? All for real? Just a little too long.

Okay. I'll make the next one shortly. Great. Okay. Okay.

Okay. Alright. So I'll I'll finish grading the midterm hopefully by mid next week. And then the the graders are helping work with the grading assignment one and two as well. So, hopefully, by next week, everything will be graded.

The thing right now is is the the final project proposal. So, hopefully, by today, most of you will create a group and come over the proposal. I might extend that to tomorrow because then, I guess, some of you guys might be in the discussion or discuss too. So, I'll I'll I'll probably extend it by by the day. That might help you guys a lot.

Right? Yes. Yes. Okay. Alright.

So I'll send a message later. But, yes, by tomorrow. Okay. So some of you already started, you know, submitting some of the project proposals. So I'll I'll start going through it and commenting right now.

So I I see a lot of WebGPU stuff. Right? That seems popular nowadays. Right? So I know you could do some compute with that to do as well, so that that fits really well.

Some of them are, like, pure graphics proposals, which I don't know that this missed last, but I'll talk to you guys about that a bit. But other than that, so far, I I like the originality that you guys have so far. Okay. So so, you know, before we jump back into, you know, more hardware features and algorithms on on how you can, you know, modify your GPU code to run, you know, better on on hardware GPUs, There's a few other more modern CUDA features. That's not, you know, kernel specific, that I wanna talk about because this is also somewhat relevant to final projects depending on the type of code that you that you wanna work on.

Right? So, specifically, if you want to convert some of your c c plus plus code that's very object heavy, this this feature will make your life a lot easier. And, also, just in general, like, any any GPU code that you write, you know, if I remember, would would make your life a lot easier too. And then, also, if you do, like, a multi GPU program as well. Right?

So this was one of those features that NVIDIA introduced, maybe, like, eight years back or so. And then it's been improved ever since. Every year, there's a there's a bit of speak to it. So I'm stealing slides again from GDC, from from the slide deck in, I think, 2020 and, so some from 2018. But this gives a really good introduction to to what unified memory is.

Right? I don't know if you've heard of unified memory before in CUDA. No? No. Okay.

So this is not a different sound. Right? So so your your GPU is a discrete device. Right? So far in the past, we manually had to allocate memory and then move the the data back and Right?

So unified memory is a feature that basically it's like virtual memory for your GPU. So you could basically make a pointer or allocate data on your CPU and use it directly on your GPU without you having to manually move things. Right? You know, that that's easier for you as a programmer. It improves programming productivity, especially when porting programs over.

We'll see a few examples of that. But, of course, at the cost of probably some performance penalties. Right? Because now you have page falls and whole page table, and then you had to migrate data manually, even though, you know, you as a programmer might know how the data should be moving. Right?

So so we'll see how unified memory work, probably makes your life easier as a programmer. And then how can you optimize unit time memory so that you get more performance improvements similar to, you know, your CUDA mem copies and CUDA mallocs and stuff like that. Okay? And especially if you're thinking of, you know, doing a final project related to c plus plus. Right?

Like, porting over code like some of the folks did from the previous years. This is this is a feature that that you use heavily. Alright. So this this topic kinda covers a bit on, virtual memory. Right?

So we haven't took an operating system. So, hopefully, this is also, like, a very excellent introduction to gauging and stuff like that. But I think most of you, we took one six one and and one one five three by operating systems. Right? You'd be pretty familiar with that.

Otherwise, if you haven't, the concepts here are pretty high level. So, so, basically, in CUDA six, that's when they introduced unified memory. So instead of you viewing your GPU as a separate memory, right, the whole notion of unified memory is now you can view the whole GPU as a single memory space. Right? So this way you don't have to worry about, oh, where's my data?

What direction is it moving? This this simplifies your code porting specifically. Right? AMD's GPUs actually has been a bit a bit more advanced in in terms of, unified memory. So, AMD, they have a product called APUs.

I don't know if you heard this term before. APUs? No? In their old days, they they call it APUs. I don't know if they call it that anymore.

But they have a on on a single die, they integrate both a CPU and a GPU, and they both are directly connected to the same memory. Right? So they all use the same DRAM. So this is what unified memory, you know, gives you the vision of, like, virtually. This is how physically AMD GPU APUs are.

Right? So that powers a lot of the supercomputers as well. So so by default, AMD has unified memory. Right? So with CUDA six, right, this is the introduction there.

There's some limitations. I think some of you guys wanna use Jetson devices for your final project. Right? Some of these limitations in the early days still apply for even modern Jetson embedded system boards just because they have a lot more memory constraint. So the way you program Jetson and use unified memory is a little bit different than if you program a regular computer or a server.

Right? So that's why some of these oldest story to context things are still relevant even though couldn't fix other than the news, like like, eight eight or ten years. Right. So in the old days, right, the main limitation that and currently in Jetson is that the amount of unified memory that you have is limited to the amount of GPU memory. Right?

So it's not full unified full full on virtual memory like we have in CPU where you have division of, like, infinite memory. It's basically just transparent mem copies that you wanna take up it that way. Right? So that that's how the old Unifin memory is and and currently with Jetsons. With CUDA eight, about six, seven years ago or so.

CUDA eight got released, and then that finally implemented, you know, like, full on page table support with the view of infinite memory. Right? So this is how you view CPU programming as, right, with infinite virtual memory. And CUDA eight introduced that feature. So all modern GPUs, discrete GPUs, right, not not Jetson or or embedded GPUs.

But all discrete GPUs more or less has infinite virtual memory. Right? You you know, that's what unified memory is nowadays. So all of the GPUs that we have in in vendor have this notion. So if your GPU only has 12 gigs of memory, you could write, piece of code that could work on, like, 48 gigs of memory if you want nowadays.

This could just be swapping. Right? So, you know, in CPU, when you swap, where do you swap your data to on a CPU? This. Right?

And it's slow. Right? So on a GPU, we actually swap to other GPUs that's mostly GPU or the CPU. Right? So so the CPU adds in the swap for your GPU in this case here.

So we'll see how this works, in different use cases, as well from that programming aspect and also under the hood as to how it works as well. Okay? Alright. So the main motivation really for having this feature was, you know, you we wanna program GPUs like we program CPUs. And over time, that's kind of how all of these new features have been introduced.

Right? Programming GPUs get a bit easier. What about unified memory? All these implementations in Python wouldn't really work. Right?

Because, you know, you don't really have pointers in Python. Right? Everything is just transparently moving back and So unit five memory allows that as well. Right? So, you know, if you have the CPU code, right, you know, we're now offering some data.

You know, we're reading the data from a file into data. We're sorting it on the we're sorting it, and then we use the data and process that sorted data. Right? And then we free it up. So if you wanna convert this to CUDA code, instead of, you know, just worrying about, oh, how do I parallelize my sort algorithm?

I also have to worry about the data movement manually as well. Right? So here, I realized, oh, the data that I need to sort, I need to allocate it and then do a CUDA mem copy, and then CUDA mem copy back the results. And then when I process it, you know, when I'm done, then I do CUDA three. Right?

This is the things that we've done already in class, you know, from all your assignments. Right? So so the code base that we work on is pretty small, so it's still, you know, relatively manageable. Right? But if you have, you know, a really large code base with lots of data and variables and and different algorithms, right, it becomes, very unmanageable.

Right? So, unified memory, you don't have to worry about CUDA memc copies anymore. Right? Even in your homework assignment, you don't have to do that. You could just replace it with really just replace your mailbox with a CUDAMalloc managed, and and that's more or less it.

And then you don't get to do it as a synchronous because we wanna make sure that that hue sort is set. So, you know, looking at this, right, it becomes a lot easier. So, really, the only only change, is that your malloc changes in the CUDA malloc manage. So so now we're allocating space in unified memory, and we don't have to explicitly move data. Right?

We we still have to put this device synchronized here mainly because actually, yeah, any guesses as to why we need that? Because kernel is synchronous. Yeah. Right. So so kernels you might not be aware.

Right? Kernels are asynchronous. So when we call qsort and when that function returns, qsort actually haven't run on your GPU yet. All it's doing is it's sending a command to the driver, and that's it. Right?

So if you don't put that device synchronized by the time you call use data, it's possible that q story still hasn't run on your GPU yet or it's currently running. So you use q story I mean, so you use use data, and you're using stale data, essentially. So we need that device synchronized to make sure that the q sort ran on the GPU and it returned the results, and then and then we continue on. Right? So that's why in in, your assignment one and two, every time you call your kernel, there's a device synchronized after it.

You might not see on the kernel dot c u, but it's in the main dot c u if you if you look at it. Right? So so all these, function calls are asynchronous. Okay. Yeah.

Any questions? No? Okay. Alright. So so, you know, in in malloc, right, when we call malloc, we get a a virtual address.

Right? So, basically, if you have a CPU, all of those are virtual addresses. On the GPU case, it's a unified memory address, but, you know, it gets converted. Right? So you could throw that same, pointer, right, data into f read at a GPU, And, automatically, that whole unified memory run time will figure out, where that that's located and whatnot.

Right? So so in a CPU, right, how do you know if your piece of data is in your CPU or in your disk or somewhere else? Right? If you took operating system, you implemented this. Right?

How do you keep track of your your version of the physical addressing? Page table. Right? So underneath the hood, it's it's just a page table as well. Right?

So just to give an idea of what's happening underneath and, you know, why things could be slow and whatnot like that. And and and an introduction to, page tables if you haven't took it. Right? Basically, every single GPU has its own page table. Right?

It doesn't make sense for every GPU to share the same page table because then, you know, where would you put that page table if you have, like, eight GPUs? Right? Now I have remote access to that page table, and it'd be slow. So every single GPA has their own individual page tables, and they're kind of coordinated, at the unified memory space with a more, like, hierarchical, like, global page table. But either way, this is kinda like the simple mental model of, a page table and what's happening.

Right? So so we have different addresses. In this case, we call it page one and page two. Those are just unified memory addresses that you get with manage. And there's a page table that points and maps, you know, the page one and page two, into the physical address.

Right? So so the physical memory, in this case, you know, page one is backed on GPU a. Page two is is physically located in GPU B. Right. So so, you know, if you wanna access the data, for example, address one access page one, and that's the page one.

Right? That's a hit. So we can just access the the page normally. If you access page two, you know, that's a hint. But in this case, if we wanna access an address three that's in page three, you know, that that's a page fault.

Right? A a page fault is basically when I wanna access a data a piece of data that does not exist, you know, in my local memory. Right? You know, you saw that a lot in in one six one and and one five three. And you implemented this, I think, too.

Like, how do you handle page misses and and how you swapping and stuff like that. Right, so, essentially, you know, unified memory at runtime is is implementing what you did in operating system, but at the runtime level. Right? So so what happens after a page fault? Right?

So so what would happen or how would you, you know, get that data that you need in this case? What happens on a page fault in general? What did you implement in one five three? Segmentation faults? No?

We didn't implement this. Oh, you didn't implement this? Yeah. We learned about it. Oh, you learned about it.

We didn't implement it? Oh, that's not fun. I thought it went through the list going through and get something from this. Yeah. You kinda have to fetch it from this.

Right? So, if it if, you know, in in a CPU case, right, if you miss in the page, you go, it's not on your local memory. So it's somewhere in disk. Right? So you have to fetch it from this, and then you update the page table.

You have to get back in. So the unified memory, they use something called lazy allocation or, like, on demand allocation. So when you kind of put them out, like, manage, right, we actually don't physically back it up anything yet. Right? Because there's no actual data yet, so I just make an entry in the page table, essentially.

So in this case, page three is kinda like that that scenario where it hasn't been touched yet. Right? So, basically, wherever we touch it that's when we physically allocate a page. So in this case, you know, the there's a page file. It doesn't exist anywhere yet, so we can just allocate the page here, in GPU b.

And now I could access it again. Right? So after creating it and updating page table, I could replay the access, and then that's a hit. So then I could, you know, go to the next instruction, essentially. Okay?

So so these are the simple scenarios. But now let's say, GPU a, now I wanna access both page two and page three. Right? So what what would happen now after this? We would allocate to it, but now I already have a copy on the other GPU.

Can I keep both? Can I keep two copies of two and three? Should I make another new allocation of page two and three? And I have two copies. Would that lead to any issues?

Some synchronization issues if you need to Right. I might have I might have yeah. If I make a new copy, I might have two versions of variables, essentially. Right? But you could do, like, copy on it just to go the regular cache.

And you just kinda synchronize them? Then then we press cache for the calls at the hardware level. We don't have this across GPUs. Right? So so the simplest way would be to just invalidate the other copies, essentially.

Right? And and, you know, that's where a lot of these performance issues comes from. So so, you know, in in in your scenario, right, if you know how the data access pattern would be, right, potentially, you could prefetch it manually by doing by putting them in copy. Right? So that you avoid the page fault.

But in in unified memory, right, if we just let the unified memory system run time and do its own thing, right, we incur a lot of these overhead type, page faults. Right? So during this page fault, there's a lot of things that we have to do, actually. Right? So the thing is we have to check to see if page two and three exist in any of the other GPUs.

Right? If you have a lot of GPUs, right, this might have to go back to the CPU and do a global check wherever that page table lands at the runtime. And then if we find that it exists, then we basically have to invalidate those page table entries. Right? So so we unmap, the page table entries for two and three from the other GPUs wherever there there exists a mapping for it.

And then after mapping it, then we have to physically move the data over. Right? So then, internally, this is this is a good enough copy, essentially. You're not explicitly calling it, but the but the unified memory run time is doing this. So then after it does a copy, then I could update the page table at my in in my GPU a.

And then I just make clear the access. Right? So now I have everything that I need. Okay? You know, clearly just some overheads here.

Right? So just kinda thinking ahead. Right? How can this process be made a bit more optimized? Like, how do you think we can optimize this and and make this faster?

No? No. I guess this Yeah. Like, how can you optimize this process? So so where does the overhead occur?

Right? So the overhead occurs from Checking whether it's in another GPU. Right? Checking whether this is another GPU. Okay.

The mem copy is we have data. Right? So it's an unavoidable Do you have to? Yep. Otherwise, how do we act on the data if it's not Yeah.

I guess on the miss, you you have to. Right? Yeah. Okay. So so the other scenario here could be like but after GPU a uses it, now GPU b needs the exact same data again.

Right? And then a needs it, and then b needs it. Right? So then that that kind of ping pongs back and Right? Cache validation.

We should just But the cache does that too, basically, at the hardware support level. And you they it's called cache slashing as as level. Yeah. But, like, if k doesn't modify the memory, then we could just, like, reuse those Right. Okay.

Yeah. So that's a good point. Right? If the data access pattern is not modified, it's, like, like, a read, then it's safe to have two copies, for example. Right?

So so that page, depending on the access behavior of that page. Right? Whether it's, like, read only, write only, mostly read, mostly write, there's optimizations that you can play with. So for example, this page is only reads. Right?

Let's say you're reading a large dataset. You read the data, but you actually don't you process the data in a different memory space. Right? Then you could definitely keep two copies. That's safe.

Right? It's read only. There's no way those two pages are getting consistent with each other. So so later on, we'll see that you as a programmer know these data access patterns so you could pass hints, with, the this is the API to pass hints to the the runtime on how to manage these pages. Right?

So so at a high level, right, the unified memory runtimes, you know, the functionality was implemented in CUDA six. And then every version later, they kinda add these new features. Right? Full on virtual memory support, and then they allow you to add hints and so on like that. And then there's there's hardware features for, like, comic operations and so on, you know, another generations.

But at a high level, right, this is the general functionality of it. So we'll see how we could optimize this again. Right? So they get to, you know, your final project. Right?

If you have a large program that you wanna convert to unify memory and it's slow, some of the optimizations we're talking about in class can can definitely speed things up as well. Okay. So, GPU memory basically you know, time memory is basically, on the map paging. So, you know, physically underneath. Right?

This is the mental model of how everything works. But, you know, now looking at code again, going back to that, you know, Naruto analogy. Right? You could see the data flow through between the CPU and GPU now of of, this piece of code, essentially. Right?

So, you know, if we as an exercise. Right? If you just go down through each of these line of code, we could just try to phrase where to say that it would exist. Right? And and then we'll see later on how we could optimize it.

So, you know, looking at this piece of code, right, the cuDML advantage, after I allocate memory and and unify memory, where where does my data physically exist at this point after this API call? Nowhere. Nowhere. Right? It it doesn't exist anywhere.

If if it was older GPUs before Pascap or Jetson, the answer here would be it exists in both CPU and GPU. That that's the reason why we can't have virtual infinite virtual memory space. Right? On those Jetsons or three Pascal generation it does not support that in the hardware? Yeah.

The hardware just didn't support it. Right? It it's kinda like, chicken and the egg on top of thing. Right? So given the hardware that they have, they implemented the software support.

And now because they have the software support, now there's a reason to implement this on in the hardware. Right? To have full pretty stable support. Right? And, also, just just a Jetson voice on they're very low power.

Right? And and they don't slot. They're they're so they're really good for what they are. It's like a little, like, tiny chip that's, like, 10 times faster than a Jetson board. Like what?

Like, some of those accelerator. Oh, but that's, like, a very, you know, program specific. Right? GPs in which I don't put this. The back of my days when I was building robots in undergrad, Right?

We would have we would have to put, like, Intel four processors, essentially, and it'll be so hot. But luckily, we were throwing that under the water anyways. So we we literally just had, like, the coolant go outside of the submarine and just, like, put those tubes and this wire around just so when we're driving, the water would just naturally cool the whole whole CPU. And even then, it would overheat and leak and everything else like that. But how does you throw in a jetson, and then you have, like, 10 times more power than that, and you have, like, no heat.

Right? So but the Jetson actually are, like, really good for what they are. But yeah. Yeah. But they are very limited.

Right? Because it's smaller. It's, low powered. A lot of these hardware features, they kinda strip from those GPUs. So they have a lot of these limitations.

Right? So Zana just said after this, it would exist in both places. Okay? So then we call memset. So when we call memset, right, I'm accessing this data.

Right? So would that cause a page fault? Would it cause a page fault in this case? Assuming modern GPUs. It would.

Right? Yeah. Because it doesn't exist on a CPU or on a on a GPU. So this is gonna cause the page fault. Underneath the hood, we're gonna actually allocate the data on the CPU side.

And then we could start processing whatever this function is. Right? And now when we call this function on the GPU, is that gonna be a page hit or page miss on the GPU side? Yes. It'll be a miss.

Right? Because the data exists on the CPU. Right, this will be a miss, and then we're gonna allocate on the CPU. This won't miss us well initially, and then we're gonna have to allocate on the GPU. Another strike?

Same move. Same strike. Oh, on the what? Nurse? The service?

Facilities. Oh, okay. Cool. Alright. Until they walk past those, I think, I think.

Alright. Right. So this is gonna miss on the on the GPU side, and then we're gonna move the data from the CPU to the GPU. Right? So it'll be exactly like the size we saw before.

But instead of two GPUs, it's it's CPU and GPU. And we just let them walk by for the next minute or so. I'm not sure if this is recorded. You can hear it. Yeah.

So I like, when you're, like, thinking about or when you're, like, reading about, like okay. I gotta tell this this. When I'm thinking about, like, the regular code arranged, I always think, like, like, it might not behave exactly the way I ran because, like, from my optimizations. Yeah. Is that the same case for, like, the GPU?

Oh, yeah. Yeah. Yeah. Yeah. So so so GPU code actually goes through two layers of compilation.

So there's the one that goes from, your c code to PTX. And then in the back end, PTX gets compiled to SAS again. So during both of those, they actually do some optimizations. So it's very possible that the code that you write gets compiled in something completely different. Right?

And and, you know, just with, GCC. Right? You know, you have, like, dash o three and o two and o one. You know? There's a trade off in code size and your performance.

GPs are exactly the same way. Right? In fact, I think if you you you know that Godbolt website Yeah. That that shows you or compiler explorer? I forgot what it's called.

Is it compiler explorer? Right? I think you could actually set GPU as one of the compilers, I guess, and you can actually see the the PTX code, as it's optimized. Right? And that's how much it is.

It's really cool. Yeah. So so, yeah, going back, you know, these these are API calls, so the compilers doesn't really reorder this. Right? But in your kernel itself, right, there's a lot of reordering at the at, like, an optimization that the compiler could do.

Right? So, you know, after we call that set value, we have a miss on the GPU. Data get moved over to the GPU. And then, you know, we call, again, this one on the on the CPU side. Right?

So when we call use data on the CPU side, will this be a hit or a miss? That'd be a hit? Well, it's depends on if it's, like or what does the set value I I this is in my update every value. That would be on the list on the CPU because it's only updating the GPU. Yeah.

Because because most of the time, all the data you would touch would would move to the GPU for processing on GPU. So then by the time I I wanna use on the CPU again, everything already migrated to the GPU. Right? So the way the migration works from from CPU to GPU here is that we only migrate a page when there's a page bulk. Right?

So for example, if only process half of this data, like, in this function, that means half of my data sits on CPU dot happens on GPU. Right? We we only move at the page boundary unless you're on a Jetson. Right? So if you're on a Jetson, when you come to set value, it does a MIP copy of everything under the hood.

Right? Because it doesn't they don't do it at the page boundary, so they do a bulk transfer of every dent, on a different point. Right? I mean, you know, very rudimentary implementation in the old days, and and they still use that in JSON. Okay.

No. So, you know, this is, from your point of view, right, how this could make it easier for you when you do your bank data management allocation and so on. Right? Like container cluster? No?

Oh, yeah. Yeah. This is asking you to last their business as well. This one? No.

This is just the deallocation. Right? We're not actually accessing anything. So this is just free. It just kinda you know, all the entries in the page table and and deallocate it.

The one above two. And you say that? Yeah. Yeah. This will be a mess.

Okay. But yeah. Yeah. Because everything's on the on the CPU. We're completely closed.

Yeah. So there is no way to, like, to only copy the certain pages. Like, if we did it without managed memory, then we would just be mem copying, like, everything unless we know exactly which sections of it would Yeah. Right. Yeah.

So so later on, there's actually a feature called, like, throughout the API. It's like, could have something pre fetch, which is essentially your mem copy with a unified memory wrapper around it. Right? So if you if you know specifically the program that you wanna move, right, you could do that pre fetch operation as well, and then you avoid page miss page faults. Right?

So that that would actually be faster. Yeah. But it's just to clear when the these data would be a minute before it's migrated. The Yeah. Right.

So so we would because because by the time I hit this, all of my data is back on the on the GPU side. So on when I call this again on the CPU side, like, the data's not on the CPU anymore. Right. So, you know, it's just a few lines of code, but there's a lot of stuff moving in the going on in the in the back end. Okay?

So this is, this is a bit of a summary, you know, for things that might be interesting, you know, for you if you're using the Jetson. Or you have, like, a really, really old GPU, like, the GPU you have when you're in middle school. Right? Yeah. Do you have NVIDIA GPUs in middle school?

No? Some of you did. Who had one? Man, you're rich. Now I'm just.

Wait. What do you guys play games on? AMD in middle school or just PlayStations? PlayStations. Oh, yeah.

Okay. We still get. And then we okay. Nintendo. The classic.

PlayStation for the. Yes. Okay. I was bored growing up, so, yeah, I didn't have, like, those handheld stations. So every time I wanted to play games, I would download an emulator and then, you know, get to all those cracked games and play on it.

Oh, shit. I'm recording. Just kidding. No. It was fun.

Right? Like, when you're playing Pokemon, you could just play out, like, 100 x, and you could level up so much easier that way. But, anyways, right, you know, if if you're using a JSON, right, these are just some of the constraints that that you have over regular unit time memory space. Something to keep in mind of you if you call that route. Okay.

So so with some of the older GPUs, right, there is a physical limitation to the amount of data that you can allocate. So, you know, with Pascal and anything older than that I mean, anything older than Pascal, like, Kaplan and Maxwell GPUs. If you try to allocate something like that, which is 64 gigs, that would actually fail. If you try this on on the better GPU, it will work. Yes.

In fact? This may actually bring down the system if you use it very heavily, right, of all that swapping back and There's a new approach to denounce service attack if you want to. No. Besides just fork bombing stuff. Okay.

Right. So so this essentially enables modern GPUs to to do oversubscription. Right? So oversubscription, it is basically, you know, with your unified memory virtual memory on CPU space where you oversubscribe the physical memory. Right?

You you could use more memory than there there there are physical DRAM, essentially. Right? So with oversubscription, this is how the unified memory essentially handles it. Right? And, clearly, there's a trade off.

Right? So, you know, on on a CPU, when you're swapping your disk, it's super slow. Right? Very slow. So, similarly, when we're open describing, it's slower too, but it's still preferable in this case to not being able to run it at all sometimes.

So that's why we have this feature, and it's used heavily a lot in in a lot of, like, sometimes machine learning workbooks and so on like that when when things don't fit in in your, memory space. Right? So in this case here, right, you know, GPU a, all of its physical memory is already full. The place table is already allocated for it for that. Right?

And then, you know, we wanna access page five, which is this exists on GPU b. So in this case here, you know, since GPU a is already completely full by sending to access page five, how would you handle that? Kick out the We kick something out. Right? Something we use.

Yeah. It's kinda like a cat. Now now you can start to see that this this is essentially acting like a cache more or less. Right? You kinda you kinda think of it that way when when it's oversubscribed.

So you learn in one six one and maybe one five three. Right? When you have cache, you know, the thing you wanna kick out is is the data that is is, least recently used. Right? If you kick out a data that's most recently used, what's gonna happen?

Right? I I use it again. Why would I kick that out? Right? So so there's, you know, various LRU algorithms that's that's implemented.

So, you know, we kick one of them out, and then we basically swap it in, essentially. Right? So in this case here, you know, we'll realize, oh, let's kick one of these pages out. The backing of a GPU would be another GPU if we have another GPU. Right?

So we would swap to another GPU. If we don't have another GPU, that thing on the right, just imagine it's it's a CPU, and it works exactly the same way. Right? So in this case, we're gonna swap a page from GPU a to GPU b. So then, you know, we we add that page table entry or GPU b to make it valid.

Right? We invalidate that page five entry for GPU b, and then we move the data over. So we move the data over it, and then we create the mapping on on GPU a. Right? So this is that that, essentially kinda like the caching type of behavior.

I've only oversubscribed on on the unified memory side. Right? Okay. Any questions with this? Is it following?

Yes? If the one gets full, does it just change where you keep swapping until the end of the Yeah. Right? So yeah. Right?

So let's say we we wanna access page six after this. Right? We would pick another NLU page. Yeah. So page five wouldn't be kicked out by another one of the other top three would get kicked out, depending on what that LRU least recently used algorithm is.

So I I don't know if you implemented LRU algorithms. No. You haven't. Right? You know, there there's a bit associated with each page that kinda keeps track of when it was used at a high level.

Right? So you can look at those bits or metadata in in in software, right, counselors. And then they they make that decision. Right. So so one interesting thing now is that you could kinda handle these concurrent memory accesses because now you have unified memory.

Right? So, you know, some of the questions before was, like, what if I don't access all of the data? Right? What's gonna happen? So, you know, only the pages that are accessed are actually swapping in and out.

Right? So so in this scenario here, notice that, you know, we don't have any synchronization here. Right? So we have data that we're accessing on the GPU side and then, like, one piece of data that we're accessing on the on the CPU side. Right?

So so my kernel, and and kernels in general, right, they're asynchronous. So that means it's possible that when I run this piece of code, both of my kernels will be accessing that data at exactly the same time. Right? So that's what we call concurrent access. Right?

So how will we handle that? On on Pascal, you know, the old version, right, that'll just be a page fault. It's kinda handled kinda in order type of thing because you do a cold transfer to CPU anyways. But in the modern version of things, right, both of these can actually have a race condition, essentially. Right?

Have you guys, you know, ran into race conditions before? Right? And, wait. So you guys don't do pair what what in parallel processing? One sixty?

Right? Okay. But outside of that, you you never saw race conditions? No? Okay.

On my old time. On your old time. You ever had a piece of code where you go, like, 10 times and it works four times? Right? Most likely, that's a race condition sometimes.

Right? It works sometimes. It it doesn't work because of the just the ordering of things. Right? So in this case, right, if we have two concurrent accesses, right, one from the CPU or from the GPU, or more generically, right, it could be from, like, two threads in your CPU program.

Right? Just depending on the order of access, you'd have different results. So so in this case here, right, it would just be, like, whichever GPU gets to that piece of data it'll fault. You know? It'll try to access it.

If there's a fault or not, then it'll transfer back and Right? So this is kinda how that's handled, concurrently. However, you know, there's different optimizations for things that we can see later on too. Right? So so we'll see how you can handle that.

So so you the GPU could handle it exclusively, which basically means, you know, only one GPU can access that page at a time. Right? So in this case, right, if both GPUs require page two, you know, it's gonna hit in page in GPU b. Right? It's gonna it's gonna hit in GPU b, so it's gonna process it here before migrating it and then processing at GPU, a.

Right? So so it's set to exclusive access. That's essentially just whoever touches it is gonna get it Okay? However, you know, there's some optimizations under that that could make this a bit faster. Right?

We mentioned before that maybe the page is only read only. Right? So if the page is, you know, identified to be read only, either through programmer hints or, you know, the runtime could do some kind of memory access kind of analysis and and and mark it as read only. It'll it'll say it's safe to actually make a duplication. Right?

So in this case, you know, we would just make a copy and keep keep both hooked up, essentially. Right? And that's perfectly safe because, it's just reads. Right? We're not we're never gonna have those pages become inconsistent.

And, you know, in in the the CPU caching phase, right, if I wanna all of a sudden, you know, update this and, you know, write something new to it, the way we handle this is that we would just squash everything else that exists in the system. Right? If we ever want to read, write to it. So so there's there's mechanisms in place to to handle that. Right.

Oh, yeah. That was the next slide. Right. So we would just invalidate all the other copies, essentially. So we don't have any stale stale copies.

Okay. Any questions? No? Okay. So now that we have multiple GPUs with unified memory, we can also have atomics that work at a system level.

We won't talk about atomics for, like, another two weeks. But atomic operations are basically, let's say you have, you know, like, a a variable a, and multiple threats wanna, like, want to, you know, do updates to it, like, a plus one. Or or the other more general cases, you know, when when you're buying an airplane ticket and then you select a seat, right, we have to make sure that, you know, that seat selection is an atomic operation because we don't want, like, two people to buy the same seat at a time. Right? So only one person can can do the update to to buy that seat, essentially.

Right? So that that's kinda like when when atomic operations come in. Right? An atomic operation is a re modified ride that that can't be split up. So now because we have multiple GPUs and we have a unified memory, the hardware now actually supports atomic operations in in the interconnect.

Right? So so PCIe actually, in the newer PCIe standards, they have hardware atomics, and then NVLink what NVIDIA created outside hardware atomics to support this as well. So so with the new hardware features right. Sorry. New software features.

Right? The there's hardware features that get implemented, you know, a couple generations later to to support that. So, you know, we can have atomic operations as well. Right? So so atomics is interesting that specifically with NVLINKs, it allows us to actually have remote access now.

So your GPU can access the memory of another GPU directly without doing a a transfer. And this is basically enabled because of of the interconnect, technology. Right? So for I mean, I think we're all too far to buy these separate GPUs. The only GPUs you can buy with NVLinks, I think, are, like, supercomputers and and all the data centers for the m MLAI type of things.

You guys could buy two GPUs and then you have that SLI link. Right? Or do they call that NVLink too also? They never call it NVLink? In two GPUs?

Yeah. Okay. Oh, I know. K. If you're rich, you have multiple GPUs.

In the past, you have SLI, right, to kinda stitch it together. So NVLink is kinda like that. It's not PCS. It's NVIDIA's own version of inter GPU communication, but it was meant for, compute, not not graphics. Right?

So so NVLink exists in and we'll talk about that more later on in class, in all the data center GPUs. Right? So so we can access the data of one GPU to another, transparently. Right? So so this is actually kinda cool.

And especially if you have that scenario where you're where you're constantly flashing between two GPUs, you could just now access other GPUs directly. Alright. So, you know, that that's kinda like, under the hood, how everything works and some of the hardware features that enable it, like, including. Oh, yeah. But how does this affect you as a programmer?

Okay. So before unified memory, it was actually a huge pain in the belief to handle, data structures. Right? So Ignoring the code on the bottom. Right?

If you just look at this data structure up here, and now let's say I have an array of this. Right? But for every objects or, you know, every one of these structures objects. Right? How many memory allocation spaces is associated with each of these data elements?

So if I wanna create one of these structs, right, and and see how many times would I have the malloc to create that? Three. Three? What's the three? We need a space for the int, space for the like, the space for the pointer.

Okay. But you could get that as one malloc because a struct is contiguous. Right? You could just size up a struct. Right?

So you could just decide the struct malloc. So all of that is just one malloc. Do any other other mallocs? Buffer? Yeah.

You need one for the buffer. Right? So so each of these data elements, right, there'll be a memory allocation for the data element. Right? And then one of these last entries here is a pointer to your name buffer.

Right? Like that by the way. So now let's say you have an array of this. Right? Let's say you have an array of data elements.

So so, you know, you have an array of, data elements. Right? So every single one of these entries Would be a struct. And every single one of these, what, have a pointer, to a buffer associated with it. Right?

So so, you know, at high level, all I all I have is an array of struct data elements. But, physically, in memory, it's an array pointing to a whole bunch of other, memory allocations. Okay? So in the old days, if you want to do a copy, right, let's say I wanna just copy my data element. So I do a CUDA mem copy of only my data element, and then I try to access that on the GPU side.

What would happen if I just copy this directly and try to access that on the GPU? Access on the Or, like, I don't know. Right. Okay. Right.

So so if I just copy this whole thing to to GPU from my CPU, right, would this data still be correct Yeah. On the GPU side? The key key and that would be right, but name would point to something. Yeah. Right.

So so what this data would look like like, for example, a key could be 10, the length could be four, the name could be video or something like that. Right? Okay. Actually, no. That chart star name is not a video.

That's actually a pointer. Right? So this this would exist somewhere else, and this this that would actually be some number, like F f four, something like that. Right? And then that that actually points to this.

So so when I do a copy, I'm gonna copy ten, four, and s f four. Right? So if I try to access that on the GPU side, right, my key would be valid, my length would be valid. Would my name would that pointer be valid on the GPU side? No.

No. Right? Because because that F f four would be pointer on the CPU side a virtual address on the CPU side. Right? So so they want this to be valid.

I would actually have to do a deep copy. Right? So so a shallow copy, you just copy that layer in the in the data structure. A deep copy is what when you basically copy everything else that that, you know, the pointer points to and so on like that. Right?

So to do that to do that operation, right, that means that I would I would actually do a deep copy operation. Right. So that that name, that field name would have to be transferred to the GPU. Right? So I would actually have to allocate space on the GPU for the name, copy the value of that name, and then and then update that pointer to the to the GPU address.

Right? And and that's essentially what you have to do. Right? That that's what that code looks like here. So it's a huge pain to bleed to do this.

Right? So you would you would allocate the array of data elements. Right? You would copy over every data element, you know, n elements of size of data elements. Right?

Size of the struct. And then for every single one of your data elements, you'll have to have to allocate the space of length for that name, and then copy from host device of that name, and then a pointer as well. Right? And then and then you update the pointer as well, essentially. Right?

Because because the the the pointer you have on the CPU side, so you have to move it over to the GPU side. Right? So this is very, very difficult to do, especially in c c plus plus. Right? So so last year, you know, when a couple of groups had their own, you know, side projects that they were working on, You guys use data structures everywhere, right, in your in your actual code that you've used.

Right? So when you convert it to GPUs, one of the challenges that they had was really, how do I handle all of these structs and classes and everything else like that? So so, you know, we would basically have to do deep copies and and and change it. Right? You know?

And and, you know, other structs like list and so on, a lot of data structures, half these pointers have to point to other memory allocations as well. Like, vectors are also like that too. So so this is all very difficult to handle until you unify memory. Right? So unify memory, all you really have to do is you just have to overload the new operator, and that's it.

So in c plus plus, have you guys ever had to overload an operator before? Right. I assume you guys did that in c s 10 or something like that. Right? Yeah.

So you can actually overload the new operator and the new operator. Right? So instead of you know, when you call, like, a new object, right, it it basically calls a memory allocator on the new. Right? So if you if you overload the new operator, you're just overloading where where the allocation or, you know, what the allocation process would be.

Right? So instead of by default, view is backed by malloc. Right? We just replace that with a managed device synchronized to make sure that that allocation is complete. So now whenever I I create a new object, the object is gonna lie in unified memory space.

Right? So if if all the data in that object belongs in unified memory space, you know, and and all my deep copies are also in unified memory space, I don't need to update my pointers. The pointers will all be in the same memory space, and I could use that on CPU and GPU, and it wouldn't be a problem. So so, you know, when we have this class here, right, You inherit from that manage class. That manage class is really just a new in the in the delete.

Right? So now that string class, whenever it allocates stuff by default, it's it's going to use unified memory space. If you call new. Right? And then if you if you call the constructor itself, right, you could just allocate the data inside of it.

You don't use malloc anymore. But everything internally, you would you would also use manage internally. So that whole you just wanna make sure that whole structure and everything under that structure is is is in unified memory space. Right? That's that's what all the.

Right. So now, you know, with that data element, right, you have a key and a and a a string. Right? All of this is other, you know, primary space. Now when you just call a new data element of size 10, you could just pass that data to your GPU, and it will automatically handle that deep copy, with just page faults and everything else like that.

Okay. See? A lot easier now with Unify memory. Right? So right.

Yeah. Yeah. So a lot of these features really were for for programmer productivity. Right? In the early days, there's a lot of weird limitations with GPU programming.

So these features make it a lot easier. So porting over code from CPU to GPU is just a lot more seamless. So now if you have a lot of c plus plus code, you just create a managed class and and make everything inherit from it, replace every malloc with with CUDA malloc manage. And in theory, that handles the whole data movement part of things nowadays. Right?

Of course, you know, if you wanna update a function in your CPUs, you will still have to write that code in CUDA, potentially. But other than that, from the data management point of view This is all this is all you have to do now. Okay. Questions? No?

Alright. So, you know, you know, this will be easier. In in this case, right, they're using example of graph processing. Right? Graph processing is is, you know, very irregular, kinda like a default thing that that they like to show off.

Okay. So so now on the performance impact of things. Right? It's easier for you as a programmer, but it can be very slow. Right?

Because there's a lot of page fault. There's a lot of data moving back and You, as a programmer, know how the data should be moving. That's why we use CUDA, CUDA mem copy. So how can we try to make this, you know, faster? Right?

You know, the main overhead here is that, you know, every page full, you know, triggers the overhead because we gotta check the page table, maybe check the page table of other GPUs. Right? That causes a lot of issues. However, if you know how the negative is gonna be used, potentially, you can avoid a page fault by prefetching. Right?

So prefetching is a very common technique, that we use in in your in your CPU. Your caches, a lot of times, do prefetching. Right? If if it it's because you have a strided access pattern. Automatically, it's gonna prefetch from your memory to to your CPU.

In the BIOS, right, if you ever hacked into the BIOS, right, there's a setting to turn off CPU prefetching. That that's what that does, essentially. Right? So you could do prefetching, essentially. Right?

So if you know in this case, know, after I do that, access my data, and I know my GP is gonna use all of the data rather than, you know, move the data with, you know, maybe, like, like, a 100 different page faults with lots of overheads, I could just do one fault data transfer with CUDA MEM prefetch or sync, which is your your CUDA MEM copy equivalent. Right? So I'm gonna prefetch the data to device s, of size size s. And then this the zeros is your device ID, if I remember correctly. Yeah.

Destination device. So GPU device. Right? So you don't have to specify the direction anymore. Right?

Because the hardware around it automatically knows where that data is. You just have to specify the destination. Right? And then and then it'll automatically move it. So on that point of view, right, it's still easier than putting on copy because you don't have to worry about direction and where it's coming from.

And then, you know, you can still move it back to the CPU, which is the the device to host direction, essentially. Okay. Any questions with this? No. Do we know which GPU our kernel Which GPU?

GPU is executing? Yeah. So we know which destination we've done. Okay. In a couple of weeks, we're gonna talk about multi GPU programming, but at a high level for multi GPU.

Kinda. Okay. So it's a lot more basic than that. So at a high level, I think there's a API to say to to set current GPU. So when you set the current GPU, all of these API calls will be targeting that specific GPU.

Right? So if you have multiple GPUs that you wanna launch a kernel on for every single one, it's gonna be like a four loop of set current GPU, and then you have a kernel. Yeah. The kernel itself doesn't have a GPU destination. It's very rudimentary.

I would I would assume you could put, the GPU ID in there. It would make a lot more sense. That that way, you know, we talked about two to three graph last week. Right? You could create, like, a a node and and map a node to a specific device or something like that as well.

But yeah. Okay. So so this is the the prefetching component of things. The other performance things that we can do, right, is to pass hints to the GPU. So, essentially, we saw before that, you know, if I know a page is, you know, read mostly or read only or if it's accessed mostly by the CPU or GPU, right, we could pass hints to the unified menu run time on that access script behavior, and essentially kinda pin a a gauge to a specific device.

Right? So the the API that got introduced for that is called CUDA mem advise. And these are essentially passing, you know, various hits. I think there's more hits now, but the the basic ones are set me mostly, set refer locations, set access by. You know, all of these are suggest.

They're not, like, actual physical mapping because the the runtime student do whatever it needs to do. So these are just suggestions to it. Right. So let's say, you know, we saw that case where, you know, two GPUs or CPU and GPU are actually getting the same data. And we know that it's mostly gonna be read.

Right? So instead of swapping our data back and how do we make sure we have two copies of it? The way we pass that in is, you know, we set code admin advise of this data of size n. Right? So we know what pages to mark.

And then, you know, every page has you can think of it has a data structure associated with some bits. So some of these bits will have, you know, this hint set to one. So then when the run time reads it and they see the page file, they'll be like, oh, okay. This is safe to make a copy, essentially. Right?

So now, when I initialize the data and and I have, a read on my on my on my GPU side. Right? Instead of moving this and and canceling this directly, I just make a copy. So now there's two copies in the system. So that's an that's an optimization that you could do.

Another one is to set the location. So you can set the preferred location. This this would basically be pinning your your data into a GPU. Right? In in Linux, have you ever tried pinning, like, a program to a specific core before?

Have you done that? No? You can. That's a that's a feature in Linux that you can do. Right?

So you can basically tell the Linux scheduler to always schedule your program to a specific core. Right? So you can pin things. You can pin compute. So this is the way you would pin the, the data movement.

Right? So in this case here, we wanna pin the data to, I think, CPU. Right? So you can say preferred location, and it'll be the CPU device. So So all of that data would always be pinned to the CPU.

And when the GPU wanna access it, it's essentially going to access the data directly on the CPU side itself. Right? So so instead of it having a page fault, the data is gonna flow from CPU or PCIe to your GPU and then your registers. Right? So so, you know, this this would be very slow on the GPU, especially if he access that data a lot.

So if he access that data a lot on the GPU, that's not what you wanna do. So this is probably something that's accessed very infrequently on the GPU side more frequently on the CPU side. Right? If it's vice versa, then you would pin it on your GPU. Okay.

But but then again, right, this this all relies on on your programmer intuition, your high level knowledge of of how the data the the access patterns are. Okay. This is populated on touch. Okay. Yeah.

Sorry. This is the same slide, but now now we're putting on on the on the GPU side. Okay. Right? So so in okay.

This only this doesn't apply to anyone because this only exists on a supercomputer. So in the supercomputers, they they have NVLink between CPUs and GPUs. So that allows the CPU to directly access GPU memory. And this kinda works because now your GPU memory and CPU memory can be viewed as one giant unified memory, and it can just go anywhere at once. That's very expensive because it requires a lot of hardware support, and it only works on IBM processors.

Right. Did you know IBM processors? No? They're they're one of the largest producers of supercomputers in the old days. So a lot of supercomputers are basically built by IBM.

So so back then, they had all these custom hardwares. Nowadays, I don't think they built a lot of them anymore. Most most supercomputers use, like, and stuff nowadays. Yeah. So so then, you know, in the old days, right, in these very specific devices from IBM, you you could have it the other way around as well.

Right. So you can also has, has something called, like, oops, set access by. This is kinda like more hints. It's, it it it's almost like the pinning thing, but a bit less strict, more or less. The fine grain differences is very hard to tell.

But, essentially, it's it's like a less strict version of pinning. So memory can still move from one day to another. It's just that, the hardware would would prefer to it to be on one space, but there's an additional counter. So if it detects the change in memory access patterns, it will still do the the data boosting, essentially. Right?

So so, you know, there's additional hardware support to keep track of, these these counters. Right? So after a while, if we get access by, by a lot, right, the hardware access counters would trigger a page migration. So so it's like pitting, but with, like, a hystericies. Like, only after missing an an amount of times did not trigger a a transfer.

So there there's cases where you you might like that. Basically, instead of swapping all the time, you would swap with some kind of, tolerance before swapping. So so all of this really is because, machine learning really drove a lot of these features again. So in the in the g d g x two system, you basically have fully connected emuents between 16 GPUs, and all of the g GPUs can pull together to have one tiny bit of memory space. Right?

So that's how we could train all of these large machine learning models nowadays. So rather than worrying about how do I partition up a model across different GPUs, you could view, like, a whole rack of GPUs as one giant computer nowadays. Right? Nowadays, NVIDIA is trying to create something called, like like, an operating system for your whole data center to view the whole data center cluster essentially as one giant computer where where you could just manage things with an operating system. So when when this when this came out, I think, like, this one rack of servers with with these 18 GPUs, I think it cost, like, $400,000 or something, like, for a per box in this big.

And and nowadays, it's a lot more powerful. Like, you got a whole rack. It's just aggregated and whatnot. And, you know, Facebook and everyone else will buy that. It's like nothing because we just need it.

Right? So so a lot of these features really were were because of machine learning and driving that because we need larger and larger memory spaces and so on. So that goes back to, you know, additional support for multi GPU as well. Right? So if you wanna do multi GPUs, I'll get it right there.

Right, we don't have to worry about micromanaging the data movement between GPUs because that that would be, like, very difficult to do. Right? So just just like as you're thinking about multithreaded programming, you don't have to worry about the data movement between threads. So that that's really another main benefit of unified memory for multi GPU programming. So with multi GPU programming, you can still program this the way you did before, but now you just have another hierarchy of abstraction.

Right? So here, you know, we we have this global index that we see a lot where we break things up into thread blocks and then threads in the thread block. Now you have just imagine now you have a GPU. Right? So you could extend this this indexing with a GPU ID and then three dimension.

Yeah. You know? Yeah. For for that for that GPU ID as well. Right?

So now you could, for example, partition your best to add across multiple GPUs. You know, just just by adding indexing to it. The only other thing is that when you call, you know, the the kernel, there's there's a for loop to launch this kernel on every single, device. So So so you just have to call it differently. But then the data, you don't have to worry about because unified memory would would just move the data between the GPUs.

Right? If you didn't have unified memory, you would have to manually partition the data and do memcpys between different GPUs. And if there's any ever any, you know, inter GPU communication, you have to implement that yourself too, which, you know, is a huge pain to do. So, yeah, that that's another benefit now of unified memory. If so if you wanna do, like, a multi GPU project, right, unified memory and and the indexing and so on like that, this will kinda give you hints on on how to partition your data across multiple GPUs.

Any questions? No? Okay. Alright. So so with unified memory and multiple GPUs, we kinda saw before.

But it's it's just that we're just basically partitioning across different GPUs. So whichever GPU touches the data it gets allocated there. So you don't have to worry about, like, where where do you pin it And then did there have policies that that manage the data across it. Right? Exactly the same way with CPU and GPUs with multiple GPUs.

What what they were with newer GPUs because of NVLink, you know, with specialized hardware support, the GPUs could remotely access other GPU memory. If you have, you know, your own two GPUs on on a PCIe, right, we would have to actually do copies and migrations. We can't do direct access like like, we have in vendor. So there's a lot a lot of hardware support to to improve the memory access behavior as well. Alright.

I think that's that's more or less I have for today. Right? So so, hopefully, you know, this kinda gives you a good idea of, you know, how unit five memory can help you in some of your projects, you know, with deep copies of objects and also just helping you import code with the data management side of things between CPU and GPU. So, hopefully, you know, if you do anything with your final project related to your own code base, this would be helpful. But, yeah, other than that, I look forward to seeing your proposals.

And, yeah, I'll send a message so that then that'll be tomorrow so you guys can have discussion and talk about it too. Okay.


Wait. Let me make sure I start recording. This is the most normal distribution I've had in, like, five years. This is like a a pre COVID distribution. Right?

During COVID, everything was skewed to the right. Maybe because everyone's, like, super smart during COVID, I assume. So I think, right, people got those at home exams and everything. Isn't this, like, a left tail there? Is that No.

This is smart. No. This is very normal. It normally distributed. There's actually nothing lower than that.

Yeah. In the past, it would be skewed to the right. Okay. They're skewed to the there's a long tail to the left. Right?

But this was skewed to the right, and the tail would actually be very low. But this was, like, I was shocked at how normal this was. Oh, to me, that's a very long design task, I guess. Okay. But, you know, the average was 75.

If you're anything above that, above 80, whatever, you're in pretty good shape depending on how things go. I'll probably prep their class. Right? What what what's the minimum to graduate? Okay.

I'll make sure everyone graduates. Yeah. Okay. So I'll skew this so everyone gets l d s a minus. Sorry.

Cancel. Right? No. Honestly, is it a d plus? D?

Yeah. You just need a c. C minus? No. You can't pass with the c Oh, wait.

C minus is not passing. I don't think so. Oh, it didn't? C minus is passing? But if you're not is passing, but if you wanna use the class, you're a prerequisite for other classes.

Oh, yeah. This is a prerequisite. Oh, so c minus is is okay. Right. So if if the class keeps going worse than this, I'll I'll curve the class, but, you know, if you earn yourself a b or an f, I I can't curve that.

Right? But, yeah, most of you have really good shape. I'll make sure you see graduation. On that note, there's an extra extra credit assignment. You probably see the channel in in Discord.

So the extra credit assignment is where is it? Make a meme about GPUs. No. No. Okay.

It's it's about to be fun, but educational at the same time. It's it's it's entertaining for me, especially if I know you don't know anything about GPUs. The memes make no sense, but you don't know that. Excuse me. Okay?

Yeah. So, anyways, the goal here is to make a meme about GPUs. Post it on Discord. There's a there's another form channel to collect your credit so you can make a post there. Basically, at the end of this course, right, there's a lot of gap between what we're doing in industry and in practice and what you're learning here to go here to learn the foundations of it so that if you had to pick up GPUs in the future or anything related to GPUs, you know, you kinda get the gist of it.

Right? There's a lot of really good resources out there. For example, NVIDIA has a very nice developer's blog, that talks about a lot of the technical aspects of, recent CUDA development and and GPU developments. Specifically, a lot of the slides that we went through in class from GPC, they started off as technical blogs. Right?

This is basically the main source of documentation for a lot of new features. So so, you know, these are these are nice resources for you to just keep track of, you know, after you graduate and get started in the GPUs, and you're not traumatized by this class. AMD recently made a technical blog also as well. Like, for example, you know, how can you optimize LLM training with SG language as an inference server for LLMs on the MI 300, right, and other stuff like that. So there's a lot of interesting rock and stuff, like, you know, how to use profiling tools on the bottom right there for for, inference and and so on.

So, you know, if you're into AMD as well, all of these resources are there. You know? That's that's probably the best way to kinda keep track of things. Right? Whenever there's there's big announcements from NVIDIA, there's a lot of developer blog posts that get posted at the same time.

But, anyways, for the purpose of this class, right, where you actually credit, it's just make a meme. Right? Because it there's lots of concepts we learn. You can use a meme to teach a a difficult concept. Right?

Something like that or something fun. But, of course, keep it appropriate, please. Okay. You know, so here are some examples from, like, Kai classes. Oh, man.

It's a bit can I zoom it out? You can't see it. Oh, yeah. Right. Anything any issue we have, you just stop shared memory on it.

We're just sure. We'll see that today. Right? That that's really what we're doing today. Right?

If anything is slow, we we stop shared memory on it. This is probably what a lot of you think you're gonna learn. You know, something magic. Everything will be parallel. But there's a lot of, you know, things that we have to learn about GPUs.

Sequitization, whatever, just memory access, and so on. And now you're like, oh, man. It's not so fun anymore. That's just so frustrating. Oh, I I don't know what that one was.

See, that one, I we never learned about in class. I think maybe gave them that. Okay. Yeah. Like I said, there's a lot of interesting things where it's very wrong and students don't know, and this is very entertaining for me.

This is a really good one on threat divergence. Right? Or what an active mask is. No? You got it?

Yes. Right? So, basically, when we have work divergence, right, does an active mask stops you from doing a program counter that you're not supposed to because you're divergent? Right? Memes can be educational.

So really try to see how you can make your memes educational. And this was just for fun. Yeah. They're just making fun of in in. Right.

This has no education about you. We haven't learned histogram yet, but This is what we're gonna resolve in a histogram. Yes. Automate operations will solve that issue. Okay.

But that would make sense in a couple weeks. Okay? So so, yeah, for you, actually, credit, just just have fun. You can post more than one, but you won't get more extra credit. Right?

Or else your whole grade could just be extra credit, and then you get all get a pluses. Some students I started last year made, like, 10 memes or something just for fun. They have no life. Or they were studying with memes by making memes. I don't know.

Alright. In terms of your final project, I went through anything I could yeah. The extra cutting is a 20% boost to our final gig. Right? No.

Maybe 2%. Oh, that's huge. I know. So so, you know, 2% means you can't ask me to bump you up a grade because that's 2% negotiate settle on middle number. How about 1010%?

No. Not so much. 2%. 2% is 10% of your midterm grade. A 2%.

Yeah. I'm gonna give you 2% of the overall class for the memes Just just because I feel bad for you guys. Okay? And there's still people who don't do the extra credit. Okay.

Am I recording? Oh, I am. Damn. It's okay. Yeah.

Super soon. It's alright. Your final project, I think I went through every single one of them and made a private channel. If I missed it, let me know. But most of the I think we converge a topic even if I have feedback with you.

Right? So if you have any questions or technical issues with vendor, use that private channel so I could kinda keep track of things. Because there's a lot of you guys doing it individually. I was hoping it was more team projects. I only have so many projects to grade, but none of you likes to do team projects.

Yeah. Okay. So, yeah, take a look there. Some of the times I have some comments. Make sure if I do, we'll just, address it.

And then, otherwise, you know, you're you're good to go and proceed on your final project. And then assignment three will be assigned today. So I I I swapped the schedule around a little bit. So we're gonna talk about matrix multiply today just so we could assign a homework assignment earlier, just because I know it's gonna get more hectic later on in the quarter. Okay.

Any questions for logistics? Yeah. Is the Apple project gonna be on, like, piano classroom? Or could we just Yeah. Yeah.

Yeah. I'm gonna make a, make a link for that. And people will get the computer restart during the time they get more extroverted. Wait. Why does it restart?

Why? At CNN, when someone wants to join, they restart Okay. This is the time I'm hearing about this. Explain what happened. The reason every hour, like, it'll reset.

It'll reboot, kick you out, and get blocked. Even if the exam is longer than an hour? Yeah. That didn't happen when I Right. That also didn't happen when I did it.

It probably happened at the start, and then they realized that after the few people all told about it. And they The thing I got about it was on Wednesday. I took my test on Tuesday, and I forgot what happened. So it must have been a Wednesday then? You you took it on Wednesday?

That's true. Yeah. Not not it it probably was the last week on Monday then. Okay. So it was a Wednesday issue.

No. It it didn't read it it worked fine on Wednesday. So the issue probably happened. And then it's The the only the only email I got about this issue was on Wednesday from one person. No one else reported it to me.

Okay. It's a bit of one off thing with their Maybe some of the computers. Yeah. Okay. But looking at the log, it looked like most of you got back all over in a minute.

Yeah. A quick question for the project. If you want if you want if you had some questions about our project, can we talk to you about that after class or I'm busy today. I could talk on Thursday. Okay.

Yeah. Or or on this board in the channel. Oh, okay. Yeah. Okay.

If if you're impacted by that computer restarting, send me an email. I just wanna get a count of it, and and then I wanna let this testing center know. Okay. Yeah. But yeah.

Yeah. Send me an email if that happens either. Okay. And then I can see I can see the log when that happens. Right?

When you log in again, there's a new session and auto log after that turns red. So don't don't lie about it. I could take a seat. Okay. Any other questions?

No? Okay. Alright. So that's all the announcements for today. I'll I'll post these announcements on a on a Discord after class as well.

So, you know, looking ahead right? It's it's now week six. Is it week six? Yes. It's week six.

Okay. Right. Yeah. So today, we're gonna talk about matrix multiply. That's your seven three.

It's still next week, Thursday. And then we're talking about streams, histogram, and some, you know, other modern topics about GPUs just to kinda catch you up to the trend. I like to reserve a couple of classes just for final project meetings because a lot of people have you know, once they start wrapping up and working on it, a lot of people have more technical questions on on, how do I approach this issue or how do I use a GPU for this and that and someone like that. So those two classes or days, would be reserved for that. Typically, we'll we'll have some kind of it's kinda like open office hours for, like, a couple of days, me and the TA.

So you can meet with with your group and I, for, like, fifteen minute blocks or so. So there won't there won't be lectures on those days. Of course, hopefully, you're working on your final projects. Right? And then, you know, week 10, we have our exam and so on.

Okay. So other than that, that that's the the rest of the quarter. Alright. So today, we're gonna learn about matrix multiply. And this is specifically, you know, one of the main core kernels that we have, you know, in in machine learning.

Right? So a lot of you guys are doing projects related to, you know, neural network training, like trying to implement it from scratch and someone like that. A lot of those for the connected layers, the math you can represent as basically a mutual multiply operation. It could transform into a convolution as well and so on like that. So we learned, you know, how we could implement a naive matrix multiply algorithm and then how we could optimize it by Slack and shared memory on it.

Right? So so that'll be hopefully, we'll get through it today in a little bit of, on this Thursday. Oh, yeah. This this week. So because Tuesday.

Okay. So most of you have have, you know, done a matrix multiply before. Right? You use most you multi you multiply matrices before in, like, linear algebra. Have you ever had to implement it in c?

No. No? Never? Okay. If you were to implement matrix multiply in c, but you have two two d arrays, matrix m and n.

Right? And then you you multiply those matrix together and you get the result in p. Right? So, okay, stepping back, you know, what what what is the matrix multiply operation? Take the rows from the one and multiply it by the column of the one.

I'll use the corresponding row and column. Right? So so, you know, every output element, right, you take a row from m, a column from n, and you, you know, you take this times this plus this times this plus this times this, blah blah blah blah blah plus this times this. Right? And then and then that result is is your is your, sum right here.

Right? The sum, essentially. And you do that for every single element until you get alpha matrix. Right? You know, this is square.

Right? But let's say, you know, you have a rectangular matrix, of dimension, you know, m, k, and n. Right. Just to get a step back, what what's the size of the alpha matrix in that in that scenario? How how large is your what's the dimension of the alpha matrix?

M by n. M m by n. Right. Right. Yeah.

This is just a refresher if you haven't seen this before. Right? So the alpha beta is n by n. Right? But the total number of, you know, multiplying ads that I have to do, Right?

It's it's going this way. Right? So so k is the number of the computation that I have to do. Right? The the the the product sum operation.

So in your homework assignment, you know, you you're gonna have to implement a matrix multiply that works for any values of m, k, and n. So it could be, you know, rectangular or it's it's not gonna always gonna be square. Right? So in GitHub classroom, it's gonna test for various sizes of matrices. The most common issue I see is that, oh, it works for square, but not for non square matrices.

You know, those are just different kind of, corner cases that you have to handle and so on. So in lecture, the lecture side will basically cover a square matrix. There'll be some pseudo code. So then you're probably gonna have to extend that to a a non square matrix, a rectangular matrix in homework assignment as well. Alright.

So let's say I have matrix multiply. We know what it does now. How would you implement this in c? Right. So if I have this in c or c plus plus, And I have I have two inputs and one output.

I have a matrix, m and n and a p. How would you implement this? Let's see. And now you just iterate over the result matrix, and then for each element, we read the elements from the two input matrices on the element wise. Yeah.

Yeah. Right. So there'll be some form of a nested for loop. Right? So how many for loops do I need?

Like, four. Four for loops? Three? Three. Or if you have a flattened matrix No.

It's not flattened. It's two d. Two d? Yeah. It's three.

Three? What what's the three form of score? Two is reverse x and y or row column of the gridlock. Yeah. But for every row, for every column And then one to traverse the input line.

Yeah. To traverse k elements. Yeah. For the k elements. Right.

Yeah. Yeah. Alright. So just just kinda do I do this? Right?

There'll be there'll be a that's it for a loop. One is gonna be to traverse the rows. One is gonna be for the columns, and one is gonna be when I do the dot product. We just call this k for here for now. Right?

Which four loop goes to which variable? Where do I how do I iterate? Are you computing row or column major? Let's see. It's row major.

Right? So so this is row major. Which one would be a row? This middle would be column, and then the inner one would be an index for k. Inner one or k?

Middle one would be column. Right. Yeah. So for row, column, k. Yeah.

Honestly, you can swap the order, and it works too. You know, in in more advanced implementations that make you swap the client, you would swap the order to make it more memory friendly and so on like that type of thing. Right? But in in a very basic version, right, it says for every row, for every column, for every k element, I do it product sum. Okay.

Right? So so, yeah, that's how you implement it in c. Did you have to implement this in assembly in in c s 61? No. No.

No. One six one. One six one. One six one. Professor.

Is that right? Yeah. I know he does that. Yeah. How's that architecture?

Oh, that's bad. Oh, okay. When when I taught 161, it was a very good big class. Right? So if you took 161 with professor Chen, right, you implemented this in assembly.

Right? And some of you took it with Elihei? No? It's Alan Knight. Oh, Alan Knight teaches it too?

Okay. Yeah. I don't think he did that in their classes. I don't think. Okay.

Right. So, in terms of parallelizing it now, right, we have ref blocks. Right? So how do we partition up this problem? So the the you know, if you look at all the other assignments we have, right, the step we have to do is figure out how we partition up our computation or input data into different thread blocks that we map the computation to.

Right? So the way we approach this for matrix multiply is that we map the thread blocks to the output, matrix. Right? So, basically, you know, every element in p would be partitioned up like a two d image, essentially, across different thread blocks. Right?

So this way, within every single thread block, a thread would, you know, do, that product sum, and then another thread would do another one completely in parallel. Right? Because because we could do that in parallel because there's no data dependencies between the the result in purple and the result in orange. Right? Okay.

Right. Okay. So so we're gonna see how we can implement a 90 version of this. Yeah. Right?

So, you know, we do because it's this two d. Right? The The output issues is it's just like how we handle the two d image like we normally do. You know, you have a row and a column index the same way we saw before with the images. Right?

Row is block ID in the y dimensions, x block dimension plus thread ID in the y dimension. And column is just the same, but in the x dimension. And because we're handling, you know, parallel threads, we don't have to iterate across a row and a column. Right? Now we just map a thread to a row and a column.

So that kinda removes that that, you know, outer two for loop that we saw before. So we just have one for loop to handle the the the products up, the k. Right? So with the mapping, I then I do a boundary condition check to make sure I'm in within the result of that, output matrix. Right?

Because it might not fit. And then it just reduces to, I guess, that one for the right? For every element from zero, you know, of k from zero to width, you know, when it's when it's gonna be rectangular. Right? Width is gonna be replaced with whatever that dimension should be.

You do n times n, and then you just add it. And then you you can get it to repeat value. Right? Okay. So so you see here.

Right? The p value, we initialize a zero. We accumulate across that k, and then we write the end result to that final output matrix. That's that's a p. And, again, because, you know, CUDA doesn't support you accessing memory like this.

Right? We had to do a one d transformation. So that's how you get, like, row times width plus k and k times width plus column. Do I need to go over that? Do you understand the indexing?

Yes? Okay. Alright. So even though I, you know, I have a output result that needs to go to p, right, I have this low p value. Right?

So this hypothetically no. But on our thinking caps, things that you learned from the. Right? Let's say I don't have p value. Right?

But instead, this this p is right here. Assuming it's initialized to zero, my output. It's initialized to zero. Right? So I just have this for loop, and every iteration in case is m times n, and I write it directly to p, and I accumulate it directly to p.

Right? Why would this be an issue compared to the version of p value? Right? Yes? Yeah.

It's a lot of memory rates. Yeah. Yeah. So every time I iterate on k. Right?

It's it's two region global memory one by two, one right to global memory. Not only that, the right to global memory is just getting overwritten, right, until the the value at the end is what I care about. Right? So I'm doing a lot of rights to global memory that I don't care about. Just it's it's it's intermediate data, right, that doesn't need to be in global memory.

So so that's why, you know, we, we have this p value. So now instead of writing my intermediate values to global memory, it's now being looked into, you know, where where would p value be? A register. Right? Yeah.

So so p value is a register. Right? So so, you know, now I'm writing the register every time, and then only the final result was to global memory. So so that makes makes it significantly faster. But, yeah, this is still very naive.

Right? So so how can I improve this? Yeah. Share it. How much?

Share it. Yeah. Yeah. You could transpose n before passing it in so you can access n's row instead of its column. I could do that too.

But if I have a very large n, that's an overhead of transposing. And then when you transpose on the CPU side, you could put transferring to the GPU Or do you transpose in place in this GPU? Then you would need double the amount of memory on the CPU side. Do you transpose on the c GPU? Yeah.

Yeah. There's there's overhead still too, right, in in terms of it might it might make it simpler and speed it up, but there's other overheads too. There's always trade offs. Right? So we're gonna see how we could use shared memory, you know, to to handle this.

Right? So so why does shared memory work? Why why would this algorithm work very well with shared memory? Yeah. Because in your for loop, you're still doing, global memory accesses with, them and, and the for loop.

Yeah. Okay. Shared memory. You put that in shared memory. You can access that that faster.

That's Yeah. But but there's, such a criteria to all program before we could use shared memory. Right? Oh, I think it has really good spatial locality. Right.

Yeah. There's a lot of locality. Right? I think this is one of the midterm problems that tripped a lot of people up. Right?

Because that that one was the Saksby algorithm. Shared memory does not actually improve that. Right? There's there's no reuse. I get partial credit.

So right. There's there's a lot of reuse here. Right? So let's say that that purple row. Right?

What other threads would make use of that purple row? They did in the purple row. Yeah. All the ones in the same output row? Yeah.

Right? So so every element here would make use of this row. Right? So instead of every single thread, they're reading that purple row over and over again. I could just read that purple row once in the shared memory, and everyone just use it.

Right? So that way I could reduce the amount of global memory access. Right? There's a lot of spatial locality in this. Right?

And and temporal to that. It's not temporal when the thread is temporal because another thread will read it. But, yeah, let's also see spatial. A a bit of both. Sorry to classify it.

Alright. So so clearly, right, there's a lot of potential of using shared memory. Alright. So, you know, the way we partition it, you know, this this kind of iterate. Right?

We we map a thread to an output element. You know, like, in in reduction, you know, we we map a thread to an input to two input elements. Right? And then in vector add, we map a thread to an input element as well. It's hard to map to an input element here matrix multiplies, so we map it to the output element.

So so it's just more natural. Right? So in this way I mean, you could you could map it to an input, but it's really weird to write that code. But yeah. So so every every thread master an output element, essentially.

And then we just partition the output element, like like the two d images we saw in the past. Right? And then this is like a graphical view of of, the data that's being used. Right? So, clearly, you know, you see multiple greens, multiple what is that?

Peach, salmon color. So, you know, that that's where the reuse comes in. Right? This is just a a graphical visual way of of viewing. Alright.

So, the algorithm that's optimized is called tiled matrix multiply. You might hear the term block, you know, block matrix multiply in in other classes. Right? It's it's basically the same thing, but in in, we call it time. And, basically, you know, the whole goal here is is to make use of, the shared memory.

Right? We we saw this slide before reduction. This is essentially what we see in matrix multiply where, let's say, we have a row and the matrix and the and the whole row of output whereas, basically, we use that. Right? We did that analysis, back when we did reduction, but this is essentially what we're gonna do now again.

Right? Because I I don't need to go over this because we saw this before. So at a high level, this timing algorithm has a lot of steps similar to reduction if you squirt and look. Right? So so there's there's a couple of steps that we have to implement.

And this is kinda like just an overview. When you implement your code, you you actually implement it. Right? So we have to, you know, identify, what data I'm loading from global memory. And then this is step, I actually load it.

You know, like, in reduction, step one would be to figure out, you know, what my indexing is when I when I load that data from global to shared, and then I I actually load the data. Right? It's the same thing. And then you synchronize to make sure that everything is already in shared memory. You know, everything completed, caching.

And then four would be to actually perform your matrix multiply. Right? You can replace that with your reduction tree, and this will be exactly the same steps that you were saying at a high level. You you synchronize to make sure everyone completes that. Everyone used up all the data that that got cached, and then we repeat.

In terms of matrix multiply, you know, we have to repeat for, you know, tile. Right? What we see in a little bit. In in reduction, we will repeat where we step. Yeah.

But we don't load another tile. Right? So there's a little bit of a difference here between reduction and emissions multiply. The last couple of steps, we just kinda loop over more and more. So so we'll see that in a little bit.

But at a high level, we squared a little bit. What you did in reduction, the steps would simply be the same in in time. It's just multiplying. It it says that using the shape memory. Right.

So we have the, you know, the three phases is, you know, we had to load a tile, and then you do your partial you know, we had a partial sum and reduction. Right? So you gotta do a partial dot product, essentially. Right? So when we do everything in terms of, our thread blocks, we no longer work in in, you know, on on a row.

Right? But we work on, like, a strip of a row and a column. Right? This is essentially all the data that all the threads in that block will touch. Right?

If you have a really large matrix, right, it it would be impossible to to cache all of this and cache all of this. Right? We can't we can't cache that for that block because because shared memory is is a limited resource. Right? It's only a couple of kilobytes.

So if I can't cache that whole strip of m and n into shared memory, what would I have to do? How would I implement this algorithm now if I can't fit that whole the whole data that I need into shared memory? Every element so one to one a relationship between, like, in one strip to another strip so you can compute, like, partial products and then Yeah. Yeah. Right?

Well, we're gonna have to partition that up. Right? So, you know, in the step, I had to figure out what what I have to, you know, cache into my shared memory. If I can't cache the whole line because it just doesn't fit, I'm gonna have to essentially partition it up. Right?

So, you know, I would, you know, maybe, you know, cache this And then once this is in shared memory, I would do a, you know, partial dot product. Right? So I would I would do, you know, this times this plus this times this and so on of that part that I cache. And then once all of the threads finish using up all those data that we cache, right, yeah, we just override it. Right?

You don't need to clear out your your shared memory. You just override your shared memory. Right? So we move on to the next, to call it phase in the slides. Right?

So we move on to the next next phase. Right? And then and then so on and so on and so on until we finish. Okay. But in every phase, right, we operate within, you know, that one chunk of data that we're caching.

Right? So so how much data should I cache right here? You know? Just to make things easy with your indexing and whatnot. Right?

How wait. What's the my my block size is blocked with my block list. Let's say 16 by 16. Right? How much data should I be caching?

Enough to fill my shared memory enough to Oh. I mean, what what what would I pick? Well, is there enough Okay. Memory that you can train that you can't Yeah. Hash 256 there.

Right. So let's see now. My my dev block size here is 16 by 16. Right? So, clearly, this dimension is gonna be 16.

Right? It has to be 16. It makes no sense for it to be anything else. But what would this dimension be? Well, it depends on how big we should run.

I mean, to make it easier with indexing, yeah, 16 would be the easiest. Right? It could be something else, like 32, if you want, maybe. But then the indexing will be a bit more difficult. Right?

So so let's hypothetically, let's say if this was 16 and this was also 16. Right? The next thing I have to figure out is how do I actually move the data. Right? How do I work out the indexing, the data that has to be moved?

If it was 16 by 16, you know, every thread would just move one element from n and one element from n. Right? You can kinda think you could you could just take that and overlay it. Right? If that was hypothetically now, you know, let's say 32.

Hypothetically, there was 32. Right? Now every thread will be moving how many pieces of data? Wait. Can you say only one?

One from n and one from n. One from n and one from n. One is removing 16 from Oh, okay. Wait. Hold on.

Let's back up a bit. There's there's moving one that's you're getting across the Yeah. Row and then down the column. Yeah. So you need as many elements as your width from the row and as many Yeah.

Yeah. In in shared memory, I would definitely be accessing it eight times. Right? But when I'm caching it from global to shared Oh, okay. Yeah.

Right. So when I'm caching it from global to shared, I don't need each thread to move 16 elements because my neighboring my neighboring threads are gonna do that. Right? Yeah. Understand.

Yeah. Yeah. Yeah. So just just imagine all of your threads overlaying to that region that you're caching, and essentially, that's the mapping. Right?

And then that gets cached into shared memory. And then you just see the image involved where you access k times in in shared memory. Now, hypothetically, you know, you you can kinda imagine what the indexing and the code will look like at a high level, right, with that. Yeah. So, hypothetically, now if it's 32 and this is 32.

Right. Now how many pieces of data would I have to move? Four. Four. Four.

Right? Two Two from each. Yeah. Two from each. Right?

Two from m, two from m. Indexing is gonna get a little bit more more complicated because now at every tile, I would have to compute two different indexes and move it. In the end, you know, it it's a lot more complicated to program. There's more indexing you have to worry about. Performance wise, believe it or not, it actually doesn't make things better.

Right? Even if they're not fully utilizing churning. Yeah. Yeah. We can analyze that later, actually.

That would be a great fundamental problem. Right? Okay. Right. So so, you know, just to make things easier with indexing, right, it'll be 16 by 16.

Okay. So we'll work out what the indexing would be, in a couple of slides. Alright. So does this make sense at a high level what we're trying to do? Yeah.

Yes. Okay. Good. Are we lost? Yes.

No. So so, this assignment is is typically the most difficult assignment. Are they final project? No. It's much easier than a final project.

I mean, it depends on your final project that you take yourself. But yeah. So, you know, we're gonna break this up into phases. Right? Because we can't fit everything to share memory.

And, basically, we're gonna load, you know, a tile at a time, right, essentially. So so let's see how that indexing is gonna look like. Right? So when we load the tile, we're essentially gonna cache, you know, one element and one element for n, you know, for each thread, you know, a block at a time. Right?

So a graphical view of this would be, you know, like, you know, this is, again, a a four by four matrix of a two by two to apply. Right? So we cache this in the shared memory. Right? In parallel.

This is done in parallel. Right? And then we do that partial dot product, you know, with k. Right? So in this case, k is two.

Right? So we did the element, then we did the element. That's partial. Right? And then we move on to the next tile.

You know, n goes this way, n goes this way. So we we cache it again in parallel and then k k of two, and then we have to find the result. Right? So so this way, you know, even though our share varies, we're still able to maximize the amount of reuse. Right?

And if you wake up the math for that, right, there's a lot of these variables of m and n that that repeats. Right? This is in a two in a two case. If you expand this to, like, a much larger matrix, right, there'll be a whole bunch of reuse. So we'll analyze how much reuse there there would be in a little bit.

Okay. So then once you're done with that, you know, you synchronize to make sure we finish caching everything before we, before we do the actual dot product. Right? And and we use synchronizations before with reduction. So So this shouldn't be new.

Alright. So let's see how we would actually do that indexing. Right? So so let's look at the half and then we'll generalize it. Right?

That's kind of the general approach we do indexing. We just work out one case. We do the next phase, see what that general form would be, and then we come up with the indexing. Essentially, we're just trying to find a pattern. Both things.

Right? So let's say, you know, I'm I'm this thread right here. So that means, you know, every output element has a row and a column index. Right? So so this would be my row, and this would be my column index.

I don't know if you can see that on the bottom. Column and this is row. Okay. So so that that points to my my current thread ID, essentially. Right?

Every thread is gonna have their own row and row and column, so every thread is gonna be performing this in parallel. Alright? So the the data that I I'm responsible for moving is essentially, you know, the corresponding element in in that tile. So if I use a different color, let's say you know, if I'm this blue thread right here, I'll be responsible for moving this element here and and this element, you know, right here. Just just, one on one responding mapping.

Right? Make sense? Yes. Okay. So so in terms of accessing my m, right, let's just use the blue example here.

Right? What would my index here be? It's it's row major. Right? So what would the index there be?

It'll be m a what would that index be? So it's row major. Right? So so blue blue's row is here, and blue's column is here. Right.

So, correspondingly, my column index would be here, and my row would be here if I map it one to one. Right? Like, you know, there's something to match right there. So row you know, this one's you know, it's row major. Right?

That's the that's the easy part. This will be row. But what would this offset be right here between here and here? What's that offset right there? Right.

Right. Yeah. The answer's on the slide. Okay. Wait.

I was I was thinking I know. No one's looking at it. I'm glad you're actually thinking critically. Right? Okay.

So, would this look better? I thought you meant, like, without that idea. I mean, yeah. I know you have to use that idea. Yeah.

That's how I was thinking. Yeah. Can you see this better? Yeah. You can.

Right? So so this this offset here is the same as this offset here. Right? It's just that that overlap. You know, if you just take all your thread and overlap it, right, that that's your overlap.

Right? So so, you know, this this index here would be m of row and, you know, this is thread index in the in the x direction. Sorry. X. Right?

Which is which is just this. Right? So, similarly, you know, what would this I'm I'm using purple now just because it it is not clear, I think. Right? What would this be?

What are we pointing at? Sorry. This one right here. Oh, on the top. I can't reach.

You know, it's it's real major. Right? So what's that? Y offset? Right.

An idea. Right. Yeah. Idea. Yeah.

So so this offset here is thread index in the y, and then everything in blue is actually purple. Right? And then the the other direction is is this column. Right? So this will be n, thread y column.

Okay. Does that make sense? Yeah. Okay. And every single thread does that through every other element, and it it it maps.

Okay. At least for the phase. So now, you know, going back to this purple one. Right? I was mapping here.

I was mapping here. So that's the phase. Now in the phase, I'll be moving this tile and this tile. Right? So, correspondingly, the data that I'm moving would be here and here.

Right? So what would my indexing be to access this? The phase. Yeah. This would be the phase.

Or, you know, we're we're engineers. We count we count phases from zero one. We count from zero. From Okay. Right.

So so the index in that array would be what? Row. Row? Yeah. Row.

Right? It it's still the same row. Right? This offset is still row. But what is this offset now?

Yeah. Yes. Yeah. It's it's a so is it the thread ID times the block ID? Red ID times the block ID?

What what I need was block ID times block ID? Red ID times Still the same block. The block ID Yeah. My my block ID doesn't change. This is still the same block.

My my phase changes. So so that you're gonna okay. I'm gonna need a for loop for that phase clearly. Right? Stride.

Right? It's almost like the stride. I'm like phase ID times block damage. Yes. It's phase ID times block damage.

Yeah. Plus. Right? Yes. Yes.

Yes. That would be it. Right? So, you know, the offset here would be if you wanna think about it. Right?

Yeah. This this offset here is still t of x. Right? Just like just like this this offset is t of x. Right?

The only difference now is dynamic offset by another Title width, which is your block dimension. Right? Block dimension. And then, you know, for my phase, right, this is still thread ID in the x. Right?

But now you just add another, you know, two block dimensions before it. Right? So so the generalized form would be something like thread x plus phase times block dimension. Right? Something like that.

Is that correct? Yeah. That should be it. So that would be that would be the generalized form of that. Right?

So, essentially, every phase, we take the thread ID, and then we just add another block dimension to it. So, similarly, you know, this would also have an offset at every phase, you know, at every single phase. Right? So, you know, n would be this is column. Right?

So so and we'll have a similar form. Right? So, you know, this is t f y plus, you know, one whole block heights, you know, title with block dimension in the y. And then this one, which should still be also here, you know, t of y t of y plus, you know, two times your block dimension, essentially. So this would be, you know, t of y plus phase times your block dimension.

Okay. Right. So we kinda worked it out. Right? Yeah.

Alright. So so that's that's the other couple of slides, essentially. Right? So, you know, after one, we had one we we we offset the t of x by one tile width, and then just generically, it's just p. Right?

So so it's p times side with plus t of y and t p times side with type plus t of x. You know? Of course, you know, we we can't write this in this form in CUDA. It it doesn't compile. But then, you know, we have to linearize that again.

Right? So that's just row times width plus, you know, that p times style plus set of x. And the other one is p times style with plus c of x times width plus column. So that that's a linearized form. Okay.

Does that make sense? Just looking at it, you'll be like, why? That makes no sense. Right? But working it out, working backwards.

Right? Hopefully, that that makes sense why why that indexing would be like that. Yeah. Literally, a lot of GPU programs just figuring out the indexing of things. Questions?

Still good? Okay. Alright. You know, so so that's the, indexing part of things. Right?

So so another thing I have to consider now is, you know, what what drop block size I should pick. So before, in the previous assignments, right, what were the considerations that we use in order to pick the the proper thread size thread block size? How do we pick it? Yeah. Yeah.

Right. We basically wanna maximize the number of threads that the hardware has, essentially. So in terms of matrix multiply, right, the other consideration now is to basically try to maximize the amount of reuse. Right? So so matrix multiply, because we access memory a lot, memory is essentially the bottleneck here.

Right? So the way we wanna speed up matrix multiply is to minimize the amount of memory access or, really, maximize the amount of reuse. Right? So we could do some analysis to figure out how much reuse there would be, essentially. Right.

So, you know, thread blocks typically could have you know, there's there's a very limited set of, thread block size that that's valid here. Right? So, you know, you could be 16 by 16 or 32 by 32. You know, given these two number of threads, I think we worked out already that on our GPU, both of these are valid responses answers, right, for thread block size. So so the other consideration now is how do I account for memory memory reuse?

Right? So you kinda have to analyze the amount of, data that is being used back in full. Right? So, you know, going back to my matrix multiply. Right.

So let's say, you know, for every phase here, right, I have to load data. Right? In this case, it's it's 16 by 16 hypothetically. So for every single phase, how many loads from global memory would I have to perform in this case here? Every every thread will lose how many pieces of data?

16 times 16 times zero. 16 times 16 times two. Right? You know, every every thread moves two pieces of data. Right?

One from n and one from n. That's a two. And every thread in the does that. Right? So that's a 16 by 16.

Right? So so the number of loads would be 16 by 16 times two. Right? Every thread, you know, moves too. Right?

Once in n and once in n. Okay. Now how many multiply and add operations are performed in every phase? In creating control though? No.

No. Just just to make just multiply operation. Right? So so after I cache it, right, you know, after I cache it, it's, like, 16 by 16 by 16 by 16. Right?

I have to I have to add this and, you know, and I have to add this. Yeah. Multiply add product. So, like, every thread is gonna do that that partial dot product operation. Right?

So how many multiplied s would there be performed for that tile, right, of 16 by 16? 16. Counting them as, like, the same cost operation? Separate. Let's just say separate.

Multiply and add are two separate operations. 16 square of each. 16 square of each. You multiply o nine y is down, and then we add Oh, no. Sorry.

No. Sixteen Fifteen times 16 times 16 times 16. I think 16 times 16. That's fine. I don't know.

Yeah. That's 16 is something. 16 for the queue. Okay. This is getting too hard.

Let's let's think of it from up to point of view of one thread, and then we just multiply that by 16 by 16. Right? That that will that will simplify things. Okay? So so don't think about it in terms of parallel, though.

Just think about it. For one thread, how many multiply and adds am I performing for a single thread? By 60 lines. Right. Six 16 multiply yeah.

Right. So so each thread is gonna do, 16 ads and 16 multiplications. Yeah. Plus or times? Plus.

Plus. Okay. Yeah. We're changing it separately. Okay.

Right. So every every thread is gonna perform 16 adds and 16 multiplies. Okay. Each thread. Now in that tile, how many total oper multiply add operations would there be?

How many address do I have? 16 by 16. Right? Right. So whatever this is.

I don't know. Have 16 times 16 times 16. Whatever that number is. No. There's a there's a two on top.

13. There's a two times 20. 29. You know what? I'm gonna do some reductions later just because there's 16.

Okay. Right. So so one way in which we could compute or measure how much reuse we have, is calculating the number of operations for memory load. Right? This is a metric.

I think some places call it, like, compute intensity or something like that. Right? So it's a it's a measure of how much reuse I'm able to get. Right? So in terms of that type of measurement, right, would a higher number be better or or a lower number be better?

Right? If I wanna calculate how many computes for every load. Right? High higher would be better. Right?

So so in that in that naive case, right, I think it's, like, two operations per well, actually, it's one one operation for load. That's the average. Right? We we load to we do a multiply and add, essentially. Right?

So so that's a very low, you know, arithmetic intensity. So in this case here, right, we can kinda calculate this by just kinda dividing it. So, you know, how many floating point operations for every memory load. Right? So so you just divide this by this.

Right? So 16 by 16 by 16 by two divided by 16 by 16 by two. Right. This kinda cancels out. So then you end up with 16 floating point operations per load.

Right? So so that's kinda like the measurement that we use. Okay? So for 16 by 16 tile, we have, you know, an arithmetic intensity of 16. So, you know, now we can repeat this also with 32 as well, right, just to see which one would be better.

In in both cases, right, we both make use of the ten twenty four threads in the SM. So now this would be kind of the the tie breaker in a way. Right? So for the 32 case, right, how many loads from global memory will be performed for each tile? Yeah.

Right? So for every thread, 32 by 32, it would do one one m one n plus two. Right? So it'll be 32 times 32 times two. Right?

And, similarly, how many multiply and add operations would that be? K. So so for every for every thread, we're gonna perform a multiply and add how many times? Yeah. That that k, right, it's, 32.

Right? Divide that together. You know, you get 32. Right? If you don't wanna do this math, just just know that the arithmetic intensity is the width of your tile.

Right? That saves a lot of time when you find an exam. So in this case, it's 32. Right? What's better?

60 by 60 tile or 32 by 32 tile? 32 by 32. Right? So so the larger my tiles, the more reuse I have. Assuming the shared memory.

Assuming it fits in shared memory. Oh, man. That's just in the shared memory? That's a great question. Oh my gosh.

So much on my slides. Wait. That was literally my next one. No. You're just guessing?

Lucky. Yes. Are you playing games? Good. Okay.

So so, yeah, let's say you asked about 16 k b bytes, not bits of shared memory. Right? So looking at this, right, we could kind of analyze how much shared memory we would we would need. Right? So, you know, if it's 16 By 16, we need essentially two of them.

Like, one one to cache m, one to cache m. Right? So so your shared memory size is 16 by 16 by two, and this is assuming four bytes. Right? So that's that's two KB.

The two KB per thread block, if I have 256 threads in a thread block, unless I hold ten twenty four, I would need four thread blocks. So that may need eight KB to fit it off, and that's fine. Right? Eight eight KB will fit in the 16. If it was, you know, double precision floating point, right, instead of four bytes, that'd be eight bytes.

You just double that in size. That still fits barely too. Right? That would be 16 KB. Exactly.

Right? So, yeah, this works too. Okay? And then for 32 by 32, you know, it's it's of course, it's more. But I have ten twenty four threads that also fits too.

Right? I'm not gonna fit two thread blocks in ten twenty four. So this also fits. Right? So in both cases, right, it fits in sharing memory, so that's not the issue.

So the deciding factor here would be, you know, just just the amount of reuse I have. So, you know, in terms of the number of threads, the amount of shared memory I use, another condition now that I worry about is just the amount of reuse I have. Right? So, clearly, you know, 32 by 32 would be the the case here. Yeah.

I don't know why that Okay. Alright. You know, just a fun exercise. I'm not gonna go into here yet. We'll probably handle that.

Right? Okay. So so now we know what we know now. One of the one of the things we talked about a little bit before was that you know, let's say we could cache 16 by 32. And this is the 16 by 16, hypothetically.

Okay? This will still fit in shared memory, probably, I assume. Right? We we saw before, you know, it's eight k b. This must be 16 kV.

So this this would be an example of me filling up my shared memory, right, and my threads at the same time. But does this actually give me any improvements, right, in terms of arithmetic intensity. That that's the type we get. So we could do the same analysis here. Right?

So how many, you know, data movements would there be here? You know? Every thread moves How many pieces of data in this scenario here? Per phase or probably? Yeah.

Yeah. We're we're analyzing for phase. Yeah. Per phase. Four.

Right? Yeah. Right. You know, this dot here would move here and here, here and here. Now what happened?

Here and here. There's an invisible button right there. Right? Yeah. So times four.

Okay. Now how much computation would I perform? Right? So every single thread would perform a multiply and add how many times? Yeah.

My my k here would be 32. Right? Okay. Two sixteen. Right?

Yeah. Yeah. Yeah. Yeah. There's still be two left over.

So 32. Yeah. 16. Right? So the or may medic intensity doesn't change.

It's still Right? So this just made my life harder to program because it's just more investing involved. But from a performance point of view, it's probably exactly the same. I mean, I moved more data, so I understand that. Everything that that everything that it tends to be doesn't change.

Right? So, you know, we that's why we tend not to do this type of 16 by the two just to fill up the shared memory. Right? Even Even though the shared memory is there, it doesn't actually lead to much improvement except making your life harder indexing. Okay.

Questions? Yeah. What is the four the 16 by 16 for component? Four elements Oh, yeah. Every, yeah, every thread moves four elements.

Right? So in the 16 by 32, we move two elements from m and two elements from m. It's kinda like we're doing two phases at a time if you wanna take it that way. Two of the the square phases at a time. Yeah.

Okay. Right? So, you know, that that's the gist of, doing a a matrix multiply, a square one, right? So so you're gonna have to generalize it in your in your homework assignment, for any arbitrary size. You know, it could be rectangle or regular shapes, right?

So how do we handle, now, these these weird boundary conditions, for example? Right? Like, let's say, you know, you you can have, you know, my thread block that sits out here. Right? And then I can handle this and this.

Right? Or my block is right here. Gonna handle this and this. Or, eventually, I will handle this and this. Right?

Oh, man. It's a lot of weird overhangs that goes around. Right? What's a simple solution to make things nice? Just pad it.

Right? What what would I pad? Zeros. I yeah. I mean, I would pad it with zeros.

Right? I'm just pad it with zeros. Zero times zero is zero added to something instead of something. Right? So so I could pad my input with zeros.

Right? Have, like, a really obtuse matrix that's like Yeah. Right? Then I would have a lot of padding. Then that means that, you know, my input matrix is gonna be artificially much larger, and that means gonna be a lot more computation that has to be done and and move from the CPU to GPU.

So so padding padding the actual input might not actually be the best option. Right? So, alternatively, instead of padding the input element, we would essentially just pad the the caching. Right? So if like, in your reduction, a lot of the issue in reduction was that that one test scenario where, like, a 100 k fell.

It's because when you when you cache, you have to handle the the boundary condition. Right? In your scenario, you've had it with zero. Right? If it's outside, you do you do else zero.

You run it to there. Right? Because zero does nothing to to your reduction. So this is essentially the same case that we're gonna have to do it. Right?

So if the data that I'm trying to catch is outside of the actual boundary of m and n, I'm just gonna write zeros into that shared memory that I'm supposed to load. And then I don't have to change any of the code when I do my partial dot product. Right? Product cell. I'm just adding zeros.

Right? So, yeah, we're just gonna pad it. Right? So so the simple solution here would be to to pad it. Right?

So so in this case and you kinda saw the code already in your in in your reduction. Basically, you know, this is m of zero and two. Right? You know, the the matrix is right here. So anything that's outside, if it's outside the boundary condition, right, instead of moving garbage values or whatever, right, you just write zero to it.

Right? You saw the coding in reduction if you implement it correctly. Right? And then this code doesn't change. Right?

Because as far as I'm concerned, it's still a two by two matrix that I'm doing my partial product sum over. So so very much like when you're reduction, there'll be some if conditions on the moving part or else or else load zero. Right? So so that's kind of the pseudocode that we see. So, you know, instead of checking row row and width to make sure I'm moving my output element.

For my input element, I have to check if that index here, you know, essentially is you know, this is within the bounds, essentially. Right? You know, all of this is still width, and, similarly, your column has to be within the bounds diff. So yeah. And this is also within the bounds.

Basically, any any day in here in here has to be within the bounds, essentially, or else is zero. That's the condition you had to check for. And finally, you know, this is the trickiest part. Now we're gonna have to handle you know, at least in a pseudo code, everything is still square. But now we had to handle rectangular matrices.

Right? We're gonna have some some, you know, instead of instead of width, width, width, width, right, that we saw before, all of these are gonna have different dimensions. Right? In this case, what they say, j and k, and this will be k and l. Right?

So the output is j and l. From from all of these code, right, all of these width, you have to figure out what width corresponds to which width and replace it with the correct width, if you know what I mean. Right? So so, you know, this problem is less than width. Right?

Is it this? Is it this? Is it this? Or is it this? Or is it this?

Or is it this? Figure out which which one does this and replace it with the correct j k URL. Right? That that's the trickiest part, I think, of this assignment when you when you generalize it. Right?

So so, really, the the biggest thing so so we test three cases, I think, in the GitHub actions. Like, a perfect square that fits in your thread block, a square that doesn't fit in your thread block, right, with the padding. Make sure the padding works. And And then the scenario is a break weird regular irregular, you know, rectangular matrix to make sure, you know, whether you got the j king l correctly. Right?

So so my my suggestion for approaching this homework, right, is don't code this right away, but kinda incrementally, you know, make it work for a square. Make it work for a square that doesn't fit within your thread block, you know, not a multiple year thread block. And then, you know, the last case of a rectangular matrix. Right? So so the biggest hint for this last one is just what width is that width referring to and and replace it with the corresponding j, k, or l.

Okay. And that's that's probably the the trickiest part of it. Okay. Any questions? No?

Alright. So with that, you know, assignment two assignment three is assigned. It's very much the same thing. Right? Go on.

Get a classroom. You'll get cloned. I think both. And then, you know, just some questions that you answer at the end. Same thing.

The TA and the grading, they should be done grading assignment one and two hopefully this week, so all of your grades should well. Alright. I'll see you on Thursday. Okay.


Alright. So just a bit of a recap. Right? So on Tuesday, we talked about matrix multiply, and then that assignment is assigned. So matrix multiply, you know, the main goal there was just to see how we could, you know, make use of shared memory in a more, you know, complex efficient way compared to what we do with that reduction.

Right? So, specifically, you know, if your matrix is large, how can we kind of break it apart and just take a tile of of data that fits initial memory at a time, and and essentially make make your memory access faster. Right? So today, we're gonna continue on that trend talking about specifically, memory again. Right?

And other optimizations you could do to improve your memory access. So this one specifically is not necessarily geared towards matrix multiply itself, but it's another general trend that we see with the the GPU hardware. Right? And, you know, like, if you think back to the reduction assignment, right, when we move the data from your global share, right, we index in a certain way. Right?

There's different ways to index that. The way that we presented before was the more memory system friendly way. And today, we'll see why why that is. Right? So, really, we're gonna talk about memory coalescing, and and this slide set goes a bit lower level.

So, you know, just to just to kinda go over, why why it happened about the hardware. Right? So, yeah, specifically, we're talking about, you know, memory access performance, right, and the and the memory subsystem on a on a GPU. Right? So beside besides space, right, you know, like, when you buy a GPU for your games, right, the amount of, you know, 12 gigs or 16 gigs or eight gigs, you know, that's important for your GPUs.

Right? So what are other factors that's important for your GPUs, like, in terms of games besides size? The what? Cooling? Yeah.

Cooling is important. If it overheats your your GPU's, naturally ramped down the power and and frequency. Right? What other factors of your memory devices do you look for? Some relative people.

Right. Yeah. Like, how fast you can access data, like latency. Right? The other aspect would also be, like, throughput.

Right? How much data you can move move at a time. Right? So, we're gonna learn a bit about the memory architecture, specifically how memories build and then kind of build up into why we have memory coalescing, in in GPUs. I don't think this topic was covered in in, like, one six one or anything else like that.

I don't think you talked about memory systems. Right? DRAM channels, banks High level. At a high level. Right?

Yeah. So so we'll we'll touch it at about that level too. So this is just kinda giving a background just so, you know, when we talk about memory coalescing from a programmer point of view, right, we know what's happening in in the hardware. Right? So, you know, ideally, the the biggest issue with GPUs, it's just the the bandwidth.

Right? Like, bandwidth is different than latency. Right? Latency is how fast I could access a single piece of data. Bandwidth is is how much data I can move at a time.

Right? Just kinda like with CPU and GPU, you know, how fast you get execute a single thread versus how many threads you can execute throughput. Right? So, you know, ideally, we want a lot of bandwidth of data flowing from your your memberships to your CPU or GPU. Right?

But in reality, it's actually very limited. Right? It's it's kinda like trickling through the straw because the main limitation there is is, you know, when you go to a computer, right, your your DRAM slot onto the motherboard, and then it's just copper wires connecting your your DRAM to your CPU. Right? So but that's a very slow, you know, narrow connection.

You can't make that bus very wide. Right? And that's kind of, like, how, you know, your DRAM chips are all designed. Right, it's all connected to that very narrow, you know, offset data that your your DDR three bus or your DDR six bus and stuff like that. Right?

You know, if if you ever look at a DRAM, SIP. Right? You know, there's also little square black boxes on it. Right? All of those are are DRAM chips.

Basically, inside of that is is what, you know, what this this diagrammatic is. Right? It's just a whole bunch of memory cells. You know, there's there's a row decoder that decides which row to open. And then within every cell, it's it's a capacitor.

Right? The DRAM is stored. So these zeros and one test capacitors. They typically store it at a very low voltage. Right?

So then we have a set's amplifiers. Most of you design amplifiers before? No? EE people? Yes.

Right? EE people make amplifiers. So so yeah. So so the sense amplifiers, increase the this, you know, the power of the signal. So we know it's zero one.

We kinda store it in in this cache. And then because the amount of data that we read in one of those chips is much wider than the bus, right, there's a mux that selects the the specific data that that goes on to this bus. Right? So, specifically, you know, that bus is gonna be that main issue. Right?

And, also, really, the the capacitor that that we're reading. Right? So, because DRAM you know, DRAM is cheap. Right? That's why we have it.

We're able to have high capacity. But the trade off there is that DRAM is very slow because we're really just storing charge in the capacitor. So because of that, your your interface speed, right, your your, bus, those tend to be clocking faster and faster as generations go by, right, because you want faster access to memory. But your capacitor can only charge and discharge so fast. Right?

That's just physics limiting it. So so your your interface speed actually ends up being a lot faster than than what your DRAM your your faster can actually handle. Right? So in in the old days with d r two and three, your interface speed is actually oh, sorry. It's slower.

Right? It's actually much I think that's actually incorrect. Force speed is more oh, yeah. Yeah. Yeah.

So interface speed is, like, eight times faster than how fast you can access your your capacitor. So so, yeah, if we wait if we just read this, right, there's no way we could saturate that bus. So there's a lot of DRAM techniques that's used to saturate your your DRAM bus. Right? And and that's really what what we're gonna exploit, from a programmer point of view.

How can we maximize, utilizing that DRAM bus and and filling it out of length? So so because of that, besides the speed of it, right, the interface width is also another issue. So so the amount of data that we read whenever we read one v DRAM cell, it's about eight times more than the width of your bus because we just can't physically put that much copper wires on your motherboard. Right? And in in modern GPUs, I think that that that sounds some worse than that.

Maybe it leaves us, like, 16 times difference or something like that. So so, basically, you know, everything kinda narrows down to that bus that that's causing an issue. So so just to kinda visualize, you know, how this DRAM works and build up to how we could try to saturate that bus. Right? This is a nice visual illustration of what a DRAM cell would like.

Right? So you have you have columns and rows. So it's a whole bunch of capacitors. Wait. Do you design this in one six eight?

In VLSI, you don't design this in in one six eight. Right? Memory cells? CS one six eight? VLSI?

No one took that? Okay. I forgot if you make this or not. The some courses in VSI, we actually designed these type of things. Right?

So so, initially, right, let's say we have this row buffer. You know, the SAS amplifier amplifies it into the row buffer. So let's say we wanna access an address. Right? The address is basically trying to access row zero column zero.

So it goes through a row decoder, and it just selects that column to read. So when when we activate that column, basically, all the charges in that capacitor is gonna flow down to that sense amplifier, and then we store it in a buffer. Right? So all your zeros and ones for that row, it's gonna be in that row buffer. Right?

So, you know, if I only want column zero, you know, this might be, like, you know, 32 bits, for example. Right? We're gonna specifically select that because my DRAM doesn't significantly error again than that that row buffer. Right? So then that that gets transferred.

So let's say I want to access my next piece of data, which is row zero column one. Right? So this is like a sequential address that's you know, if you're doing a vector at, right, you're you're accessing I, I plus one, I plus two, I plus three. That's, like, the next address. Right?

So if I wanna access the next address, right, your row buffer basically acts as a cache, essentially. Right? So it's gonna hit in that row buffer, and I realized, great. That means I don't have the access, you know, the actual capacitor part, which takes a lot of time and and it's slow. So I could just directly get that next piece of data, right, a lot faster.

Right? So if you ever wonder why random access is much slower than sequential access, you know, it's because of that role buffer connecting that you have in your community DRAM. Right? Even though DRAM stands for, what, random access memory, it sucks out random they access and stuff. Right?

So yeah. So so, essentially, right, we you kinda wanna make sure that the data that you wanna access, you know, it's all kinda sequential. Right? Let's say, oh, a five is a very wide bus here. Right?

Right. Very wide buffer. Right? You stick to pitting. Right?

So now if you wanna access row one, just like a cache, right, we're gonna realize, oh, we have a miss. So what what are you gonna do? Right? You gotta basically pick it out and meet the next row. And then, you know, that we set a capacitor.

Gets through the sense amplifier to increase the voltage, and then it gets buffered again. Right? So so we're gonna incur some penalty for accessing that, in a much slower way and and so on. Right? So so from this, right, you could tell that, you know, if I'm accessing a single data at a time, potentially, if they're in different rows, hypothetically.

Right? And I clear out my robot for every single time. It's actually very much underutilizing my plus. Right? So, you know, I'm accessing my array.

I'm sending a piece of data. I'm actually gonna really gonna send a piece of data. You know? If you look at the activity on that plus, you're just gonna get, like, spritz of data once in a while with a lot of idleness. Right?

So modern DRAM systems, like, especially with DDR two, three, and and on, right, they they have this burst load. It's like a burst transfer mode. And on GPUs, they basically do burst transfers by default. And and that's why we're gonna have memory processing. So, you know, if we have, you know, neighboring data access, right, like zero and one and, you know, up to 85.

Right? All of those are basically accessing the same role buffer. So we could basically send those right one right after another and and saturate the bus. Right? So so, you know, we're gonna have that access to your core and then transfer data, transfer data, transfer data until, you know, that whole load buffer is set.

Right? So so that's what that's what burst transfers are. And that's all how modern memory systems work. But, you know, looking at this, right, this only works if you have sequential memory access, essentially. Right?

So if you write code to access memory in a non sequential way, you're not gonna, you know, have this efficient memory transfer, you know, policy that that exists on your hardware. Right? So so memory coalescing, we'll see later on really is how can I get my code to access data in a sequential way, you know, with neighboring threads accessing neighboring pieces of data, essentially? Okay. Yep.

Any questions? No? Okay. Alright. Yeah.

So, you know, it is a bit low level, but, you know, that's a mechanism that's that's in place. Right? So so, you know, if you want to, you know, more fully utilize the the memory bus again, right, There's some overheads here, right, of accessing the array that I can't really hide. So the way modern DRAM chips, you know, kind of saturate the bus is by essentially pipelining. Right?

So when when if if you think about it, one of your array is accessing your capacitors and amplifying it, you can have another array, which we call a bank, doing a data transfer. Right? So so, essentially, a lot of those little black shifts that you see on your DRAM, each of those consists of multiple banks, typically, like, four or eight banks. And they're done in a way where, you know, there's enough banks to saturate your your memory's bandwidth. Right?

So for example, if I already have data here and I'm transferring, there there could be another memory access that's currently in flight that's being, accessed, you know, you know, accessing the array and the capacitor and discharging and so on like that. Right? So by the time all of this data gets transferred, this data will be here. So then this one could transfer it, and this one could access this next piece of data. So it's the hardware is essentially kind of pipelining this to higher memory options that we can see, etcetera.

Right? So so we call these different, arrays attached to the same channel as memory banks. Right? And the goal here is to really fully maximize that that memory channel. Okay.

Does it make sense? Yeah. It's a it's a bit low level, but, you know, if you have multiple banks and, you know, so efficient, eventually, you're gonna fill up all of that memory bandwidth. Right? So this is not only how to realistically, those chips that you buy have four or eight banks.

So so they can fully utilize it. Alright. So so that's the the hardware point of view. Right? Basically, we have burst that that occurs in the DRAM in order to maximize your your memory bandwidth, the bus.

Right? So you can kinda view your whole memory system as sections. Right? So, basically, you know, you have this physical address space, and the most efficient way of transferring data is is in terms of a DRAM burst. Right?

It could be, for example, a burst could be a 128 bytes hypothetically. Right? So So you can kinda view your whole memory system as chunks of contiguous a 128 bytes of data. Right? And your goal as a programmer is to write code that accesses those a 128 bytes continuously by neighboring threads.

So, you know, in this in this simple example, right, let's say you have 16 byte address space and your versus four bytes. Right? In the image space, you know, four contiguous bytes would would be considered a burst. So if your neighboring threads, let's say, have a work of size four, right, all access the same data that maps the same burst, that that's considered coalesce memory access. That's efficient.

That's very efficient. Right? We'll we'll see later how what kind of code will access this or not. But this is just defining what coalesce and uncolescess. Now if we have uncolescess memory access where, like, reduction, right, your thread access every other piece of data, right, especially when we start going down the steps in the reduction algorithm, that's considered very it's a uncollasp access.

Right? Your neighboring threads are not accessing neighboring pieces of data just like neighboring threads like to access the same program counter and do the same thing. Right? So so in terms of not just the instruction that they're operating on, but also the data that they access, They all like to do basically the same thing or neighboring things. Right?

That that's just how GPUs work efficiently. Similarly, if your things cross you know, if they're still continuous, right, but they cross different burst sections, that's also an uncoordinated access because that requires two revamp bursts. That's typically not your issue to worry about. But, you know, if you want there's, like, keywords and see, like, align. Have you ever used the align keyword before and see?

No. Right? It's it's probably too low level. You could use line keywords and see to make sure that that data structure is is, like, aligned up in a cache line, for example. And then accessing that data structure would would would be much faster.

So so things like that, and you'll be able to, like, for example, to make sure your allocations are aligned in per sections. So a lot of times, processing per sections, they're more or less handled by the compilers or or the runtime. Right? So so your your worry as a programmer really is, can I make my neighboring access neighboring addresses? Okay.

So if you wanna think about it. Right? If you want a all your threads and a warp to access neighboring pieces of data, your indexing is gonna have to look like some form of something something something independent of thread ID plus thread ID in the exponent check. Right? Because, naturally, when you have this term here, your neighboring threads are gonna access neighboring pieces of data.

Right? Make sense? Yeah. Alright. So so, like, we saw a reduction.

You know, we can access your data in two different ways. One way is coalesced, one way is not coalesced. You know, we can analyze that if you want later on. We'll we'll see some examples of what's what's matrix multiply right now. But, you know, the key takeaway here is just to make sure you have that, you know, plus thread ID x indexing.

And then your your code is more or less coalesced. Right? And, you know, and then in terms of two d memory space, you know, we have this wraparound, basically. Right? This is also, like, another consideration too.

Right? Essentially, if if you make your your tile too small and, you know, your memory you know, ends up being this long. So accessing this and accessing this may actually not be contiguous, right, if you're if you're making it still it's a little large size, for example. So so when you consider to the arrays, you had to consider, the size of your tiles too, right, besides just the memory we use. But but just also.

So if you recall, like, on Tuesday, right, we talked about whether we should do a 16 by 16 or 32 by 32 matrix multiply. Right? And and 32 by 32 has more reuse. Right? It turns out that if we view it from the point of view of the DRAMPers, I think 16 by 16 results in, like, double the amount of memory access because their their access is not coalesced as well.

Right? So the reduced projection is just is also more memory coalescing friendly. Alright. So let let's do this, like, simple exercise. Right?

Just to think about memory coalescing access. Right? So, hypothetically, let's say I have my matrix multiply, and this is how I'm accessing data. This is actually not representative of of the actual implementation. Right?

But just let's say, I have two threads. Right? Thread one is here. Thread two is here. And they're accessing, you know, a in this direction and b in this direction.

Right? So if I look at at this axis right here, right, with thread one and two doing a matrix multiply operation. Right? Would would that memory access from thread one and two be considered a a coalesced access or uncollapsed access? Are they accessing neighboring pieces of data?

Are they next to each other? I mean, it's it's not trying to scale, but assuming they're next to each other. Assuming that's row and rows are swollen. Are are those neighboring pieces of data? Yes?

No? Yes? Right. So so let's say this address right here is address. I don't know.

Let's say 400, and this width is, let's say, 100, and we're executing it ints. Let's say ints. What would what would this address be right here? What would that memory address be? 500?

Oh, it's like is the width a 100 ends? The width is a 100 ends. Yeah. Oh, okay. A 100 ends.

It's like 30 3,305. 307? Is memory is memory addressable by bytes or bits? Bytes. Bytes.

Right? Oh, that's Yeah. So you should multiply by four, not 32. Oh, okay. Yeah.

Alright. So, yeah, so so memory address 400 means, there's a byte at that address, not a bit at that address. Right? Memory is by addressable. Right?

Right? So so this address here would actually be, you know, 400 plus oh, right here would be, like, another a 100 times four bytes. Right? So this would actually be addressed at 800 right here. It's 400 next to 800?

No. Right? So so that access actually would not be contiguous. Right? Yeah.

So so that would not be considered a a coalesced access. Right? Because because at the same time, you know, we're we're accessing 400 addresses away. Right? Now if you consider these addresses, that thread one is accessing.

Are those neighboring addresses? Would accessing those address be considered coalescing? Yes? No? Yes?

Why why why would you say yes? But am I accessing them at the same time? No. Yes. So so big batch the emissions multiply.

Oh, right. I'm I'm doing that that products, dot product, whatever. Right? I have a for loop that's that's going plus k. Right?

So so, yes, these are neighboring addresses. But coalescing means that I have to access the state of data at the same time. Right? And I'm asking those data at the exact same time. The same instruction.

Now, like, there's a for loop. Right? You you access one data at a time as you go down in this chain. Right? So so even though I'm accessing neighboring data, they would actually be from two different, mode instructions.

Right? Because there's that for loop for every instruction, I do this then this then this then this then this. Right? So so my concurrent access is actually between thread one and two, not thread one and one, right, if that makes sense. Right?

So yeah. So so, you know, these accesses of thread one and two that happens concurrently during the same work, would not be coalesced. Right? So how about b? Would b's memory access be coalesced?

Yeah. The coalesced, I mean, it's only 1. It's it's on a it's all the threads in the work. So so neighboring threads in the work access neighboring addresses at the same time. So so, indeed, would thread one and two access neighboring addresses at the same time?

Yes. Yeah. Yeah. It would. Right?

Because, you know, we're doing that product sum. And there's a four loop. Right? And at every four loop, we're accessing these two pieces of data that's next to each other. Right?

So so b wouldn't be coalesced. Right? A a would not. You know, this is just like a hypothetical example. This is not how how it's actually implemented in real life.

Right? That's this is not the actual access pattern in real life. This is just illustrator. Right? So so in this in this figure, right, you know, b would be QLS because your neighboring threats are accessing neighboring pieces of data at the same time.

And then a would not. Right? A a would look like they have a stranded access. Right? Like we saw before, you know, their their neighbors in the y dimension, but their address is physically, you know, width times size of your data type data type addresses apart.

So it looks like it's a stranded axis. Alright. So now just looking at the real matrix local piecode, let's see what type of memory axis we have. Right? So so the two global memory axis we have, right, is your is your m and m and n right here.

Okay. So looking at m right. This is the naive version. This is the naive version. Right?

So so looking at at m. Right? You know, your neighboring threads would access this and this. Wait. If I want to you know, orange would access this.

Green will access this. And then blue will access this. Right? And then all of these will basically access the same row. I'm just, you know, too lazy.

Right? So this is not how this would look like. Right? M and n. Okay.

So so looking at n, would would n access the kernel s? Yes? Yeah. This is the same as the site before. Right?

That was not my last. Yeah. Good. Yeah. It is.

Right? If you recall, right, the the four that we see would be, like, something something independent thread ID plus thread ID in the x segment. Right? If you just look at the indexing itself, you you know, it'd be like k tensor plus column, but column is this. Right?

So we do have this independent term of plus plus thread ID in the x dimension. Right? So then just look at the code, you know, that that indexing would be coalesced because I have that plus that ID in the x dimension, and all the other terms don't have a threat ID in it. Right? I'm not multiplying something by threat ID, for example.

So so then, yeah, n n would be coalesced. Right? How about m? Would would m be coalesced? No?

That was a little trickier. So so all the threads in the work, what are they accessing, at the same time? The same address. Right? They're all accessing the exact same address.

So this is kinda like a a yes and a no. Right? So the your neighboring threads are accessing the same address rather than neighboring addresses. So So the hardware unit, there's a memory coalescing unit in the hardware that identifies these memory access patterns. So because it's all accessing the same address, that unit is not gonna send, like, 32 loads to the same instruction.

It's gonna coalesce into one load. Right? So, technically, it is coalesced, but you're not receiving a whole burst back. Right? You're just receiving four bytes, essentially.

So so, technically, it is coalesced, but you're not taking full advantage of a a burst transfer. So that's why it's very inefficient still. So, you know, that's kinda like a yes and a no answer right there. It's kinda hard to classify that. It's not it's not also necessarily uncoalesced here.

Yeah. So then, yeah, ideally, we need one multiple uncoalesced. Right? Mhmm. So how would you call us m in this case?

How would we call us m? Well, tile matrix multiply. Yeah. So there's a there's another benefit of it. Right?

So so if you kinda recall from the the tile matrix multiply case, Now when we have a tile, we have to cache, a tile at a time. Right? So your neighboring threads here would cache this and this at the same time. Right? So so those accesses there would be neighboring addresses at the same time.

Right? That does that make sense? No? Or is it hard to see? Can you explain that?

Yeah. Okay. It's not very obvious. Is it better if I just draw it with a different colored dots? Yeah.

Okay. Let let me just do this. Okay. So if I zoom in on on a part time caching, right, right, so this is this is this. This part's this.

This part's down here. Let's say my my work is 32 by 32. Right? So I have a whole bunch of threads here, which I'm not gonna drop over to. That's gonna take me forever.

Okay. And just for fun, this is black one down here. Right? So so these threads are gonna access what address in that tile of m and n. This is the cache version of m m and n.

Right? They're gonna access what? What do they what are they responsible for moving in m and n? Where would that rep dot be? No one remembers Tuesday?

Same place. It's exactly the same place. Right? We just kinda overlay the threads over that memory address, you know, the the the matrix. Right?

And And and it's a one to one mapping. Right? It's it's an overlay. So so that indexing that we saw here that we derived on Tuesday basically gives us that that overlay, a one to one overlay. Right?

So so, you know, black removed this, Red will move this dots, and green will move this dots. Where is it? Blue then purple. Blue will move this, and then purple will move this. I'm too lazy, but it'll be exactly the same way.

Right? So so, you know, with that with those four threads on top, let's say that's a word that I'm lazy. Are are they accessing neighboring data at the same time? Yes. Right?

For reference, we're accessing all of this at the same time. There's a parallelism. Right? All all my threads in the block is accessing that at the same time. Okay?

So yeah. Right? So so all of those colored dots on m, they're accessing the the neighboring threads at the same time. If you look at the indexing, right, you see this independent term right here, that that thread ID. And then everything else, row with p times tile width, those are independent of t of x.

So so, yeah, that would be coalesced as well. And then n, assuming I drew the same thing, right, would behave exactly the same way. If you look at indexing, you know, there's not independent of thread of x. You know, I have a thread of y. Thread of y doesn't matter.

Right? Because because my memory address system is all role major. So I I just wanna make sure that my thread of x is independent. And then I have plus column, and column is defined as this. Right?

Where you have that independent the thread of x term as well. Right? So so, you know, that that and the the n indexing also meets that criteria. But, you know, visually, that that's what's happening. Right.

Does that make sense? Yeah. A little bit. Okay. If if you're confused, the takeaway is look at the index.

Do you see something that says something something something plus net ID in the x? And if it is, yeah, it's spelled less. Right? That's like Cliff's note. Wait.

Do you guys have Cliff's notes growing up? That was, like, before Chat. What is it? SparkNotes. Oh, SparkNotes?

Yeah. That's stuff. Yeah. They have those in libraries. Have you seen a library before?

No. Okay. So, yeah, that that's more or less a takeaway. Right? Trying to find that thread ID in the x dimension as an independent term.

Questions? It's ready for why. It's not matter because it's already. Because coalescing only matters when you when you access contiguous addresses. Right?

Memory systems are designed in a row major way. Right? Right? So so, naturally, row major means contiguous addresses are in the x dimension, right, across columns in the same row. Y ID would be traversing rows.

Right? So y of zero and y of one, you know, would be where was that picture? Right? So, you know, this address and this address will be Y Of 0, y Of 1, or whatever. Right?

So it's in the strider access. Right? You want contiguous addresses, which is naturally in the in the x dimension. Right? So that's why why why t t of y it's gonna be in the y dimension.

It doesn't impact coalescing behaviors. Now if my system is column major, I don't know why you would design a memory system like that. Right? Then you would look at why. But, you know, everything everything's row major.

Okay. Any other questions? Alright. Yeah. So so, you know, this is basically the same thing that we were just talking about before.

I drew it out. I don't know if it makes sense. I'm a visual learner, so I like to draw things out. Okay. We're good?

Yes. Alright. So, yeah, that that's one of the things that to to take away. Right? We just wanna make sure neighboring addresses front of each other.

Alright. So just to give a bit more overview on on the memory system of things, like, on how modern GPUs. Right? We we keep squeezing more memory access, performance out. Right?

Modern trends of GPUs is that, you know, we we have to be able to have enough memory bandwidth to feed my GPU core, which gets faster and and more compute heavy and data hungry. Right? So from, like, the p 100 all the way to the v 100, which is kind of being released right now. Right? Your whole memory subsystem, right, has actually grown a lot in terms of the amount of data it could it could throw into your GPU chips.

Right? So in the early days of GPUs, like, when you were a kid, it's like elementary school, I think the memory bandwidth was, like, tens of gigabytes per Now then we saw hundreds of gigabytes, and now we see terabytes. So terabytes per are actually pretty common nowadays even in your consumer ones. Right? So so these RTX one are the consumer cards.

Even those now are terabytes, at least one terabyte. And then, you know, your data center class cards for machine learning training and high performance computing, they're significantly more, like like, eight terabytes or more. The main difference between these two type of technology, is that, you know, your GDDR and stuff, you can think of it like they sit on the motherboard. Right? You know you know, when you have your your discreet GPU card, that that GPU card is like like a motherboard.

Right? So the memory chip is on the motherboard itself. And then all of the modern data center GPUs, they move to a technology called high bandwidth memory, HBM. If you follow GPU news or computer news, you've probably seen that before. Right?

HBM? No? No? Okay. Just me.

So HBM technology allows it to be significantly faster. So we'll see why that occurs, actually. Right? So so, yeah, these are two two of the GPUs. If you take off the heat sink, right, this this is actually what it looks like.

So your your GDDR chips, right, are these little black squares, you know, are these little black squares. They sit on the motherboard. Right? That means that connection to your GPU chip is is through copper wires. And not only that, that means you're limited by the the pins that you could put, right, between your your chip and your your memory chip and your GPU.

You know, if you ever built your own computer before, right, like, CPUs have those long, little, narrow pins that you have to put in. Right? Some of those pins are your memory channels. Right? So and some of those pins are your PCIe channels.

So all of those are are very limited. But these memory ones are typically if you ever take it out, right, and you look on the back of it, you you see, like, shiny little circles that's, like, slightly concave. And then if they're on, like, ball joints and then you solder it with a heating gun. You ever had to solder those stuff with things? No?

Yeah. They're fun. Sometimes you just put it on and you put the top gun on it, and then it just kinda moves into place. It sucks in. It's very satisfying.

But then if it doesn't work, it's a pain to remove it and clean up the solder. But, yeah, I don't know. Anyways, that's the easy side of me. Yeah. Right.

So so very pin limited. Right? So because of that, it's very limited bandwidth. Now with the HBM two, you notice that your memory is actually not on the motherboard itself anymore. Right?

It's actually on your GPU die itself. So so you have you heard of chiplets before? No? What is You heard the phrase. Right?

Yeah. So so monitor GPUs and CPUs, they all move to something called chiplet design. Basically and and we'll talk about this in a couple of weeks. Basically, at the high level, we we can't really fabricate large chips anymore just because, you know, your transistors are so small, and then we keep getting a lot of errors, you know, the heat goes down. So we we built larger chips out of smaller chips and then just label it together, essentially.

Right? You you use breadboards before in class. Right? Imagine your breadboard is just another piece of silicon. So you just put silicon on top of silicon, and then, you know, you do, like, do silicon via the TSVs and stuff like that, and then connect them together, essentially.

Right? So so this is essentially now instead of your memory communicating with your CPU, GPU, or CPU through copper wires in your motherboard, it's just another metal layer in in your silicon. Right? No one took one six eight here? No?

Yeah. Right? You had to do all those metal metal layers and routing it by hand. Yeah. Fun stuff.

Right? So that's essentially, you know, the the connections now. Right? Because now we're dealing with with metal layers, you got more chins, essentially, because they're smaller. And the signal thing is a lot faster because we're not dealing with copper.

So so because of that, you know, your data center GPUs can have, you know, like, four x more bandwidth compared to your consumer grade cards that you could buy yourself. But but, you know, this is mostly for machine learning training. Like, games if you still wanna play games, there's no need for this amount of bandwidth. Right? That's why HBMs don't exist on consumer grade cards.

I mean, you you could. It'd just be really expensive, but games, you know, you don't take advantage of it. Unless you do ML training, then then you wanna get your hands on one of these a one hundreds or a eight hundreds or whatever, those type of cards. Right? So so, you know, modern generations of memory.

Right? You know, you go from, like, GDDR three, four, five, six, seven, or something like that. All of those really are just different standards. They kinda encode things differently. I assume some EE people took communication courses before.

I don't know. I hate those classes. Yeah. Right? So you can encode things in different ways.

Right? And then you get different signal then and and put more channels within the same number of pins. Right? They get really clever with this type of stuff. So so that's kinda, like, you know, the difference between the different GDDR generations.

Right? They're able to encode things in a much more compact way, giving the physical constraints of the number of pins they have and faster too with some signal integrity. HBM, on the other hand, we're just gonna build labels, essentially. Right? It's so much more easier.

I I don't know if it's easier. So this is actually the one thing that that AMD has an advantage on. Right? So so AMD was one of the pioneers in building chiplets, and they were the to market with these, chiplets based CPUs. Right?

That's why, I don't know if you guys remember, but, like, for a while, you know, all all AMD CPUs were horrible and Intel is basically the best CPUs. And then all of a sudden, AMD CPUs are much better now. Right? That probably happened since you you were in high school, I assume. Right?

AMD GPU CPUs are better in high school for you guys. Right? Yeah. So that was around the time when when when AMD CPUs were using chip like device. So they were able to make it, you know, much larger, with better yield at a cheaper price.

Some of them have HVMs, like supercomputers. I think modern CPUs now have some HVM too. In fact, some of their caches are, like, HVM also as well. And then that that allows them to essentially label it together. Right?

So you can actually three d stack your DRAM, with some logic die. And then this interposer is is how they kinda communicate through each with each other, right, between your CPU and and your DRAM directly. So this, you know, this is how modern all modern CPUs and GPUs look like nowadays. You know, if you don't have, you know, your your physical DRAM will will, of course, still be out here. Right?

Your physical DRAM device. But but, you know, more integrated stuff, have your DRAM on the same chip. Right. So, the MI 200 x was just powering the the x six scale computers nowadays, the supercomputers. Right?

They actually now figure out that, hey. Those breadboard, right, that you're kinda labeling everything on top of, you can actually put logic in that breadboard itself. Right? So so your your breadboard isn't necessarily just connections, but you could put features and functionality into it. So a bit you know, AMD calls this Infiniti cache.

So they put a cache inside of your breadboard, essentially. So, essentially, you you have, like, l one, l two, l three, Infinity Cache, and then your memory, essentially. So there's a lot of interesting things that that's going on in the packaging space, for for devices nowadays. Right? So so, literally, the the the cache hierarchy, you know, you add another layer of hierarchy nowadays.

Yeah. Right? And then with NVIDIA so so NVIDIA caught up, and now NVIDIA has chiplet designs too as well. One interesting aspect of that now is that, you know, you can have your GPU basically consists of multiple GPU chiplets. Right?

So so, you know, you you think of your GPU as, like, one giant processor with a whole bunch of SMs. Right? You can imagine how you have SMs on different chiplets, you know, or, like, a couple of SMs on different chiplets, and you have your memory on different chiplets. So the modern GPU is, like, the a 100 and up. NVIDIA has a feature called MIG, multi instance GPU.

AMD called it partitions. AMD marketing has four core names. You could slice up your single GPU based on chiplets and expose that as multiple physical GPUs. So so a 100, if you run NVIDIA SMI, right, you'll see one GPU. But with MIG, you could expose it as seven physical GPUs.

Right? So when you when you run a mini SMR, you see, actually, you have seven GPUs. Or you configure it. Right? You can configure it so that you can have, you know, three GPUs, one of one slice, two slice, or four or four slices.

So so then you have, like, GPUs of different compute capabilities. So so this is, you know, some other things that can happen now because of, chiplet big design. And from the memory point of view, if you think about it from the security point of view, right, if you run wanna run, like, multiple programs together, this provides isolation between your memory spaces as well. Right? So from a security point of view, in a shared environment, in a cloud, right, this this provides a lot of strong security guarantees, between your memory.

So so you can't really snoop on someone else's memory access for side channel attacks and so on, like, that type of thing. So so that's kinda, like, the the modern view of memory systems nowadays. Right? But from your programmer point of view, you know, memory coalescing is still a thing. Next week, we'll talk about more optimizations for data movement.

So we're gonna we're gonna talk about streams and how we can hide the memory access latency from your CPU with computation. And then, you know, on the hardware side of things, these are some of the the innovations that's going on too to improve your memory. Because because nowadays, you know, memory is the main bottleneck for a lot of GPUs and and workloads, especially with machine learning training and so on. Okay. Yeah.

It was a short lecture today. Any questions? If not, I'm done. Enjoy your Thursday. Have a nice weekend.


Good. So so last week, you know, we talked about matrix multiply and how we can use shared memory in a more, you know, optimized way. Right? Basically, dashing in a bit of a tile that fits in the shared memory and then basically, dashing it and then using tiles to swap it in and out, right, depending on how long your metrics are. And then that's your assignment three, which is due, I think, this Thursday.

Right? Yes. Okay. I might push it back to Friday because my my grade is not gonna be great anyways. And this way you get these office hours to bug your TA instead of me doing my office hours.

Yes. Okay. So so I'll probably move that to Friday. So we're gonna continue on to talk about more optimizations. So in the beginning of class, right, we talked about how, you know, you have global memory, and then we introduced the concept of shared memory.

It turns out there's actually a lot of different memory spaces and GPUs. So so besides this, you know, we have constant memory, and and today, we're gonna introduce something called pin memory as well. So we're gonna look at more ways to optimize, more more data movements, essentially. Right? So, specifically, besides the data movement that we have from your global memory to your compute units, you also have memory, access from your CPU to GPU with CUDA mem copies and stuff like that.

Right? So we're gonna, like, have to optimize your CUDA mem copy essentially. So so, you know, the topic we'll talk about is something called, like, chain host memory. So if you haven't taken operating systems or one six one, this goes into virtual memory a bit. So so we're gonna take this a little bit low and introduce it.

So, basically, you know, your your GPU sits on a PCIe card. Right? And it's it's discreet to have a separate memory space. And the way you handle the data movement is actually through a DMA engine, direct memory access engine in your on your GPU card. If you think of better systems, you probably program a DMA before.

No? Yes. Have you heard of DMA before? No. Yes.

No. Not really? Okay. So so, basically, you know, one way to move data is to load in stores. Right?

The alternative way is basically, instead of your your CPU issue instructions to move every single byte of data, This is, like, offload engine. There's a DMA engine that does the data transfer for you. So rather than doing a whole bunch of loads in stores and move the data manually, this DMA engine, you just offload that command. For example, move x slice from physical address a to physical address b. Right?

And then that that DMA engine will handle the actual data copy, essentially. Alright. So because of the DMA engine, there's actually some limitations, that causes correctness issues, and ways around it that that CUDA does to essentially to enforce correctness. Right? So the there's some performance overhead from this that we'll see in a little bit.

So just a a bit of a refresher to see how this whole system looks essentially. Right? You know, like, when we do vector add, right, we have to allocate data to your host memory, populate it with a vector, allocate data on your c on your GPU side, and then and then move the data over before the, process on the on the GPU side. Right? Alright.

So so just, you know, as a bit of refresher again, how do you allocate data to your CPU? Neo. Neo or or malloc. Right? So so when you call malloc, you get a a pointer back.

Right? That tells you where I allocated that space. So so MATLAB, does it does it give you a virtual address or a physical address? Virtual address. A virtual address.

Okay. Right. So so MATLAB will give you a virtual address, and it'll allocate some page here, for example. Right? So this this memory that exists in in a memory, main memory DRAM, right, has a certain address as well.

Right? So so when we allocate stuff to the main memory, you know, does your memory chip take a physical address or a virtual address typically? Your memory chip. Your memory chip. Yeah.

On your DRAM. But the if you if you if you give your DRAM a virtual address, will will it understand it? Yeah. Or No. No.

Right? Your CPU is Yeah. Yeah. Yeah. Right?

Your CPU understands virtual address, but the DRAM, the physical hardware has no notion of what a virtual address is. Right? So so the address on the on the DRAM side is a is a physical address. Okay? So so there's a translation that goes on in in your operating system that you implemented, I think, in operating systems, right, that that handles the snapping, right, in the form of a page table.

Right? So then, you know, it gets translated through a page table to a physical address, and then, you know, that's your, you know, let's say, mail f a, something like that. Okay. So on your GPU side, right, how do we allocate data or, you know, reserve space on my GPU side? How do I reserve space in my in my GPU?

And then. It's a CUDA call. Yeah. CUDA CUDA MELLA. Right?

It's just CUDA MELLA. Right? So CUDA MELAQ gives you a physical virtual address. It's a physical address. Right?

We learned about unified memory, before that. That's kinda like the virtual address for for GPUs. But but in this case, it's a it's a physical address. Right? So we get some physical address location here, so it points directly here.

There's no page tables or anything else like that. Alright. So then how do I move data from CPU to GPU? What? Yeah.

This is a CUDA mem copy call. Right? So underneath the hood, right, this is basically how CUDA mem copy works. Thank you. Oops.

So to do the move, you do CUDA mem copy. Right? And this actually does the the movement through the DMA engine. Right? And then you get your value eight here, essentially.

Okay. So, you know, at a high level, right, now you know that CUDA map copy is essentially sending some kind of command to your copy engine or DMA engine, essentially. And and that handles the activate of movement. That's when we had to specify the direction as well. Right?

So the DMA engine knows they'll switch direction to copy it to. Okay. So one limitation of a DMA engine is that it it directly communicates between different, memories. Right? That's that's just how DMA engines operate.

And and at a physical level, there there's no notion of virtual address. Right? So when you do a DMA copy, the commands that understand is a physical address or physical address. Movement. Right?

To move physical address a to physical address on the CPU side. Alright. Do we see any potential issues with this type of setup? Yeah. What happens if the GPU region that we want to move has been, like, debited from memory and put into?

Right. Right. Yeah. That's that's a potential issue here. Right?

So on the GPU side, right, the the space allocated for a is not gonna move. Right? It's guaranteed to still be there. However, on the CPU side, right, this mapping here, it's it's not a static mapping. Right?

It's managed by the operating system. This mapping could change at any time, whenever there's any page faults or whatever like that. Right? Or I don't think we did, like, defragmentation or anything else like that. But, you know, because of evictions and the way we shuffle things, right, that that that mapping is not guaranteed.

Right? So so what we call a CUDA memcpy. Right? Copy, you know, takes, you know, this virtual address and this physical address. And then when we send the DMA command, right, we send these two commands for it for the DMA.

Right? Because that map is not not guaranteed, it's possible that as I'm doing my mem copy, my operating system could change this p of a to something else. Right? Let's say, oh, no. Now I'm gonna change it to p of a and point it to here instead.

Right? But my DMA engine is still pointing to, you know, this old address. It doesn't know that my a got moved halfway. Right? Because of that that mapping then, that's not static.

Right? So how can we guarantee that my mem copies are correct and the data that I need to copy is essentially copied all the way through. Any guesses? You know what? Automate control.

Yeah. Automate construction to hold you. Yeah. It's like use a VA instead of PA. Right?

We yeah. But but the hardware only understands PA. Right? So you see how to do that translation. Yeah.

There a way we can prevent the physical data from moving? Yeah. You can. Yeah. Right.

So when you allocate data, it's a Linux feature. Right? When you allocate data on the CPU side right. When you create a page I don't know if you ever done, like, low level Linux programming before. Right?

You could you could tag some pages as as a pin or, like, not swappable, essentially. That way, the the operating system's page table knows not to ever touch it, that mapping, essentially. Right? But, you know, that actually causes some issues because you actually have a limited amount of it. So so typically, the amount of these, you know, pin pages that exist, it's a it's a limited amount.

Right? So when the when the CUDA driver gets initiated, they actually allocate a certain region of memory here, which we call pin memory, and it's small, that that's guaranteed for that virtual to physical just mapping not to not to change. Right? So, essentially, to avoid the whole issue of, the page table mappings, switching halfway through your mem copies, what ends up happening is that whenever we call this mem copy operation, you know, my DMA was essentially, oops, mapped to this pin region because I know it's guaranteed not to change. What ends up happening internally is now there's there's a internal copy that occurs.

Right? So so this is how your your main copy actually looks like in in real life. Mem copy actually incurs two mem copies. So one from CPU memory to CPU memory and then CPU memory to to GPU memory. Right?

So so from your regular heap space right? So so my lot goes into heap space. Right? So this would be in heap. It will get copied into the pin memory space where where your driver basically allocates it and mark those pages.

And then the DMA will will do that transfer from from the pin memory to your, CPU memory, using that that DMA memory. Okay? So so this is essentially what what CUDA mem copy is doing. Right? So so, clearly, there's some overheads.

Right? I'm copying twice. I can't avoid it because I need this for correctness. Right? Alright.

How can I optimize this? Right? There's there's some overheads. Data movement from CPU to GPU is a is a mean it's a pretty big bottleneck. Right?

So so how can I optimize this non copy operation? It's not transfer data at all. Right? Nobody can avoid that. So Instead of initially allocating outside of the PIN memory, you could just initialize inside of the PIN memory and Right.

Yeah. Right? So so instead of allocating the heaps, of catalog, right, it's possible if your data is small enough. Right? You can allocate your your data directly into PIN memory.

So then that way, there's no extra copy from your heap to your pin memory, you know, from within your CPU. Right? So, yeah, that that's one way of doing it. Right? Later on, we'll look at stream and see how we could essentially overlap this.

We're we're not gonna avoid it. We're gonna hide the latency, essentially. Right? But if you can't hide the latency, one way is to directly allocate your data into PIN memory if if it fits. Right?

So if we directly allocate the space in the PIN memory, There's a API for that, but we'll see in it later on. So your your a would just directly go in here, and then that's it. Right? We we don't have to do any other transfers. So this is essentially, you know, the optimizations that we can do for for your put in mem copy.

So the so the next couple of slides is essentially, you know, just describing what we talked about. Just just drawing. Right? So so, basically, you know, your your CUDA malloc your your malloc, sorry, gives you virtual addresses that has to be transferred to physical addresses. However, DMA, you know, only knows physical addresses, so it's possible that that virtual page could be swapped out whenever I'm doing a bad copy.

So in order to avoid that, I'm gonna need to use something called PIN memory. In the next, it might be called locked memory or page locked memory depending on the terminology. So now there's a special API that allows us to use this this special region of PIN memory. So now, you know, you could allocate memory with, you know, CUDA malloc, CUDA host alloc, and then CUDA malloc manage. Right?

Just depending on the different memory spaces you want to allocate to. So so I talked about that. Right? So so the API to to use the PIN memory, it's got CUDA hostalloc. So this is essentially another API from CUDA that allows you to to target the PIN memory space.

So so would this be a a replacement for CUDA malloc, or would it be a replacement for malloc if I were to, you know, use this API to optimize a program? Let's say your vector app. You You would replace malloc with CUDA. Malloc. Right?

Because there's a whole side memory memory space region. Right? Pin memory doesn't exist on the on the GPU side. But rather, it's it's in the whole side, the the CUDA runtime, essentially. Right?

So so CUDA host logic essentially allows you to to allocate to this pin pin memory space. So, yeah, it's it's essentially a replacement for for your MATLAB. And and just by doing that, your your CUDA mem copy operation will be twice as fast. Right? So if you ever time your CUDA mem copy if one of the pointers come from malloc versus one of the pointers come from CUDA host malloc, you could remember the copy where you try to ask us.

You're avoiding an extra memory copy, essentially. Right? So if you're doing your final project, you know, this is a very easy optimization that you could do if if you're using a small amount of memory, essentially. Okay. So so the code look looks something like this.

Right? So the CUDA holds that lock API looks very much like like malloc, where you you pass it a pointer, the size of the allocation, and then there's there's some flags. By default, this put a host all off default. I think there's some flags that that just changes the behavior of, that PIN memory a little bit. In most cases, you don't have to touch that.

But it's very much just like a a regular mail lock call. Okay. Any questions with this? No? Does that make sense?

Yeah? How do you on the account, do you have the number? Is it the same way it is? What is it? Free?

Yeah. Just put a free post. Yeah. So every time there's a unlock, there's a free associated with it. Okay.

If you have a AMD GPU, AMD has something similar as well. The terminologies are pretty similar now. So I think some of you are yeah. Limits on how much you can pay. Oh, that's typically limited by the operating system.

So so these these pages are physically allocated when when a driver loads up. And in Linux, there's a limit to how much memory space each driver can allocate. So, typically, they're in the order of, like, a couple of MBs, like, hundreds of MBs. Yeah. It's not like infinite, infinite states.

I think there might be ways around that too. Like, you could change the Linux parameter or recompile it and then change that limit as well. But, yeah, it's typically limited by by Linux. Okay. So that that's one optimization for your, data movements.

Right? So the other one is called computer streams. This is actually very commonly used not only for for data movement optimizations, but for other forms of, optimizations such as, you know, if you have multi GPU programs. So so, for example, you know, machine learning inference. Right?

Have you heard of, like, pipeline parallelism and tensor parallelism and all these other stuff for LLMs and whatnot. Right? So so all of your modern LLM inference engines and ML inference engines, they all make use of CUDA streams because it allows concurrency on your GPU. So so, essentially, a lot of times, when you write a kernel running on a GPU, it might not use up all the resources of a GPU. So you can actually run concurrent kernels using using CUDA streams, essentially.

So so, normally, you can only run one kernel at a time, but it allows you to run multiple parallel kernels. So now you you can think of it like another layer of parallelism, essentially. Right? But you have multiple GPUs. You would have to use this too.

Okay. So so the example we learned today is essentially a form of, pipeline parallelism, basically to hide the the data transfers of your workload with more computation, essentially. Right? So so the example that we use use here is this vector add. But, you know, this this also applies to every other program we we write in class.

So so if you wanna think about it from a vector f point of view, right, we we have vector a. We have vector b. Oh, they should be the same size. Right? You add those, and then you get vector c.

Right? So so before I could do that, the add operation, in vector add, I basically had to copy all of my a and all of my b. Right? So I had to wait until my whole transfer of a is done, my whole transfer of b is done before I could do my computation. Right?

So this data transfer time, I can make faster with, you know, 10 memory, hypothetically. Right? But still, there's a lot of data transfer time that is keeping my my my GPU idle, essentially. Right? So, you know, one way I can try to make this faster would be, you know, transfer less data.

Right? Let's say I can encrypt data, compress my data. Right? That that's one way of doing things nowadays. You can compress your data and then send compressed data over.

But then you could try to compute on compressed data, right, or encrypt the data, like, homomorphic encryption or whatever like that type of thing. But, you know, that's not always possible as well. Right? So so what's another way you could kinda optimize this? So instead of waiting for a and b before I transfer before I do my computations, can I, you know, somehow mask that or avoid, like, dinner transfers?

Yeah. Well, transferring a and b are independent on each other, so you can do them at the same time. Right. Okay. So, basically Like, I wanna do b at the same time here.

Yep. And then Okay. So so would a and b use the same resources Yeah. Physically? I don't know.

They would use different memory spaces. Right? But but what's what's doing the physical copying? On the DMA. The DMA.

Right? Can the DMA engine do two copy operation at once? You only need two DMA engines. So so, you know, they they both use the same physical resources. Right?

The DMA engine are both attach you know, there's one DMA engine that attached to the PCIe bus. The PCIe bus is bidirectional. So, technically, we can do two operations at once. Right? But they just have to go in opposite directions.

Right? So, you know, this is a device to host direction. This is host to device, and this is also host to device. Right? So so in theory, you know, c could overlap with a or b.

Right? Because they're using different resources. So so in theory, I could I could transfer b and c or a and c, in in parallel. But but, no, I can't I can't do a and b at the same time. Right?

So so what's another way in which I could potentially optimize this and and speed things up? Yeah. You could transfer, like, a piece of a and b, do the computation, and then transfer it back as you transfer, like, the next Yeah. Exactly. Right?

Yeah. So so when I did my vector add computation, I don't need all of my a and all of my b to compute on it. Right? Because the way we we partitioned up the problem is that, you know, this is handled by a thread block. You know, this is handled by another thread block, essentially.

Right? So so when this thread block wants to operate, all that needs are, you know, are this data here, this data here, this you know, to get this data here. All of this could still be transferring, and this will still be still have work to do, essentially. Right? You know, so so this is an optimization that that we can do, you know, when we have this type of behavior, essentially.

Right? You know, with matrix multiply, it's a little more complicated, right, because we, you know, we're dependent on some columns. You know, the row one's easy to handle, but the column one, it's not necessarily contiguous. It's a little harder to break up, but it's still possible, right, with with some index and trace. But in this case here, right, I only need the data that I need to compute on.

I don't need to wait for the rest of the data before I start computing. Right? So we could essentially break it apart. Right? So so, you know, I could, you know, transfer let's say this is one one one two two two.

Right? Ideally, you know, I could transfer a of one, b of one, and then I do my kernel computation for one, and then I transfer c of one, and essentially overlap, the one. Right? So so so as I'm doing my data transfers and computation, you know, like, a of two and b of two, you know, put put a pin right here. So you have some overlapping with the kernel computation, and then k of two and then c of two.

Right? So so, essentially, you're gonna software pipeline this more or less. Right? The logo here is essentially you take advantage of of all of the hardware resources that we have and and, overlap. So this is a feature.

Basically, it makes use of this feature kinda like device overlap. In the early days of GPU, not not all GPU support this, like, all modern Linux do now. So this is just for completeness. And maybe some, like, older embedded boards might not support this too. So so you could get your device properties, using this call, like, get device count.

You get a GPU number for every GPU number. You can't get device properties, and and each of them has some some, properties like unifying memory or device overlap, or tensor core or ray tracing core or whatever like that. So you could you could check for what hardware feature your your GPU has. Right? So So so I'm on a GPU student, so, technically, you can kinda skip this.

Nowadays, it's it's assumed that you don't have a twelve or fifteen year old GPU. Right? I assume you don't. Yeah. So so this is essentially the the the timing diagram of of the the full overlap that we want to see.

Right? So if you kinda look at it, you know, during one of my fully pipeline stages. Right? There's a device to host transfer. There's host to device transfers.

So I'm making use of both directions on my PCIe, and then I have my, you know, kernel computation as well. Right? So it's all of my hardware's been being fully utilized. Hypothetically, if my vector add is not taking up all of my GPUs, streams will allow me to also do computation from other kernels too. Right?

Maybe someone else's work load, hypothetically. That would be different features, NPS or MEG or whatever like that. But but, you know, there there's other ways to do more concurrency. You can add more parallelism to your parallel kernels, essentially. But for now, right, the whole goal is how do we achieve this this type of pipelining?

Right? So this is where the notion of a CUDA stream come in. The way they they conceptually think of a stream is essentially a queue of operations. Right? Every time we we call it CUDA API, we're sending a command to the to the driver.

Right? And then the driver would inform the hardware, you know, what to do. This CUDA malloc allocates space on the GPU side if it's a kernel call. It would launch some computation on the GPU side. The my synchronized would basically lock on this queue until everything before it finishes and so on like that.

Right? But, essentially, every every CUDA API, more or less, is an entry into this queue or a stream. AMD terminology, they might call it a a queue. Nowadays, they call it stream. In the past, it was also called a HSA queue.

Right? But a a queue or a stream, I think queue is also an OpenCL terminology. Right. Yeah. So so a stream and a queue is basically interchangeable.

Right? It's just NVIDIA calls it a stream. Right? So this stream is essentially just a FIFO queue of commands. And, typically, you know, the this this FIFO queue can only execute one operations at a time.

Right? So so if you think of it, you know, we will have, you know, CUDA mem copy, kernel launch. You wait until the code is done, and then you do another mem copy of the result back, essentially. Right? So all of that would would sit sit in that queue in order.

The one issue with these streams is that, you know, it's fivefold. We can't reorder this. So if anything blocks, we have to wait until the rest of it is done. Right? I don't know if this is covered, but, you know, just for completeness.

Mem copy is typically a blocking kernel. So that means, you know, let's say my stream has all of these. My mem copy won't pop from this queue until it completes. Right? This kernel launches asynchronous.

So the moment it hits the the head of the queue, it's gonna launch the kernel, and then it's just gonna pop out already right away. Right? So so, you know, we need a device synchronized, which blocks to make sure that the hardware is done, which informs this device synchronized, plan, in order for it to pop. Right? And then I could do my next, mem copy, essentially.

If I don't have if I don't have this device synchronized, right, because my kernel launch is asynchronous, I can run my CUDA mem copy right away, and I get incorrect results, essentially, because my kernel hasn't completed. Right? So so that's just for completeness why we have that device synchronized. Right? Because because everything is in this command queue.

Right? So with streams now, right, if you wanna do something in parallel, we're gonna have to create multiple command queues or streams. So this way, we allow the GPU to do concurrent operations at once. Right? So if you kinda look back at this, you know, this this timing diagram.

Right? At any one point in time, we're doing three operations, essentially. Right? There's a there's a device that hosts data movement. There's a kernel launch, and then there's another host device data movement.

So in theory, if I wanna implement this, how many queues would I need at minimum to support this? Right. Right. Yeah. Right?

So is there any stream to only do one thing at once because it's kind of blocking at the head essentially? If I wanna do three concurrent things, I would have to pick three streams. Right. So so, you know, look at this example of two streams. Right?

You know, we could populate essentially our vector add into different chunks, right, across two different streams. So, you know, a, b, vector add, copy c back, you know, of chunk zero and chunk one. In your head, this is kinda the mental model that you would have of populating creating two streams and populating it. So so let's see how we could we could do that. Right?

From a coding point of view, you would try to create the streams, and there's there's APIs to do that. So you have a a stream handler, and then you do CUDA stream create stream zero and stream one. And then that essentially creates your two streams. So we would then have to populate those streams. Right?

And we had to tell it tell your CUDA API essentially which stream to go into. Right? So the way that's done, is using, you know, slightly extended versions of the API that we know of. So so CUDA mem copy now before, historically, CUDA mem copy was blocking. That would mean when we would basically just, like, if I got a CUDA map copy here, I won't go to the rest of it until that CUDA map copy is stopped.

Right? Basically, I only ever have one thing in that stream, and and that's not what I want. I wanna essentially populate that stream and then let the hardware start executing it. So we have a a a synchronous version of of mail copy. So could the mail copy as sync?

So this way this way, when we just push the command into the stream, we'll return back to the host so I could go to my next instruction and, you know, to just keep populating that that queue of commands. So CUDA memcpy of sync is is your regular memcpy of sync, with an extra parameter that that tells you which stream it gets populated on. And then with your kernel code, we have some parameters here at the end, besides, you know, your your number of ref locks and ref locks size. Right? So the zero is actually your shared memory size.

We we haven't touched dynamic allocation of shared memory yet in in your matrix multiply and reduction. The the size of shared memory is is fixed. Right? We know how much shared memory we need to use. In your four assignment for histogram, the amount of shared memory you need is a bit dynamic based on number of bins that you have for your histogram program.

So so we'll see how we could make use of this in assignment four. Right? But here's zero because we don't use shared memory for vector add. And then the parameter here is just the the stream that gets populated into. Okay?

So so this is eventually, you know, wrapped in a four loop. Right, because we have a whole bunch of different segments that we're doing our vector across. So for every single, you know, two segments, you know, we have a a and a b that were populated. Right? So so transfer of a b kernel operation c for every single stream.

So, you know, in in theory, you're populating it this way. But when you're handling CUDA streams to order in which you populate, it actually has a big impact on on your hardware performance and and time of meeting. Right? So so, virtually, in your mind, you have different streams, but, physically, we're actually just pushing it into the physical queues of the different devices. Right?

So we see here that that we're doing no. Sorry. Wrong way. A b kernel c of upstream zero and then a b kernel c of stream one, dot continuous. So what's end up happening or how that looks like after you populate it is we have, you know, a b kernel c a b kernel c.

Right? So so it has queued up in this way right here. So, you know, the we're gonna go through this and and see that. It's a little bit unoptimal, but, this is actually you know, you're using streams. Right?

But is this the the the hype up operation that we expect to have? So, essentially, your copy engine and your kernel engine, you know, this is the the hardware that's gonna look at its task queue, essentially, to see what task it has to do. So you can think of it like that. Every cycle, a copy engine will see at the top of the top of the queue to have anything to process, essentially. Right?

So we could kinda run through this and see what that timing diagram would like to see if this is the the expected behavior that we want. So, you know, in the cycle, if I'm on the GPU side, my SM that's doing the kernel engine, that does the actual computation. Right? Do I have any tasks that are ready to execute, you know, at time equals zero? So I have kernel zero.

Right? But there's a dependency right there. That is dependent on the mem copy of a and b. Right? That that dependency was kinda enforced when you create that CUDA stream, essentially.

Right? So the CUDA stream is kinda like your your data dependency between different CUDA APIs, if you wanna think of it that way. So so these arrows are are enforced by that stream operation. So no. There's there's nothing I could do.

Right? I'm I'm dependent on a and b. So if you look on the copy engine side of the DMA engine, does the g DMA engine have any work that it could do at this time? Yeah. What what can I do?

Meb copy of I can do the mem copy of a. Right? That mem copy a has no dependencies. So, you know, if I'm going from, like, time of zero. Right?

The operation I could do is mem copy of a of a of zero. Right? I could do the mem copy of a of zero. So, you know, this would essentially kinda pop it, and then I see the next thing. You know, my my public queue kinda shifts down.

Right? So so the next cycle, does my kernel engine have any work to do yet? No. Not not yet. Right?

I'm I'm still waiting on b. I still have another, unsatisfied dependency. Right. So let let's say a finishes, and then does my copy engine have anything to do at this point in time? I can.

Right? Yes. Okay. Right. So now I could do I could copy b of zero.

That's done. And now my head of the stack, right, essentially points to there. Okay. So now my copy engine goes to the next echo. Can my copy engine do copy of c at this point?

No. Right? And, like, kernel engine finally do something? Yes. I finally have what to do.

Yeah. Why why does it have, like, two streams, one for device nodes, one for device nodes? Is there, like, just some kind of operation? You could kinda view it that way, I guess. Oh.

Right. Yeah. It it is possible to actually, like, pop two, especially if they use two resources. Oh. Yeah.

In the in the hardware, I think they just implement it in this way because the DMA is one unit. So there's one hardware queue for it. So then in the software, it it gets reflected that way. I don't know. The the kudos source code is is closed source at least.

On the AMD side, that's how I know how it works. That that that part's been open source, so I've been hacked. Right. So so now I could do my kernel operation. Right?

I could do my kernel of zero, and then this gets popped here. And then, you know, I can't do c of zero in that, essentially. Right? Okay. So now in my next time step, what can I do?

Can I do kernel one? Not yet. No. Not yet. And then finally do c of zero.

I can do my my copy c of zero. Right? Because that got satisfied. Right? So so this is the point now where, the copy engine can actually do, like, multiple things.

As long as there's three resources. Right? I could I could pop this multiple times. I mean, it's probably, like, the next cycle, but that's, like, a couple nanoseconds later, essentially. Right?

So, you know, I could do c of zero. I popped the next stack. Can I do a f one at this point in time? Alright. Yeah.

So a f one will probably start right here. May maybe it's offset by, like, a couple of nanoseconds. You just can't see it. But, you know, the now from now point would be completely overlapping. Right.

It's not gonna do a f one. Right? So after a f one, then what will happen? What what would the next thing that I could process look at? B yeah.

B. Right? Right. So I would I would copy, then I would do kernel one, and then wait. Final c.

Right? You know? That that that's that's that thing was left over in in a data dependent order. Right? So, you know, this that would be b f one, a f one, and then c f one.

I didn't draw it in the next place. Right? Okay. Do I have overlap with this computation with the way I'm doing with streams? Do I have some overlap?

I think. Yay. I passed. Right? Yeah.

So so there's some overlap here of, host to device and device to host. Was this the type of overlap I wanted? I expected? No. Right?

So what was the main issue here? Why why was I not able to get the overlap that I wanted? I'm going to. Is it what? Right?

There was something that was blocking. Right? So so there was an operation here that was blocking other ready operations, essentially. Right? On the CPU, if these were instructions, I could do an instruction reordering in the hardware to avoid that that head of the line blocking.

Right? But here, these keys are five for order. The hardware or the or the runtime can't reorder it by itself, so it's gonna be up to you as a programmer to reorder these instructions to get the idea of overlap. Right? So so what would you reorder here in order to, avoid a an operation that's blocking other ready operations?

What would you reorder? It's possible to, like, take mem copy c and, like, mem copy c zero and move it all. Move it to the, now. Right? Yeah.

Yeah. Right? So, yeah, the the big culprit here was was mem copy c. Right? You know, at this point in time, you know, everything was blocked by mem copy c, and everything after it, you know, even though a and b was ready, a and b was ready since the very beginning.

Right? Memcpy c was essentially blocking everything, so it couldn't do anything. Maybe move it to the bottom, but it shifted one down. If I move memcpy just one down. Yeah.

Let's just swap this. Assuming things take unit time, but if they don't take unit time, then Yeah. Okay. So if you do that, I could do a b kernel c. Wait.

C wouldn't be doing anything yet. Oh, I guess c and a would still be But wouldn't a Oh, a a would start here. Yeah. Right. And then c would be here.

And I guess you could do b and, well, okay. The the I guess it depends how long a and b takes. Yeah. I mean, that's not very clean. I mean, both of these are ready.

The brute force would just be moved moved this completely past the auto ready instructions. Right? That's what a hardware instruction scheduler would do on your CPU. Right? Basically, we order instructions until all of the ready instructions are at the front.

Yeah. So so, you know, ideally, I would I would swap this somewhere here. Right? So all of my ready instructions and my not ready c would would swap essentially. Right?

So so these two would then swap here essentially. Okay? Then at that point, I should avoid all of the the blocking that that causes, you know, any issues in theory. Right? So, yeah, that that's essentially what we have to do.

Right? The the ordering in which you populate these queues essentially has a big impact on the effectiveness of your of your streams. So, you know, like like we derived before, this is what it would look like, using the previous ordering. So now we want to essentially move all of your c's to the bottom, right, and all of your a's and b's together. The way to think of this is to essentially populate these streams, not, you know, with all of your streams zero and all your stream once you get it, but group them based on operations.

Right? So you essentially wanna group all of your host to device operations, all of your kernel operations, and all of your device to host operations together, you know, for the different streams. So then this way, more or less, you kind of avoid that blocking because because all of these instructions up here will be ready. And then this data dependency would be on here, and this data dependency would be on here. Right?

You kinda view it like data dependencies between blocks of operations. Right? You can think of it that way. So if you if you populate it this way, you should get the expected results. Right.

So now we have this this reordered one. Right? So so like we saw before, right, you know, we have up hills. I would do a and b. Right?

Went to here, and then I do my kernel computations. K. Right? This is kinda where we got stuck last time. So after my my k, you know, is executed.

At the same time, is there any other write ready operation on my copy engine? Right. Yeah. We were stuck before, but now we have a. Right?

So I could do my a here. Right? And then we could also do my b right afterwards. And then when my b is done, then I could do my c, essentially. Which is that since b and c are different variables, they can execute.

Yeah. They they get executed at the same time, technically. Yeah. Right? It's like, you know, a couple of nanoseconds apart, but these operations take a couple milliseconds.

So so in theory, right, they they they almost completely overlapping. So then, you know, we get this overlap between k and a, and then the rest of the the blue would would continue executing. Right? So so this is kinda getting there to the point of, the the overlap that I'm that I'm expecting. Right?

So so, you know, assuming that the current computation was longer than a and b in this case. Right? They have that overlap. Almost complete overlap between your data transfers of of a and b. Right?

So so now, you know, if you look at this, compared to, you know, not not a stream's version, right, where I have to copy all of my a and copy all of my b before doing my my computation in. Right? Instead of having this, like, really long period in the beginning where I'm doing nothing but data copy. Right? My my kernel is essentially starting here right now.

So so so all of this time here is essentially that that performance optimization now. Right? You no longer wait that that time that this leaks. And then my c is also at the end is also shorter as well. Right?

Because because that c gets mass at the very end. I'm only gonna have, like, this very small section of c for that for that last batch. So so you save some time with with the a and b because they're masking my computation. And, also, at c, I'm doing less transfer. I'm just doing the transfer the last time.

Right? So so the strength is where where our, performance optimizations will will come into place. It'll be a lot faster because of that. Right? But, you know, this is still not not fully overlapped.

Right? So if you wanna fully overlap this, you know, how will we get that full overlap that that we want? How do I essentially shift this over a bit more? Right. Right.

So so the if I want full overlap, how many concurrent operations would I need to do? Oh, three. Three. Right? So I had to add an extra stream.

Right? And if you add an extra stream and populate it the same way, I think you'll get that full overlap. Right? And we don't have to change the Change it. Right?

It'll just be like imagine just another stream here. Stream two. Right? And then I think I think we need to get that full overlap. I don't know.

You could try it. That used to be an assignment. We used to have this. Back then we used a profiler on it. And then for a while, you guys didn't have access to their profile.

I've got some permission issues, so I think they did something else. But, yeah, you know, this is another optimization, a pretty low hanging fruit optimization, you can do for a lot of your final projects as well. Right? So if you have yeah. I know, like, some folks are doing, like, a image processing pipeline.

Right? You can literally have, you know, n numbers of streams equal to the number of pipeline stages that you have and, like, fully do a software pipeline version of it. Right? That you know, that that's one optimization that you could do that, especially if you have a lot of images flowing through, essentially. Yeah.

Any questions with how the streams work? Yes. So there's gonna be a stream for every, element of or, like, every thread block, basically. Oh, here? Oh, like okay.

Oh, that's a good question. Like, how would I partition my problem across the different streams here essentially. Right? Probably not, like, for a. That's too fine grain.

If you look at this code, they they have something called segment size. So the way they they broke this up is that, you know, you still have, like, a threat block responsible for certain sections of your vector app. Right? That's your block size. But a a segment size, they define as, okay.

You know how you size the number of drift blocks that you have for the problem? Right? Now you size the number of drift blocks that you have for a segment. Basically, one of these partitions in the pipeline. So so let's say, you know, I'm only gonna partition, you know, it's not drawn to scale.

Right? Right. I could kinda partition it where every four thread blocks, you know, ten twenty four, like, four k four k elements is is one of the pipeline stages, essentially. Right? So so a segment size in this case would be, you know, four k hypothetically, you know, assuming each each of these steps are less than 24.

So so each of these stages would be will handle four k. That's that's how you would partition it. Yeah. Would you, like, set that based on the hardware DMA limitation? Or Oh, okay.

How how large will my segment size be, essentially? Yeah. Because, like, if it's bigger than the DMA, then you don't really Yeah. Yeah. Right?

Because they still end up waiting too long for it. Right? And if it if it's too short, there's probably too much too many, overhead of of all of these API calls and everything else like that in synchronization. So, okay, how you would properly assess this is kinda, like, a little bit tedious. So so looking at this, ideally, the the most benefit you would have is, you know, all of all of your a and b is completely overlap with your computation.

Right? In reality, because it's vector add, your vector add is such a simple piece of code. It finishes so fast. This This is not trying to scout. Like, for example, your computation can only be like this, hypothetically.

So that doesn't actually look very good. Right? You can make it smaller so your data transfer is smaller. But at the same time, you're gonna make your your computation also smaller. Right?

So just the ratio of the data versus compute here, it doesn't actually really help too much. Right? The the main benefit comes with workloads where you have a lot more compute than data transfers. Right? So so if you have a lot of compute and your data transfer is a lot smaller, ideally, you you size it in such a way where there's there's complete overlap, but not too much because now there's some idleness here on your data transfer.

Right? So you kinda wanna balance these two. It the relationship is not always linear. Right? Yeah.

It's not linear because there's there's data transfer overheads and everything else like that. It's it's also, like, Amdahl's law. You could think of it that way too. So, you know, you could kinda reduce this, and then, eventually, there there may be a point in time where it crosses where you have more data transfer overheads than the actual compute just because DMA transfer is slow. It's it's not it's a nonlinear relationship.

How do you build on finding data something profile it? Yeah. You gotta you gotta kinda profile it. And then, like, depending on the hardware and your interconnect, the autonomy looks different too as well. So I should get But but you if you're if you're, like, hand tuning this on, like, a supercomputer, your hardware doesn't change.

Right? Or, like, on a you know? These these software pipeline and concept applies also in, like, TPUs and other accelerators as well, that type of thing. Right? You can do this overlap.

Well, you know, there's a lot of fine tuning and hand tuning. There there's research projects that has one time systems that could try to auto tune these parameters. Oh, that's cool. Yeah. So they would auto tune parameters like this.

They could auto tune, like, compiler optimizations and hardware configurations and the block size and all of these at the same time. Yeah. There's a lot of those research going on. It's very fun. Right?

Yeah. But yeah. Any other questions? Basically, the whole the the takeaway here is I wanna sign this so that I can completely overlap my my data transferable computation more or less. And then, you know, use your profiler and iterate.

It's it's really hand tuning. It's like when you do machine learning training. Right? You don't know what hyperparameter to pick, so then you just keep putting random stuff and go like, oh, wait. It's faster.

Oh, it's slower. I don't know what's happening. Yeah. It's it's it's essentially the same thing. Right?

You just have to put type of hyperparameters here in in your Twitter program. Like, number of their blocks and stuff is also hyperparameter if you wanna think of it that way too. Alright. Any other questions? Okay.

Does this make sense? Do you feel like you could apply it to your final projects? No? Yeah. Yeah.

What's the difference? This is also, you know, a very it's a very basic optimization nowadays. Right? Almost every GPU optimized program nowadays makes use of streams, and and unified memory. So so streams and unified memory are the two, modern features that we talked about that's that's almost basically commonplace nowadays.

There were other stuff that I mentioned, a little bit like, you know, CUDA graphs and whatnot like that. Those are a bit more, like, niche depending on the use cases, but these aren't very common. Okay. Right. So so if you wanna get, like, more overlapping, right, you may need at least three buffers.

I think when I had this assignment a couple years back, someone tried, like, four or five or six buffers. It it was just extra code. It didn't actually impact anything. So you need at least three, essentially. Okay.

So so in terms of synchronization, it's possible that, you know, you wanna do some synchronizations within your stream or across streams. So CUDA has APIs for that as well. So, you know, CUDA device synchronize essentially synchronizes your whole GPU. You know, up until now, you only had a single stream, so device synchronize would synchronize that. Right?

If you wanna do more five grain synchronization only within your your stream itself, this could have stream synchronized, so you only synchronize all the you you block only within that stream. We don't have it in the slides, but since then, now we have features where if you wanna synchronize, like, across streams like, for example, if if you have a running on this stream and b is running on this stream and you have data dependencies across streams, there's something called a CUDA event, synchronize or event wait or something like that. Right? There's there's that event waiting mechanism in place where where you could, essentially inject task dependent, data dependent tasks essentially in in those streams. It it gets really complicated.

That's why through the graph, APIs were created, so then you could express it just as as a graph, essentially. Right? So, yeah, that that's kinda like the the programming evolution to try to just make your life easier as a programmer. Alright. Any other questions?

These are just the wait for kernels to complete. These are wait for kernels or mem copies or other stuff. But, yeah, typically for the kernels. Yeah. But don't mem copies block the kernel stream?

No. We no. No. We have to use stream, mem copy as synchronous. I thought asynchronous is asynchronous for the GPU name, but synchronous within stream.

Oh, oh, oh, oh, in a stream? Yeah. Okay. Yeah. Yeah.

Yeah. In that sense, yeah. Within a stream actually, within a stream, everything is kinda blocking. Yeah. You can only do one thing at once.

Yeah. So so the mem copies would would block for the one? Are they waiting for a kernel? It would also okay. Going back to the example.

Okay. So So we need b. Right? Yeah. Yeah.

Yeah. Something. So so this this would all of these would block in the in the stream. Right? The reason why we still need this device synchronized is that, on the host side, we can call the device synchronized.

We need to we need to block the host, essentially. So so when we when we in when we call these, assuming it's mem copy asynchronous. Right? They don't block the host, but they will block the stream. We need the host to block, so that's why this device synchronizes in here.

Oh, okay. Yeah. So so the device synchronizes is in the stream. And when it reaches the head of it, it would unblock the host thread. Unblock?

Unblock the host. Yeah. Because the host is blocking until we re receive that device synchronize. It's done. Host being the CPU or the Yeah.

The host is in CPU. Yeah. Not sure I get it. Why does this okay. This example isn't really why would the host be waiting for why would the host be blocking until it hits the device?

Yeah. This isn't because this is mem copy, not device sync. If this is a real streams and this is a synchronous, I think your device in sync should be right here, actually. Right? Because you wanna make sure all of your back to add the slides.

Before the CPU uses the data. Oh, okay. Yeah. Yeah. Yeah.

Yeah. Right. We wanna make sure everything is done before the CPU uses it. So so we have the device synchronized to notify the host. Right.

Doing this manually is a pain. Right? That's That's why you have CUDA graphs as well. You could you could make a a node for CPU computation that's dependent on GPU computation. And you have a lot of coordination between CPU and GPU stuff.

I would suggest you use the CUDA graph API. Okay. Any other questions? Alright. Wait.

I think I'm done. Hold up. Oh, yeah. That's it. Okay.

So so that's it for CUDA streams. So this is another optimization like I mentioned. That's very common. So, hopefully, if you do your final project, you could find a way to include this optimization as well. And yeah.

So we'll see you on Thursday. Yeah.


Alright. So off, some announcements. I I made an announcement right before class on on Discord. So there's no class, no lecture this Thursday or next Thursday. Yay.

Right? No. Okay. So, hopefully, you know, you'd use that time to work in your final project. So so instead of having lectures, what I've done in the past and this year as well, is that I'll just, you know, use that time to basically make it an extended office hours.

So you can you and your team or yourself, if you're working by yourself, that could reserve a fifteen minute slot and just meet and discuss anything about your your projects. So, you know, the whole point of this class, you know, for the lectures and the assignments is to give you just the foundations of GPUs. But there's, you know, a lot of other ways of using GPUs, and other challenges when you're actually applying it, you know, towards a larger project. Right? A lot of things we're doing now are really just two examples to to let you understand what's happening.

So then, you know, the goal of the final project is for you to actually get your hands dirty and, you know, build a project from scratch using GPUs. And because of that, you might have different challenges depending on the type of project you're working on and whatnot. So, in the past, you know, a lot of people would just comment on talking about random things like, why isn't this environment working, or how would I map this algorithm? You know? Just very general ideas of how, you know, depending on what your project is.

So so if you click on that link, right, there's a there's a calendar reservation system where you could select, you know, Thursdays and some time slots. I think I have, like, four hours available on Thursdays. And then, also, you can also sign up to meet with the TA. So the the Friday and Monday time slots are are with the TA as well. Right?

So if you have questions and you don't wanna meet with me, you could meet with her. So, you know, there's there's lots of opportunities for you to pick out brains and hopefully get you started on your final project. Really, I just don't want you to start on the weekend before it's due. Right? Because then it'll be a rush work, and some people, it actually happens.

The day before it's due, they go like, hey. This library is not on vendor. I'm like, I don't own that server. I can't help you. Right?

So if you study a project earlier, you would run into these issues where you would know what dependencies are missing and and whatnot, and then we can help you a couple weeks before. So so, really, that that's stats. Okay. And then just other things. Assignment four is due next week.

That's histogram. You know, just some pseudo code. Most of it's already there. Hopefully, it's a pretty simple assignment. And then your final exam is the last three days of week ten.

Right? So that's that's two two weeks away or so. Right? And then you can start reserving it next Friday. So, yeah, time flies.

And then you graduate. Most of you, hopefully. Right? Yes. Any questions with logistics or anything?

No? We're good? Okay. Alright. So today's topic, we're gonna talk about multi GPU programming.

So a lot of the things we talked about up until now is how do you program a single GPU. So multi GPU programming is kinda one of these things that you normally do nowadays because everything is just at such a large scale. Right? A lot of workloads no longer fit on a single GPU. Like, a lot of machine learning models and scientific workloads, even database programs and everything else like that.

A lot of those are multi GPU nowadays. So we're gonna see how you can program multi multi GPUs and what are the challenges of doing that, specifically with communication. So, hopefully, this kinda just gives you a background on on that. I think some of you may have proposed a multi GPU assignment for your final project because Bender has four GPUs. Right now, by default, everyone's using GPU zero.

So if you wanna, you know, parallelize your algorithm across multiple GPUs, this will kinda give you some background in in how to do the communication between between these workloads. Alright. So so this is, again, borrowing slides from, you can see. So this is from 2012, like, in the early days of CUDA. But the API standard really changed.

But it it does show some of the nice, systems challenges that they had that they had to be aware of. Right? So so everything that does from here is still valid. Right? That's that's completely changed.

But there's a lot of similar foundations to, programming multiple GPUs. Right? So we look at, you know, the the challenges of multi GPU programming and whatnot. Right? So like we mentioned, the reason why we wanna do multi GPU programming is because we you know, things don't fit anymore in in a single GPU, so we have to scale it out.

And you could scale it out really just, like, two scenarios where you could scale it out. Right? You can have multiple GPUs within a single server, like we have on vendor, or you can have multiple GPUs across multiple servers, such as, like, a supercomputer or a data center or something like that. For those of you who took CS one sixty, the parallel programming course, did you learn MPI? You did.

Right? Yeah. So that's, like, the common way of communicating between multiple server nodes. So a lot of workloads that make use of GPUs and multiple server nodes, like, for example, in high performance computing, will be like a combination of CUDA and MPI programs. Right?

And nowadays, there's ways to kinda go around MPI as well. So we'll see what that is. So depending on how your GPUs are connected, right, whether they're on the same server or across different server nodes, Your GPUs will communicate in in various different ways, right, with different paths. Right? So if you have if you work across multiple server nodes, you would use something called message passing.

That's what MPI is, message passing interface. Right? Even that in CS one sixty. And if it's, like, on vendor where you have multiple GPUs in the same server, they typically communicate either through your CPU host memory, like, in in most consumer brain parts, or the more fancier way of communicating between GPUs is called peer to peer communication. So your GPUs could communicate directly without the involvement of CPU.

So this is the early days of GPUs. So a lot of this still depended on PCIe. So later on towards the end of this slide set, we'll we'll see how the modern GPUs handle, peer to peer communication and and speed that up. But, you know, just just, you know, some brief background. Right?

So for GPUs, right, because we want multiple GPUs to do computation, to get back to streams. Right? If I wanna do multiple things at the same time, I need to put the commands in a different stream so we can run it concurrently. So so with, you know, strange. Right?

The traditional thing that we used to do was to essentially overlap kernel and memory, that allows us to do different things. Right? We we learned this before. But we will also need to overlap these as well, kernel computations. Right?

So your kernel computations will be in different streams, but, specifically, the way you would send a command to a different GPU, we would have this API called, like, CUDA set device, essentially. Right? So so CUDA set device will specifically tell you which device it goes to. So, basically, anytime you call CUDA set device zero or one, any APIs after it will be sent to that GPU. Right?

So it's kinda like the concept of streams, but, a lot more rudimentary. Right? And this is still used today. So if you have a lot of different GPUs, typically, you wouldn't write it like this. You would write, like, a for loop, like, for I zero to eight.

Like, if you have eight GPUs, could have set device I, and then you would run a kernel and a mem copy. That that's how you would do, multiple GPUs. And then when you do communication, this is still valid because, you know, your your consumer cards have every different access patterns than than the sender cards. You gotta see whether you could do a peer to peer access. Right?

So if your hardware supports peer to peer access, the way I okay. I'm gonna go to salvage. Okay. So you have you have this here. Where does this pen go in?

You have your CPU and then you have your GPU. You have two GPUs. And we're connected over p set e. Right? If you have if you have, like, eight GPUs on a PCIe, PCIe is is not a bus, but it's actually a tree.

It's a tree network. So all of these are connected to, like, a PCIe node, and then it it's connected to here. Right? So if you don't have peer to peer access, the way your GPUs communicate is that it goes through CPU memory and then over to your to your GPU. It's very slow.

Right? Because every time I wanna copy from from GPU to GPU, I copy from GPU to CPU then CPU to GPU. Newer GPUs, right, that at this time, for me, was the one that introduced pure access. And, really, we we relied on PCIE to have this feature. Peer to peer access enable us to communicate just directly through that PCIE tree.

Right? So the the PCIE supports directly communicating from one to another. Another big issue here is that let's say you have, multiple CPUs. Right? Can you guys buy motherboards with two CPU sockets as a consumer?

No? Probably. Can you? I mean, if you're rich. You you can buy anything with enough money.

Mhmm. That's true. Okay. Right. This is very common in servers.

Right? So so in servers, it's very common to have two CPU sockets. What that means is that if you have multiple GPUs, they'll be under different PCIe root complexes under the the the the the CPUs. Right? So so some of the GPUs are connected directly to PCIe.

Some of them are not directly connected to PCIe because they would have to traverse through the CPUs. Then they call this, like, QPI or UPI depending on the standard of how Intel and AMD communicates their their CPUs directly to each other. Right? So those are gonna be peer to peer access. You know?

The reason why we need to notice is that depending on whether your your GPUs are connected directly together or not, there's different CUDA mem copy costs that you could use in order to optimize the communication. So so as you can see, the topology of how your hardware is set up is becoming kind of important. Right? This is kinda like server architecture now. Right?

The architecture of how your server is aligned. If they are directly connected and you do have peer to peer communication, there's another CUDA mem copy on it this year. So it's called CUDA mem copy pure sync or we also have one without a sync, that allows you to to essentially bypass the CPU and just communicate through PCI e. So because it's it's weird for multiple GPUs, you know, you still have your source desk and destination, but now you have two different devices to tell you where that source and destination exists. Right?

So in the past, with cuDNN copy, it was a GPU to a CPU. You know, it's implied that always one of them was a CPU with hosting device and device host. Here, you just specify device ID. And then this way, you you know whether it's between two two CPU or two GPUs or a GPU and a CPU depending on whether they have peer to peer access. So so, you know, if you have peer to peer access, it's faster because then you just go through the the shortest piece of ePath.

Right? It's kinda like networking now, essentially. Right? And then we don't have the stage of memory through CPU, so that avoids an extra copy overhead. So this is one way of accelerating memory, sorry, data communication, on GPUs in the early days.

Alright. So now how does this impact the programming aspect of things? Right? So before, there were peer to peer mem copies. You would manually have to actually mem copy from c from GPU to CPU and then CPU to GPU yourself.

So you actually have to manage those buffers yourself, right, which is, you know, pain in the to do. So now you just call that CUDA mem copy pure sync. So to give you a more concrete example of how this will look like if you have a multi GPU program, this this basic example is of a, you know, one d, decomposition. So at a high level, you can just think of it like let's say I have molecular dynamics or something like that. Right?

They just kinda bounce. You know? You have particles bouncing around, and then you kinda simulate it. And then just, like, forces and acting on each other like an end body problem. You you've seen these end body problems before, I assume.

Right? No? Yes. Okay. Just imagine there's a whole bunch of planets, and they have and, you know, they exert gravity on each other.

Right? So there's forces between them. And then as they kinda flow around, you would have to kinda recompute the forces between the planets or the particles or the molecules, whatever like that. Right? So if you wanna partition this problem across multiple GPUs, the traditional way of doing it would just be to, you know, just divide up the space, whether it's in two d or or one d or whatever or three d.

And you just assign one of these regions to one GPU. So you would compute a time step, you know, of where the molecule would move or the particle would move within within one time step, And then you communicate the new forces to the other g your neighboring GPUs. Right? So so, basically, at every single time step afterwards, right, these would have to exchange information about the forces or, like, you know, or in case a particle goes into the next region or something like that. Right?

And between these particles, there's some forces that exist between that, the different GPU regions. Right? So so a lot of these scientific workloads basically have, you know, a compute phase and then a communication phase, and they're typically done iteratively, you know, in time steps. So that's how they do simulations for for a lot of these time. Typically, workflows, like weather prediction and all these other stuff.

Right? So let's see how we could do, you know, this this type of communication, specifically. So, you know, one one typical algorithm that you would use to communicate, they just call it left right. And that's basically everyone send data to their left neighbors, and then everyone sends data to their right neighbors. Right?

And then at the end of that, you kinda receive all the information that you need. So this is actually known as it's pretty optimal on traditional PCIe devices, because they fully utilize your PCIe links. Right? So so if you kinda recall, your PCIe is bidirectional. Right?

Just like when we did our CUDA streams, the lecture, we we basically wanted to, you know, do a transfer in both directions at the same time to to be fully pipelined in order to utilize the hardware. Right? So similarly here, when we do a left right approach, essentially, we're fully utilizing both directions of the PPA link as well. Right? So to just kinda visually see what happens, you know, everyone sends to the right, and they receive from the left neighbors.

And the way that it traverses the PCIe tree would look something like this. Right? So we kinda look at every single link between, one of the PCE switches and the GPU, you know, this one outbound is one inbound connection. This one inbound, one outbound. Right?

We're kind of fully fully utilizing most of these links in in two directions, at least in the middle. Right? So so this is pretty optimal. And then when we go in the other direction, similarly, you know, my my bidirectional bandwidth are both fully utilized. So in this case, you know, this is pretty okay.

However, it gets a bit more complicated when you have, you know, more than HGT. So you just get a larger tree essentially here. So in this scenario here, you would also, be pretty optimal. Right? You don't really see any any links that are contending.

So for example, you don't see, you know, the any GPUs of of two outbound connections. Right? They'll they'll only have one in and one out. Right? Does does that make sense?

It's a pretty simple algorithm. Yeah. Okay. So the way you would implement this is basically just a lot of four loops. Right?

You know, for every GPU, you would do a CUDA mem copy pure sync of your ID plus one, you know, your neighbors to the right. And then the next step, for every GPU from I to minus one, your neighbors to the left, essentially. Right? So the way it will look like, you know, it's just a for loop for every single device, and you go from GPU I to GPU I plus one and GPU I to GPU I minus one in the other direction. Okay?

So in theory, even if you don't have this sync, you know, this would actually be correct. Right? Because you do a right copy and then a left copy, and everything will be great. However, this is actually one of your times where Synchronoss actually leads to better info improvements. But thinking back to our previous assignments.

Right? We we have synchronizations like barriers, in place because we needed to enforce data dependency and correctness. Right? So in this example here, when I'm doing my transfer to my right and my left, do I have any data dependencies with that type of communication? Yeah.

If I communicate to my left and my right, is my communication to the left dependent on the data I receive from my right? Is there any data dependency? No. Right? My my data dependency is on the compute phase before for the the time slots I was computing on, for example.

Right? So so, technically, you don't need the, synchronization here to be correct. However, the issue here is that, you know, if I if I don't have a synchronize, right, it's possible that some of the GPUs that's sending this way finishes So then it's gonna start sending this one when other GPUs are still sending this way. Right? So what ends up happening is that there could actually be conflicts between the the PCA Unix.

Right? Because because now two GPUs may, for example, be trying to send in the same direction on the PCIe link. So there could be contention. So, ideally, if you wanna avoid contention, right, between this left and right stuff, that'd be a synchronized. So this is one of those rare cases where adding a synchronized actually makes your code faster.

Right? Even though it's not necessary for correctness, it avoids contention between PCI e links. Right? In all of our previous assignments, our our synchronize was to enforce correctness. Now here in in this example, synchronization can actually improve performance just just by avoiding contention resource contention.

So, you know, these are just, like, small optimizations that that requires a lot of hardware insights, essentially. So you can remove that synchronized, and I I think they show here that, the panelers that you get goes down from, like, 15 gigabytes bytes per second to 11. So it's a pretty big hit by not having that synchronized. Alright. Any questions?

What's this? Okay. Alright. So so this was before the days of unified memory. Right?

So if we have unified memory and we still have this problem, would this still be necessary if I have unified memory? No? Right? I just have pointers, and every GPU can use other GPU pointers. Right?

I don't know how to do any of this anymore. Mhmm. So so, you know, this was one of the reasons why Unified Memory was created. I think it was created maybe, like, two or three years after these slide sets. Right?

So so, you know, back then, as GPU started scaling out and work with scale out to multi g p GPUs. Right? Manually managing all of these multi GPU communication was a huge pain point for programmers. So that was one of the main motivating reasons for unified memory besides, you know, ease of porting code from CPU to GPU. It also greatly simplifying multi GPU programming.

Right? So if you have unified memory, you no longer have to call this explicitly. You could just access the data directly and, you know, the pages will then manual, will will be managed, and it'll go to each GPU individually. There's still some optimizations if you use unified memory. Like, for example, if, multiple particles are on the same page, but they're on different GPUs, then that page is gonna keep ping ponging back and So sometimes you might have to, you know, remap where the data falls in memory space just to avoid that ping ponging.

But, you know, there's always ways to do optimizations. But in general, unify memory would would greatly simplify this as well. If you if you don't have to unify memory, then then you do this one. So so another thing to consider, for for default GPU programs, It's just really where your data lies and and how your your GPUs are communicated, connected to each other. Right?

So so there's a lot of interconnects that exist on your on your laptops and your desktops that you don't really think about. So PCIe specifically, can cause a lot of these issues. And, also, if you have multiple CPUs, like like in most Windows 10 shavers, may have, essentially a Numa effect. Right? Have you heard of NUMA before?

No? No? Okay. Not okay. You don't hear this in one six one either, Okay.

Yeah. So so NUMA is about non uniform memory access. It's an issue that occurs when you specifically have, you know, two sockets connected to different DRAMs. So, you know, when you build a computer and depending on where you put your d DRAM slots, you tell them when to put, like, pairs of it. You you know what I'm talking about?

Right? When you put it, like, here, it might be wrong, but you put it here, you get that little bandwidth. That type of thing. Right? Okay.

That's because you're you're explaining memory channels. So, essentially, every single CPU has a different memory channel. If you have two sockets, right, some of those go to different memory channels, which connect to different CPUs, essentially. You can kinda think of it that way. You can kinda see that if you build a desktop before.

So, basically, you know, all of your DRAM is one virtual memory is one physical address space to you. However, depending on where it lies, right, if my if my a dot out is running here, right, this this longer path will be slower. Right? This will be slower because that DRAM is physically connected to a different CPU. So as as you go through my memory channel and then through my my CPU interconnect, right, which is QPI or UPI, And they just call it infinity fabric.

Everything is infinity fabric too though. So then that means whenever I access memory, I have not uniform timing. So so that's what NUMA is. So on the CPU side, now you've been thinking of of GPUs. Right?

There's a lot of, you know, optimization techniques to avoid NUMA access. So, for example, you you could try to manually pin memory pages to the same socket as you are. Right? So this way, the pages here are always local memory access. Analytics, if you're running a program, you could pin your a dot out to specific course.

Right? So then you could map your your computation to where your data is as well. And then, you know, it gets a little more complicated because now you have network cards and storage devices, and they're connected to specific CPUs as well. So you have non uniform them network access and SSD access and everything. You know, and that that that's just on the CPU side of things.

Right? So to add on to that, now we introduce your GPUs. So your GPUs are connected to these are essentially your PCIe, switches. So just depending on on where your program is running, and you do mem copy, right, there's different paths that your mem copy could take. Your your whole desktop is session network of devices.

Right? You could if you took computer networks, you could you could think of it that way. And we have an undergrad computer network class. We do. Right?

Yeah. Okay. It's not it's not required, hun. Is it a tech collective? Okay.

So we know all the TCP IT stuff. Okay. Yeah. Right? So so for example, you know, the fastest net copy would be directly to the same, you know, if your GPU and and memory is connected to the same CPU.

Right? So this is considered a local device to host copy. Hypothetically, you know, this would maybe, like, 6.3 gigabytes per second. Now, alternatively, your GPU could be connected to, you know, you know, CPU one socket. So, you know, every single CPU has decent lanes that they're responsible for.

Right? So this one's connected there. So So then this will have to traverse through some kind of interconnect, between your PCET e switch and then go all the way to your DRAM. So this this specific overhead is gonna bring it down by about two gigabytes per So you lose you lose, like, what, 33% of of your bandwidth just because you are you're not aware of where your data transfers are, essentially. Right?

So so just, you know, as a summary. Right? Depending on, you know, whether you traverse here or traverse here or local or remote, and and also in different directions as well. So, you know, going from this way or going from this way, even in the same local direction, local, local mapping. You have differences in, in your data transfer.

I think this is just because of how loads and stores work from the on the GPU side of things. But, anyways, you know, there's a lot of heterogeneity of things here. Right? And then some GPUs could be even more complicated. Yeah, sometimes you got multiple layers of PCIe switches so then you can have a whole bunch of different timing parameters.

And now this is starting to kinda get really hard for you guys to manage, right, as a programmer. If I wanna squeeze up the the a knock on the performance again, it's It's very difficult for you to to micromanage all of this. Right? Luckily, there's libraries now. So there's there's just, you know, the the things you have to worry about.

Again, I like to show these old slides because it it gives a really good idea of what's happening underneath. So in the old days, right, if you wanna, you know, get the best performance, a lot of times, you would have to figure out how your hardware looks like. So there's actually tools if you're curious about how your hardware is all arranged. Right? There's, like, a hardware locality tool.

You could run this. It's from the MPI validation. You could run this, and it'll give you a nice little tree view of how your devices are connected. Right? Or on Linux, have you ever run, like, LS PCI?

Some of you have done that. Right? It it really shows you kind of like a tree of how your PCIe trees devices are connected. It's essentially kind of that, right, but in a in a more compact way. But, anyways, you know, this this this all leads to, you know, a lot of the modern challenges with with modern multi GPU systems.

So these are my slides that that I, I gave a talk at AMD. Check out the rest of that. So the rest of these are are basically my slides. Kinda spilling. Right?

So, just just to kinda give you an idea of the modern challenges now in in multi GPU systems. Right? You know, in in class, you kinda learn what an SM was. Right? This is basically the view of the GPU that that you know in class more or less.

Right? So there's a lot of different execution units. There's tensor cores or tensor parallelism, you know, machine learning workflows and so on. You know, one of those tiles nowadays, you you, you know, you you stick forward them together, and now that's what a modern SM is. So what they realized was as their SMs kinda grew and grew and grew, it became very underutilized because the work scheduler was, a bottleneck.

So now every SM is kinda broken up into four with, like, four data paths in internally. So this this increases utilization. Now we take, you know, 80 or something of these SMs together. You have a modern GPU. Right?

So so the v 100 has 80 SMs. I think some of the modern ones, like, each 100 has, also net five, like, 80 or a 100 or so. But either way, man, you just copy and paste it a lot, and then now you have a modern GPU. Right. And then you could interconnect those together nowadays.

So this is how a modern data center GPU looks like, in supercomputers and cloud systems. This this is more or less the default one. There's more expensive ones that I'll show later on. But but, basically, NVIDIA realized that interconnect is is really a a huge bottleneck. So they created their own PCIe version.

So so they call it NVLink. So, you know, all the GPUs are still connected to traditional PCIe switches. But, internally, all the GPUs are also connected to direct peer to peer links called NVLink. So this is literally the the interconnect topology of it. Right?

To me, it looks kinda like a hamburger. So you have some fully connected nodes and then some sparsely fully sparsely connected nodes as well. So, essentially, this was around the time when machine learning took off, and machine learning has a lot of communication. Right? So, you know, in machine learning, they do a lot of computation, and then they kinda synchronize all of their weights.

And then they kinda iterate on that over and over and over again. Right? So when they synchronize their weights, that part specifically, was a huge bottleneck. So when Emulate was created this is what part of the marketing slide. It's weird.

It's inverse. So instead of just training on a a GPU I mean, on a CPU, which is seven hundred and eleven hours, If you put this over to eight GPUs, it's now eighteen hours. Right? And then if you include NVLink, that gets cut into half. So about, like, seven seven forty four hours.

So so a lot of the overhead in in machine learning training is really just communication. So instead of your traditional path of communicating between GPUs, which, you know, if you don't have PCIe if you have NVLink, they'll go through your PCIe switch over your CPU and then do another PCIe switch. Now it could directly go like this, for example. Right? So, hypothetically, just that improvement, gives gives you, like, almost, like, two x improvement in communication.

So so this was, NVLink and NVLink two. And the system that they demonstrated, that was, like, the d g x one system. It's just so all of that all of that that you see is in a little tiny four u server rack. Right? I think that that server rack itself just costs maybe, like, I wanna say, $50,000 when it came out.

Right? So then you can put a whole bunch of those, and now you have a a supercomputer. Right? So I think NVIDIA sells pods. They call it pods.

And some of the supercomputers in Europe are basically designed like these DGX pods, that that NVIDIA builds out. And in many data centers, like XAI has a whole bunch of these pods for the data centers. Right? And then because of the whole ecosystem around it, there was a huge amount of software ecosystem that kinda built up around it. Right?

So now everything is containerized in in the cloud, and it just works for NVIDIA. AMD is still playing catch up, so a lot of things are broken with AMD stuff because their software just never they never invest in their software. But it's getting there. But yeah. But but, basically, nowadays, right, GPUs are using light in the cloud.

This was an older slide, in the early late twenty tens. And even then, you know, the amount of GPU demand was growing exponentially. And now with AI is, like, even more expedient. That show, I think it's crazy. So so some trends that we see nowadays, right, is that, you know, your your Internet connect is kinda getting more complicated.

So So we saw it before with all those psette trees, and now you have NVLink, and now you have double NVLink. So some some GPUs are faster than other GPUs if they're communicating. So so the libraries end up basically, you know, building these double NV link communication channels and other lower bandwidth channels, and then you're managing the channels altogether. And, really, this is just because we're we're kinda limited in space, the amount of chains that we have on the GPU. So to kinda get around this, they made the next logical leap and basically created a switch that you can connect all of the GPUs together with.

Right? So so we have something called NVSwitch nowadays. So instead of peer to peer, you can kinda think of it like a gigantic crossbar network switch that's kinda in between, all of the all of the GPUs. Right? And, specifically, that connection is all in the backplane of a server.

I don't know. You you might be too young. Do you do you remember when NVIDIA bought Mellanox? No? So Mellanox was a was a network company that created InfiniBand.

Basically, you know, Ethernet is, like, the poor man's version of InfiniBand because that's what you could afford as consumers. In data centers and everything, we use InfiniBand. So Mellanox is the company behind that. And NVIDIA bought them out because they realized, oh, we need communication. We need to feed all of these GPUs.

So one of the outcomes of this is all of these different interconnect technologies, on the server, that thing and stuff like that. Right? So so besides, you know, servers interconnect getting more and more complicated, software is also getting complicated as well. When this server came out, the NVsearch one, I think one rack cost, I think, like, 3 or $400,000. Like, just one rack.

Like, very expensive. That's why people work at NVIDIA are millionaires now. They have huge profit. So so besides the hardware getting more complex, right, your software is also getting more more and more complex. So different workloads have different multi GPU computing patterns.

Right? So a lot of machine learning training, they use ring or tree communication. I will see a little bit in a, we'll see how that's like. A lot of other webhooks could use point to point, like NPI programs. They use point to point or gridlock communication.

There's a neat project out of Lawrence Livermore to call it Illumina. It's basically a a wrapper. You just say communicate between some different devices, and it'll figure out in the back end what to use, whether it's it's a, NPI or, like, CUDA over NPI or or other Mellanox or whatever like that type of thing. And then in data science, the RAPIDS one is a lot of pipelines, so you kinda have a linear pipeline. Or you could have a regular, very irregular memory space.

Right? So it's it's kinda like unified memory across multiple nodes. They call it p gas, or or open shared memory, in the in the high performance computing space. But, you know, your hardware's getting more complex. Your software's getting more complex, so it's kinda hard to kinda reconcile them.

So that's kinda the the challenges that we have nowadays. This is an interesting site because back then, everyone realized that, hey. There's a lot of challenge with this multi GPU programming. So everyone just kinda created their own standard. So so, you know, NVIDIA came out with NVLink.

AMD had Infiniti fabric. Intel was one of the backers of something called compute, CXL. And marketing term, they call it XELINK. So so Intel's data center GPU use XELINK, but it's actually just CXL. And then a lot of other companies made their own.

I think, some of these were IBM and Xilinx and other companies. I forgot what it was. Like, so they call it c six, open cap, or gen z. And the silly thing was everyone made their own standard, but everyone was a member of the other person's standard as well. They were just, like, kinda competing with each other, and they kinda got redone there.

And, eventually, all of these died out. So these are the only ones that's left right now. So unless you're at AMD or NVIDIA, everyone has to use CXS. So CXS is kind of the default for a lot of just general accelerators. Right.

So you see some of these terms is really just custom interconnects. Right. So because of that, data center started making their own servers, custom servers with with these different, GPU server architectures. Right? So in the early days, you know, Summit was a supercomputer.

But then, in Facebook have you heard of the open compute project? No? Okay. So so there's, something in the industry called the open compute project where a lot of these hyperscale data centers like, Facebook and Microsoft and Google, they they make their own standards for how you design high density servers and data centers. Right?

So that that's known as the open compute project. So some of the servers that came out of the open compute projects, like, Facebook call it Zion. It's just a generic server that's fully interconnected with, like, a standard accelerator interface. So you could stick different accelerators in there, like NVIDIA GPUs, AMD GPUs, graph core IPUs, and stuff like that. And they would just use this fully connected.

It's really just p c physically, they're just PCIe five hopper wires, but they run different standards on top of it to get, like, ambulate speed or whatever like that. Right? So then the the servers will automatically reconfigure the the hardware however they want for for their design. So so this was, like, how data centers are responding to just the influx of accelerators nowadays in in interconnect. So this is a, an older one.

I'm sure now they have a lot of different other designs. So so this is kinda what's happening on the on the infrastructure side of things in in data center. Right? Okay. So now how can we make from the programmer point of view, right, make what make all of these more efficient?

So so the main challenge here is that, you know, programming multi GPU programs is is very hard. Right? You're programming multiple threads already, and that's really pretty difficult. Right? Now if you imagine that you had to manage the communication between everyone, it's very hard to do.

So, you know, the main solution there is someone's gonna build a library for it. Right? And luckily NVIDIA did. And then AMD literally took this code and just put an r instead of an n. And and just made it work with their with their phones, with their GPUs.

Right? So so around this time, when all this machine learning started exploding, NVIDIA created a nickel this this library called nickel. The collective communication library, NVIDIA collective communication library. So all of your machine learning code, PyTorch, TensorFlow, whatever, the back end of it, they're all running when you do communications. Yeah.

No. You know, if you're running PyTorch code, you don't have to micromanage data transfers. You just use. The only exception would be, you know, like, DeepSeq in in China. Right?

They kind of made their own library of that's even more efficient. That was kinda, like, some of their secret sauce that they were able to, you know, get their their deep sea models to be super efficient. Right? But at the core level, in that high level, this is how most of the multi GPU communication is done nowadays. So just to give you a background, and this this exists on on your server as well on vendor.

Right? So if you wanna do multi GPU communication or programs, you could use this library. So so, basically, a lot of machine learning workloads make use of something called collective communication. Hold up. There's a nice picture here.

Right? So there's there's a lot of foundational communication patterns that exist between computer programs. If you took one sixty and you and you did MPI programming, it follows a very similar model because MPI also has collective communication besides point to point communication. So you have different patterns like broadcast, gather, gather, ob reduce, all gather, reduce, and all. Right?

Your program reduced before, will be in a single GPU. Right? So if you have multiple GPUs, you could reduce it with just a collective communication pattern. In this case, it's a it's a reduced operation. So so the library essentially implements all of these in a very highly optimized way.

So if you could map your multi GPU workflow communications in one of these patterns, it's just a matter of calling various Nikko API calls. Right? It's a lot more simpler to think about now. You don't have to micromanage the data movement. You just have to think of your program's communication as certain patterns of of communication, right, and map it to this API.

Okay. Just to kinda give you an idea of what these look like. Right? Broadcast. You know, if GPU a zero has a, you wanna broadcast it, that means everyone gets a copy of a.

Right? This is the most basic operation. Scatter is if if GPU zero has a whole bunch of data, I wanna partition it and give everyone a different piece. Right? So you could kinda think of it like, if I have vector add and I want to split it across multiple GPUs, right, that would be a scatter operation.

Right? So I'll partition up the data that I have across multiple GPUs, and then every GPU will work on that piece of data. Right? So you guys are kinda doing a scatter operation already, kinda between different type blocks. If you want to think of it.

Right? Gather is just the opposite of it. Right? So everyone has a piece of information, and you're gonna collect it back into one one one piece. And then all gather, You collect it back in the web page, but then everyone gets a copy of that.

So it's kinda like a a bit of a gather plus a broadcast combined together. So this this is, this is used, I think, in in some machine learning workflow phases as well. So reduce reduce is specifically are reduce are are are used the most in machine learning training. Basically, if you wanna think of it, when we do training the way we do training and we split it across multiple GPU is that every GPU will have a copy of the model, and they would train on different subsets of data. Right?

And then every once in a while, we would sync synchronize all of the weights, maybe, like, average out the weights of all the models, and then we repeat that process. Right? So so our reduce essentially you know, if if you have weights a, b, c, and d, you know, respectively, we would reduce it, average it, add it, whatever. And then everyone gets a copy again. So now all of their weights are synchronized.

And then we and then we do the next iteration. Right? So, specifically, all reduce is is, like like, at least 60 or 70% of all the communication patterns that exist in machine learning training. Okay. So there's a lot of research and and optimizations that goes on specifically for this this collective.

I I think at one point, like, every single major, cloud company had the old version of Nickel. Like, Uber had one. I think it was called, like, Gloop or Gloop or something like that. Oh, no. Gloop.

Microsoft had one called, like, NCCL. Baidu had one. Like, literally, every every company had their own implementation of on reduce because everyone's just just trying to optimize this. And then, you know, reduce scatter, all the all, and and so on. Right?

So these these are a bit different patterns that that's not used as much. So just to give you an idea of how all of these are implemented. There's there's a lot of challenge in how we could, implement these collectives. Right? So, specifically, when we wanna do these communications, we in an optimized way, you kinda have to be aware of whether your size is a a large or small transfers, because small transfers, you know, latency is gonna be the issue.

If there's large transfers, you want your bandwidth to be optimized. Right? So depending on your algorithm that you implement underneath, you could design one that's better for for bandwidth or one for latency. And, also, you gotta be aware of your topology, because depending on how many of NVSwitch or NVLink in the in the hybrid cube mesh depository or just PCE on your own computer, underneath the library, it will it will pick a different implementation for for communication. But at a high level, right, they're not gonna perform, you know, at the at the end very similar ways.

So let me just go to here because it's just a nice visualization of everything. So so, traditionally, a lot of the ways in which we do communication are are, communication using rings. Right? So, basically, if you wanna do a broadcast, it's not GPU zero copying to one, one copy that's and zero copy to one and zero copy to two and zero copy to three. But rather, it'd be zero copy to one and then one copy to two and then two copy to three.

The reason why we do this is because of just how the hardware is arranged. Right? So if you kinda recall, from that hybrid cube mesh topology, your GPUs are kinda connected, you know, in in this design pattern. Right? So, naturally, if you have four GPUs, you know, there are there are rings that kinda exist, you know, on the hardware topology.

Right? So the algorithm we use would be a ring. Alternatively, if we just do communication like this, I think that's how, like, networking handles broadcast. Right? If you in the, DCP IP, sometimes you do broadcast.

You just pick every single IP, and then you send it to it. Right? This is actually challenging because, let's say if you happen to be allocated these four GPUs, right, you could you could send it to here directly. You could not send it to here directly. You could send it to here directly.

So this link here would be specifically extra slow because it has to go through pset. It's not directly. So so that's why everything tends to be in terms of rings. It just mass better to the hardware. Right?

Okay. Are there other ways in which we could kind of optimize this algorithm? Right. Because right now, the way this is shown. Right?

One can't copy the two until one received all the data from zero. Right? And then similarly, two can't copy the three until two got all the data from one. Right? Are there other ways to kinda optimize this broadcast algorithm to make it faster?

No? Yes? If I'm asking the answer, yes. But how? I think zero seven two one seven three.

Like, in a pipeline way? Yeah. Yeah. Right? So you can you can pipeline the data transfer.

Right? So a lot of the general technique we learn in class for our algorithms, like, in streams. Right? When I do my vector add, I don't need all of my data to start doing my vector add because I only need a small piece of data that I need. Right?

So similarly here, when I do my my data transfer, a lot of this can they're independent from each other, essentially. Right? So why would I have to wait until I get this piece before I start sending this piece over? Right? I'm gonna if I get this piece I could start sending this, and then I could receive this later.

It's the whole notion of pipelining, essentially. Right? The general concept of pipelining, you know, it's kinda like the one of the low hanging fruit optimizations that we do everywhere. So so, you know, if if, hypothetically, you know, if it takes, you know, one second to transfer everything, Right? So instead of taking three seconds, right, I could end up pipelining it where I would instead just take one chunk of it.

Right? So, you know, let's say this is 1. Right? This is just 1 of a second. Then in the next iteration, I can start pipelining my my data transfers.

Right? So the time it would take for me to get all of my data, right, would be, like, 1 here, 1 here, 1 here. And then every single one after this would also take about, you know, one tenth at the time. Right? So in total, it would take me one second plus two tenths.

So, like, it would take me, like, one point two seconds hypothetically now. Right? Which is better than three seconds that it took before. So so, you know, by me, pipelining this this data transfer, would allow me to to make this significantly faster. So in the Nickel library, a lot of the data communication right?

If you say do a collective operation on four gigs of data, you know, assuming my model is four gigs of data, that four gig is gonna be broken up into these chunks, which is typically a couple megabytes. So that that's how the that's how the hardware essentially optimizes this data transfer. Right? So then you could do a transfer in in that way and eventually the over The total time will be significantly less than, the number of steps times times the total data. Right?

So, yeah, so so that's a lot of optimizations that that we're doing in in the library itself. Okay. Any questions? No. So if you if you like networking, right, you don't have to necessarily work for a networking company.

Right? All the accelerator companies, chip design companies, heavily used interconnects as well. Right? So Intel, AMD, NVIDIA, you could join them and work on networking as well. So so, you know, broadcast is a very simple case.

So this is another visual example of how something a bit more complicated, all reduce will be implemented. So so graphically, you know, this is how it would be done. But if you kinda think of it like how I would implement this from a coding point of view, it'd be significantly hard to say. So so with our reduce, the operation would be I will have to add all of my colors together, and then everyone gets a copy of that result. Right?

So, you know, the naive way would be to add transfer all of my green to GPU one so that I can add my green and my red, and then I send that over to GPU two. I add those together, and I send that over to GPU three. Kinda like what we did with broadcast. Right? And then I send that result around again to give everyone a copy.

You know, that will kind of provide two trip around the ring. Right? I add it once, and then I I broadcast it. That's actually very slow. Right?

We could pipeline this and and also make use of all of my links as well. Right? If you kinda wanna think of it from a hybrid point of view, you know, broadcast will only use this or this or this or this, you know, at at at a time. If I wanna make the fully use of all of these links, I would have to start pipelining it as well. Right?

So so the way to implement this algorithm at a high level, you kinda get this pipeline fashion where zero sets to one, one sets to two, two sets to three, three sets to zero of a chunk. The chunk is basically, you know, the data transfer size times the number of GPUs that you have. It's kinda like a buffer that we we have it like. So so after this communication phase, you know, what what GPU one would do is it'll add this together and then send it over. Right?

So now just following this right now, I have my green and my red. So my g p two will add blue to it. I then send that over. And then I will add yellow to it, and then I finally have the full result. Right?

I have my full result right here. However, GPU one, two, and three don't have that yet. So then just kinda loops around a bit again. And now I do my copied operation to the other GPUs. Right?

It's very complicated. So, you know, you can imagine coding this is very difficult. Right? Yeah. So one of our PhD students, I think, like, two or three years ago, graduated and joined this team at NVIDIA, for the Nickel Group.

I think his starting salary was, like, 300 something k a year. Yeah. So if you have, like, a very niche skill set, a lot of back end work. Right? You make a lot of money, infrastructure type of things.

So so yeah. And and and, you know, it's a very I think it's more recession proof job. It's back end. Everyone needs back end. Okay.

Right. So then you get you get the full results, and then you move on to the next chunk. And then and then you you perform this, pipeline addition and then broadcast operation again. Right? This is a very simple overview of it.

Nowadays, they have trees and fat trees and, depending on what is internode or intranode, they have other algorithms as well. It It it it gets very complicated, but it's at a high level what what all these algorithms are doing. Okay. Any questions? No?

Okay. Right. So then it just goes around again until we we finish it off. Oops. Alright.

Yeah. So so, basically, all of these are are built using rings. So one of the things that the hardware has to do was is to, discover the topology of your hardware and then build these ring channels, essentially. Right? So most of you could have, PCIe switch, right, at home.

Or or well, this is this is what vendor looks like. You have four GPUs connected to PCIe. So the ring would actually just keep bouncing back and between the PCIe switch with no involvement of the CPU. If you have a dual socket server, your ring would look something like this. It'll have to kind of traverse your PCIe.

So so in this case, you know, there's a lot of different channels that could be created as well. So, you know, this is the library. Back then, they call it a research library, but now it's very much production ready. And and it has a whole bunch of different patterns as well. So back then when this was released, the green ones were the ones that were already created.

All the other red ones are available now. Not then, but, you know, every iteration of Deepgram, they kinda introduce more features. So some of them was just more communication pattern. So that was we support, gather gather auto awe, and the neighborhood is basically regions of GPUs talking to each other in the auto awe way. And recently, they also have have a point to point support too as well.

So very NPI like if you wanna just just do point to point communication. Okay. So I don't think we need to. Okay. So how do you use how do you use Nico, actually?

Right? So instead of micromanaging all of those, couldn't then copy pure sync that we saw before for every single, you know, buffer. All we had to do is essentially just try to map our workloads pattern to one of the connected communication and call it. Right? So, essentially, here, you would create a, you know, handler, nickel comment it off.

Basically, this this will initialize the library to tell you how many GPUs you have and which device it runs on so that it could create the channels. And then, like, all multi GPU programming. Like, for every single GPU, set the device, and then we call it nickel out reduce. So this will essentially coordinate all of the GPUs to do a nickel out reduce operation. So you have a source data that you want to reduce.

This is gonna be the final result, where the final result will be. And this is the size of the data that you're reducing. Sum is the reduction operation. Right? Your reduction could be sum, increment, decrement, min max.

Right? All of these are supported in the library. The data type, each of these communicators, each of the GPUs have their own handler for for your, pulse. And then every single one goes with a different stream. Right?

Because we don't want it to be blocking. So so, you know, if you just replace this with zero, you're not it's not gonna work. It's just gonna hang. It's a deadlock. So this is this is why, our GPUs multi GPU programming requires multiple streams as well.

Right. So this is a very, you know, there's a very specific pattern for multi GPU programming. Right? And then if you wanna do something, right, there could be a kernel call right here, for example, for every single GPU. In machine learning training right here, it would just be that the next compute iteration, and then you and then you just repeat this over and over.

And, you know, they say it performs very well, very close to mem copy, which is, you know, your peak speed. And and they're constantly improving the performance every single time somehow. And it's open source. Right? So if you wanna see what the algorithms are doing or if you wanna hack it, like, some of my PhD hacked it for some of their projects.

You know, you can get the source code here. And then when AMD came out with their their MI three hundreds and one hundreds and whatever, they literally got the code from Ricco Nickel. They cloned it and just replaced it and called it Ricco. And then they they implemented on top of that. They literally used Nico's code base to to build off of it.

And then Microsoft has something to call MCCL. They cloned this too and just implemented on top of it. So I I don't I don't I don't know why, but it's very generous of them to make it open source. Everyone's copying from this now. So this is, like, one of the foundational libraries that that that was very foundational to, the growth of machine learning, about, like, you know, seven, eight years back.

Okay. That's the last slide. Alright. Any questions? Okay.

So, hopefully, you know, this kinda gives you an idea of how modern GPU challenges, you know, are and and kind of bridge what you learn in class and and some of the modern challenges of designing GPUs until you get into this. So, you know, if you want in your final project, if you're doing multi GPU programming, this is a library you could definitely use. It's a really app vendor. I think some some folks used it last year as well. Yeah.

Otherwise, I'll see you next week. So if you wanna meet on Thursday, feel free to sign up for a Slack. Hopefully, start on your final project. Right? Okay.

Yes.


Alright. What is that? Okay. Yeah. Just some quick announcements.

Seems like last week. Right? So lecture this Thursday. So some of you have met with me already. Some of you are almost done with your final project.

That's great. Some of you haven't started. At least you're thinking about it now. Right? Yes.

Okay. But, yeah, either way, I'm here out Thursday, most of it, so you can come bug me. Whatever. Your summit forced you this Friday. I extended it.

Right? I'm not gonna be done Friday anyways. So, yeah, we'll we'll push it back to Friday, give you an extra day. This way, you need to have three slip days. That puts it on Monday, and then we do the review on Tuesday so that that timeline kinda works out.

So next week, you know, we have a final exam from Wednesday to Friday. And then I think the reservation starts this Friday. So it's the same week as last time. Same process. And I'll make the exam easier.

Right? Oh, fine. Yes. Yay. Thank you.

Yeah. Is the exam related? Technically not. I mean, it's we're gonna focus mostly on the content after the midterm, but a lot of the stuff we just kinda build off on. Right?

Like, the notion of work divergence and everything else like that, we don't forget in the half. So, but, you know, we won't talk about what was it? Reduction, for example. Right? Those type of things.

Yeah. So it it'll mostly be from after, after the commencer. Would it she she help be helpful for you guys? Or yeah? Yeah.

Okay. For reals? Yeah. Yeah. Okay.

Yes? Okay. So I'll be good enough for two. I feel so bad for you guys after that class of commitment that you guys have. So you earned it yourself.

K. Last what? The the dish distribution of your mixer mix. Oh, okay. Yeah.

You guys earned it. You you guys deserve to have a. I mean, if you guys did crazy, you you wouldn't get one. So, yeah. So that's that's next week.

I'll make an announcement. Yeah. You know, like, regular eight and a half by 11, both sides, that type of thing. Mhmm. And then oh, I forgot to put it up here.

On Discord, I made an announcement. There was a survey that the CS testing center put out. So I know some of you guys had some feedback to me about this testing center, like, computers restarting or CAs being loud or whatever, like, that type of thing. Right? Those are feedback that they need to kind of improve on the process and stuff.

Right? So, definitely, if you have complaints or compliments, you know, you could let them know. Okay. Any other questions? Yeah.

I'm just curious. I I I I really like my project idea for the class, but, I'm just curious. It will be helpful if I want to get a job, say, a career in India in the future. Like like, say, like because usually, the there's a lot of competition. So, like, how do you make it stand out?

I think this how can you make this project stand out? I forgot. What are you doing? Like, ray tracing. Oh, ray tracing?

Yeah. But most of their careers are similar, like, security and AI. I mean, not necessarily true. So Tyler. So so, you know, you could kinda clump what industry does into, like, front and then back end work.

Right? So, like, the stuff that you mentioned, like, AIs and, okay, security and compilers and all this other stuff, they're more back end type of work. Right? Infrastructure. And then in terms of front end, I guess, ray tracing and this type of stuff is more front end type of work.

So front end work would not necessarily only be NVIDIA, but there's a lot of companies out there that make use of GPUs, that has needs for acceleration and stuff like that type of thing. Right? So so for example, some some of my nice students, I know they graduated and they worked at Qualcomm with you on GPUs, but that was a back end job. The one on front end, one of my undergrad students who took this class two years ago, he got a internship at was that company that made the oscilloscopes that donated it to the Hesite. Which one?

Hesite. Hesite. Yeah. You got an internship there. Right?

Using GPUs to accelerate something. I forgot what. And then and then because of that, he got another internship at some biomedical company. Again, they were trying to accelerate some of their workloads using GPUs. And then somebody flipped that into AMD research internship as undergrad, which was very shocking to me.

Right? But yeah. So so, you know, he got really into the old GPU programming stuff, and he said that because of this class, it kinda opened up the doors. So I wouldn't just, you know, say only NVIDIA works on GPUs. Right?

But a lot of companies that you don't think of now nowadays use use GPU programming and stuff. Right? Okay. Yeah. So, you know, keep it broad.

Right? Especially with this job economy. Mhmm. Yeah. Yeah.

That is economy's brutal. It sucks. So I graduated in o eight, o nine during the last recession. So this reminds me a lot of that time. Yeah.

So so I chose the same school. Like, I I went to PhD, and I hid there for, like, six years. But, you know A lot of people a lot of my, like, like, a lot of adults sometimes that, like, I know, in the market, like, tell tell me that you could do PhD because by the time you're done, the whole market will have figured out how to deal with the AI or That that was kind of, like, my thinking in in the late 'nine. Then this is that what you're thinking? Well, back then?

So I was, I did my undergrad in three years. So so I was like, I'm 21. I don't feel like I was responsible enough for, like, a real time full time job. So I know I wanted a a graduate degree, and I didn't wanna pay for it. So I just applied for PhD for fun, and then I got in.

So I figured if I didn't like it, I would quit after two years with free master's program. But I realized I kinda liked not having a nine to five job and just doing whatever. You know? So so I I I completed my PhD in state. But, yeah, by the time I graduated, you know, the the market was much better.

Yeah. Yeah. Like, if I joined Google and or Intel and video back then, man, I'll be a millionaire right now. I think the starting package for me back then was, like, 300 k. If I got, like, Tesla and Nvidia stock back then, I wouldn't be here.

Yeah. I know. Congrats. Right? Whatever.

Why does it be a part time? This would be a for fun thing. Yeah. Yeah. I wanna do this for fun.

Tesla. No. No. Like, I I I know a lot of people who, like, get rich and then they come back and and teach and stuff like that. Right?

It's a very common thing to do. I don't know. I I enjoy teaching. Oh oh, yeah. You could do your teaching evaluation now, by the way, right, until the end of the week.

So do it after your exam. Alright. Any other questions? No? Alright.

Okay. Oh, in terms of the speech I mean, okay. Back on the on the notion of that drop thing, there's that lead code for, like, GPU websites, right, that some people have been doing. So, you know, if you wanna do that for fun and put your name on some leaderboard and say like, hey. Look.

I can write very efficient GPU code. You could you could do that. So for summit four, there was another announcement. Right? There was, you know, three, three different ways of programs to run, like, and then the arguments that you've passed in.

I don't know if you looked at your assignment for yet. No? Yes. You have. Right?

So so most of it, you know, if you do no arguments in one argument, the default number of bins that will be created is forty ninety six. You know, when we create shared memory, typically, it's like, underscore underscore share ints a of array and of of a size. Right? Typically, when you when you put in that size, that size is static. So at compile time, it it knows how much space to allocate.

Right? So for that last argument, where you have to put the number of digits, that number is a runtime parameter. Right? Because you put it in as a command line parameter. So then when you pass it in, it the compiler doesn't know what size we create at compile time.

So so static static shared memory allocation doesn't work. So there's a way to do dynamic shared memory allocation. It's a bit of a roundabout way. So this is one of those CUDA developer blogs that we talked about. So a lot of times, when CUDA comes up with new features like shared memory and how you program it, they write these very detailed technical blog posts, which is kinda like the default way of documentation for a lot of these things.

Right? So so for here, the parameter when you when you launch a CUDA, specifies the size of the shared memory allocation that you have. Right? So in that case, it'd be, like, number of bins times the size of the bin counter, like, the ints or whatever like that. And then, yeah, they do that.

The only difference is they had to do an externcy externship in us, and then some weird things to get appointed to it. But other than that, other than that setup, the rest of it, you use Jeremy exactly the same way. Right? So there's just, like, two small extra steps that you have to do if you wanted to do, a share memory allocation, dynamic share memory allocation. And another weird thing is that if you have multiple share memory variables that you wanna make, like, thinking back to your matrix multiply, you have two share memory variables.

Right? That that total allocation, you have to add it all up. So this is, like, n of I, n of f, and n of c, and then size of. So so that's, like, three three different variables that are there allocating dynamically. Right?

So this it's just, this weird little wrinkle to it. Right? But it's it's really well documented. So so that's something you could do in in your assignment four. The alternative way, which is just aesthetically allocate $40.96 and everything else is just blank.

Right? That's that's a easy way around it, but, you know, if you wanna try the the dynamic shared memory allocation, this documentation for that. Anyone tried that yet? No? Yes.

Some of you did? Yeah. Okay. It works. Right?

It doesn't if you count it, does it freak out? Okay. Okay. I think that's all the announcement I have for today. Alright.

So the last lecture for today I mean, this is kinda like the last technical lecture for the whole class. Right? We're gonna kinda talk about the hardware challenges that we're dealing with. So, specifically, when we're designing GPUs, what are the trends and how we design these these GPUs from a hardware perspective and kind of what's driving, you know and now that you're trying to, like, fabrication and whatnot, how that's kinda driving the way we build GPUs, essentially. Right?

So this is one of those lectures where I'm gonna phrase AMD and because AMD actually did really well in this area. Right? So, some of you probably heard of multi chip modules or chiplet design, right, before that you ever know? Is that is that similar to multi core or different? It's different than multi core.

Right? So you can have a multi core processor that's not multi chip, but that's mostly CPUs, you know, in the old days. But but chip layout we're talking about today is is a new way of how you physically build the chips. And and that's really because of how Moore's law is is causing challenges with how we fabricate chips and and the cost issue with yield and everything else like that. Is the Moore's law still going to the effect?

Or is that still gonna work? Is it still, like I mean, Moore's law is still going. It's just slowing down. Yeah. It's it's still it's still not dead dead yet.

It's still going for a little while. We'll see. Until we get, like, caught some computers or something. Right? But, anyways, right, so this started, you know, a couple of years ago, maybe, like, three, four years ago, where a lot of companies started coming out with products that are multi chip designs.

Right? Specifically, AMD was one of the the earlier adopters of this, and their usage of multi chip modules or chip design is a lot more advanced. Right? So, you know, like, AMD on NVIDIA not using GPU chiplets. We have a big lead here.

So this was actually one of the big reasons why you know, this is on a GPU space. It started adopting chiplets. But on the CPU space, AMD adopted chiplets maybe, like, six, seven years ago, when the when some of those, you know, Ryzen and Threadripper started coming out. Right? So they started adopting, chiplet designs there already, and that, you know, brought their cost down.

And nowadays, I think AMD starts having a larger market share than Intel for for processor designs. Right? CPUs CPUs. Right? Wait.

They've already they've overtaken already? I think in the server space already. Yeah. And it is in the space. Yeah.

So so Intel chips, historically, like, the last couple of years, have been, like, super buggy, like, even in a microcode and everything. And any of these chips have just been, like, much better and more efficient with both more cores and everything else like that. I, like, bought an AMD processor at the start of 02/2023, and the market share said it was only a 30% store. But I did like to see a few of those. Yeah.

Yeah. Expect to take off some. Yeah. So so the the consumer space kinda lags the data center space a little bit. You know, you can't they they kinda put the more more advanced stuff there, and it kinda trickles down.

Right? But yeah. So so, like, in terms of the data center and the cloud space, AMD has a larger market share than Intel now, I think. And a lot of the exascale computers that that we built in The US, they're all AMD based as well. Yeah.

I don't think Intel or any of them have any any contracts in The US. So this was, you know, kind of put into play already a couple years back, like, five five three or five years back. Right? So I think some of the recent AMD GPUs are the consumer cards that's multi chiplets. They've already been in the data center space for the last couple of years.

Right? So so then even back then, there's a lot of news on it because about how chiplets save AMD. You know? Because they they got a heads up on energy efficiency and avoiding more slime, so on and so on and so on. Right?

So this was in the news, like, a long time ago. I don't know. You probably don't keep up with power of the news. But, you know, this is kinda like how these multichip designs look like. You know, in the old days, when you look at a CPU, you you see that, like, silver lid on it.

Right? And then you put your thermal paste and whatnot on. But if you if you pop that lid open, typically, you would just see one of these silicon thighs. Right? So the way it works out is is that instead of building one large monolithic chip, like, imagine this bigger, it'll be a lot more expensive to build.

We break it down into smaller pieces of chips, and then essentially Lego them together. Right? Essentially, when you build these chip nowadays, it's like building Legos more or less. So you can have a Lego piece for memory, for CPU, for GPU, and you just kinda stick it together all on the same board. And that's how you get your modern CPUs or processors or SSCO computers nowadays.

Right. So there's a fundamental trend as to why industry has been moving into this. And if you're, like, low level EE and you're into packaging and everything, right, there's a lot of need for, like, engineers that does, like, the packaging technology, you know, with the Chips Act and and Intel and FAST being built out in The US. There's a lot of expertise needed for this field. I think most of you are too high level for that, but, you know, there's there's a there's a growing field for that.

Right? So so the move towards shipment design, right, not not just AMD and Intel, but the whole industry as a whole, if if you have been going to hybrid design, has been kind of moving into this. So now they're starting to create standards on how do you build chiplets. So there's a universal chiplet design. It's like PCIe, but for chiplets, on a interposer level.

So a lot of FPGA companies, for example, would start building these type of things. You know, FPGA is also built built with chiplets nowadays too and so on. So so from a hardware point of view, a lot of things in a in a hardware space is now triplet design. There's a new faculty that we have in EE, professor Wangtang Li. He's introducing a lot of these courses.

Oh, you guys are graduating. It doesn't matter. But but we're having a lot of more, higher design courses nowadays that that's hopefully gonna be integrated over the next couple of years. Right? So, hopefully, we'll have a lot of more more courses.

Right? But, anyways, so I'm gonna borrow a couple of slides from industry and academia to just kinda give a background on on chip design. And this could really kind of bridges what we learn in class in in the industry and what's happening in the industry nowadays. Right? So the set of slides come from, Natalie Emery Jerger.

She's a professor at University of Toronto, and she worked with AMD Research a lot. So this this whole chip of design, AMD Research has been working out for, like, the last ten, fifteen years. And, you know, you see a market nowadays. Another interesting fact that I just learned about her is that, Jew what's that name? John Cena with a wrestler.

That's her that's her cousin. I just found that out recently, which is pretty interesting. Okay. But, anyways, but yeah. So so the whole point of doing chiplet design, it's really for cost.

Right? Everything in industry is driven by cost. We wanna do and, you know, we wanna we wanna basically build products that people buy at the cheapest cost possible. When I was at Samsung, that was literally all they talk about. How how does this impact the bottom line, essentially.

Right? And very much so, our our companies do that nowadays. So this was, you know, back in the CPU space. You know, that was the place where chiplets were, designed for for AMD, right, with the processors. So this is kinda like all of the research work that went on behind why AMD now has a larger market share to Intel, right, from, like, ten years back.

And now I kinda triple that. So nowadays, these are the processes that's built for the hyperscale exoscope computers. Instead of building, like, the one giant 64 core CPU chip, nowadays, it's actually a lot cheaper to build smaller 16 core CPU chips and then kinda stitch them together. Basically, fabricating large chip is very expensive, and it's just cheaper to manufacture, right, when you have smaller chips. Any guesses as to why it would be so expensive to make a larger chip?

Thus, it's named I'm not a silicon. Right? So so why would making a large chip be more expensive than making four small chips? Any guesses? Yeah.

The margin of air of air is higher, like, when you're making it with the I forgot what the mesh what the machine is called, but, like, the the batch or Yeah. The the lithography and stuff. Yeah. Right. Yeah.

Yeah. Yeah. Right. So right now, we're basically getting to the point where, like, your transistor is so small. We're basically at the limit of what the the lithography process can do.

Right? Lithography basically etches your so the cause is glass, essentially. Right? And the the the smallest amount that we could that's just basically get the, you know, the wavelength of the light, right, the size of the wavelength of the light. Basically, we're limited by physics.

So so transition is so small now that we're getting to the point where we can't accurately etch the glass, essentially. So a lot of times, there's a lot of errors, essentially. Right? So you have if you have a larger piece of chip, right, and I need all of the transition on that chip to work, right, if I have a lot of errors, there's a greater chance that one of those chips will have a defect. Right?

So if I want, like, a perfect chip, that's becoming increasingly rare. So that's why it increases cost. Right? The year goes down. So, you know, when you when you're buying a processor, right, you see nowadays a lot of different core counts that you could buy.

Right? You see, like, 18 or 16 or 14 or 12 core processors. Right? So so what they're really fabricating, for example, is maybe they're always just fabricating 24, and then they do test on the silicon chips. So if some of those cores have a defect, this fuses on on the silicon cord, but they just turn off those cores, essentially.

Right? So if you buy a 12 core processor versus a 16 core processor, physically, it's the same piece of dye. There's probably 16 cores on all of it. It's just that some of them have defects that you just turn off more or less. Right?

Also, for you know, it's it's the same run through the the photogrammetry machine. Right? Also, for, like, a 12 core processor, you could buy some of a higher clock frequency and some of a lower clock frequency. Right? It's still the same chip.

It's just that when we're etching the chip, right, depending on how perfect it is, you know, some of the transistors could clock at a higher frequency just because the timing margin is better. Right? And some, you you would have to clock at a lower frequency. If you clock at a higher frequency, you know, some of the timing might get incorrect. You did all this time, you know, waveform diagrams, right, in in your one twenty or something like that with a clock signal to it.

Right? If your clock signals off, right, some of the results will be wrong or incorrect, right, essentially. Right? So so so depending on how perfect those transistors are, you could clock it at different speeds. So so some of those chips could clock faster, and they'll be fine.

Some of them could clock lower, and then we just sell it at a lower frequency chip, essentially. Right? So so, you know, we we basically run through the same the five h three process, and it should we do a whole bunch of tests depending on how much yield, where the errors are, and either turn off the course or just run it faster or slower. And then we sell it as different SKUs with different product names or something like that. So that that's how industry is handling things nowadays.

Right? So because of that, right, you kinda see that, you know, as you have a lower core count, you know, your silicon is actually smaller. Right? There's less chance that there will be errors that increases the view, so, therefore, it's actually cheaper to fabricate. So so, therefore, you know, this whole push with increasing you and lowering cost really led to this whole chiplet design.

However, you know, this is not always a solution because now when you introduce chiplets, you kinda spread things out. Now there's longer communication lines between each of them, and there's challenges. So so we'll see later on in this slide set how we handle these challenges. So hypothetically, this isn't the visual way of viewing what happens. Right?

So, you know, hypothetically, let's say, you know, you have this silicone piece, and then you have, like, defects here, here, here, you know, in random places. Right? If you wanna, you know, fabricate a chip like this, like, oh, no. They all have errors. That's not good.

Right? Now if you make it even smaller oh, look. Now all of a sudden, I have some chips that actually work. Right? That's essentially the, like, the the whole philosophy behind this.

So now if it just take, you know, all of these chips that works together, now all of a sudden, I have one core that works when before I didn't have any cores that work. That that's really the whole reasoning behind that. So so, you know, from a wafer, you know, you have some errors. If you bring down the smaller pieces, all of a sudden, my unit increase. Instead of six CPU's, now I could have, you know, what is that?

Seven, eight CPU's. Right? That increases my unit and lowers my cost. Great. And because of that, you could also we I mentioned before.

Right? Some some chips could be faster or slower. So if you test all of these smaller chips and put all of the smaller chips together, the fast chips together. Right? It turns out now you can even ramp some of these quad four chips to to even higher frequencies than before up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up up on that really high end ones.

So if it was not chiplet based design, we would never see that that clock we could see ever. And then and then, you know, you could overclock it and everything else in that type of thing too. Alright. So, you know, that's not great. But the downside of that is that when we bring chips apart, you know, we increase the latency of things.

Right? So if you wanna think of it you know, let's say I have a core here that's communicating with a core right here. Right? When you communicate there, it's just metal layers. Right?

If you took c s one six eight, you know, you routed metal layers between gates and whatever like that. So that communication would just be metal layers. However, now if I'm communicating between here and here, I have to basically jump between different chips, essentially. So these wires take a lot longer, and it increases the latency. So we're actually gonna lose the performance.

Okay. So now, you know, if NVIDIA goes and say, hey. We're gonna sell you a new GPU. That's only 10% faster or 5% faster. No one's probably gonna buy that.

Right? I think I saw the news. Right? The new, was it 50 something? Is it 50 something now?

Right? Like, the new fifty seventy is still slower than, like, a thirty ninety or something like that. Something ridiculous like that. Right? So so, you know, a lot of, NVIDIA GPUs, I think, graphics side is is running into some of these issues.

But, anyways, right, the the there's a trade off between cost and and the latency that or performance that we have to that we have to handle. So so later on, we'll see how NVIDIA handles that in a little bit. But now from a manufacturing point of view, right, how do we actually build these chips for, you know, in order to have high bandwidth and low latency and so on like that? So in the old days, right, we have something called, like, multi chip modules. Basically, this is the early designs of CPUs where we would have these different chips, and we would basically put them on directly on a motherboard, for example.

Right? This this kinda avoids a lot of the pin limitations, so that means, you know, we have higher bandwidth between these chips. So this is kinda like if you have a dual socket processor and then you put them all on the same chip, it's a little faster. And a lot of the technology nowadays, in processor manufacturing is not necessarily in how do you make your processor perform computation faster, but how do I make data movement faster? So a lot of the innovation in the packaging space really is how do I connect these different chiplets together?

So some of these innovation, like, in the old days, they would create these, something called an interconnect bridge. The way you could think of it is that these bridges are like, you know, SLI on a GPU where you kind of directly connect two GPUs together. Or if you wanna take a Lego analogy, each of those Lego pieces are connected by a, like, two by n piece that you put on the edge. Right? Does that make sense?

Everyone's playing Legos? No? I have kids right now. So right. So so each of these are Lego pieces.

Right? One way to connect them together is you have this two by n piece that you just kinda put right here. Right? That kinda connects this piece together. Right?

And then they kinda communicate through that like a bridge. Right? I should have brought labels to demonstrate this. Right? Okay.

Right? So so that that's okay. Right? So that means, you know, we have a lot of point to point communication between neighbors and whatnot like that. But if I wanna communicate from, like, here to here, that gets a little challenging because I had to just go through these bridges all the time and traverse it.

It it gets really slow. The latency still increases a lot. That doesn't help me a lot. Right? So the next thing that they did was that they created something called silicon interposer.

The way to think of this is yep. You you, you know, you guys clear up breadboards before? Yeah. Yeah. Right?

So embedded systems. In embedded systems. Right? So so imagine you put all of these, chiplets on a breadboard, and the breadboard's also made of silicon itself. Right?

So now instead of just having a bridge connection between neighboring chips, you could create this whole network, you know, of metal layers, in that silicone piece, and then it could communicate with any other chip, you know, on that Lego board, essentially. Right? So then that's kind of, what a lot of these early AMD GPUs were using. The Vega Radeon, which came out, I think, like, two years ago, and all of those Threadripper stuff and Ryzen stuff that came out, that that AMD had a head start with deployment. That was it.

So this is really one of the big changes that that made, AMD chips a lot more better than than Intel chips. And then nowadays, right, we realized, hey. If our breadboard is basically another piece of silicon with metal layer in it, why can't I just put logic into it itself? Right? So now you could think of it like, you know, smart breadboard that that could do some logic in it as well.

So for example, SoC chips, like, your PCIe bridge logic, you know, you could push into the interposer. So now instead of just having legal pieces, this legal piece can also do stuff too. That's no. Legal's not not even an analogy there. So we kinda see this nowadays.

I think we mentioned it a couple weeks back. In the AMD MI 300, they started putting cache another layer of cache in that interposer level. Right? So now you can have, like, like, a CPU, a memory chip, and then another cache in here. We'll cache this in here as well.

So so they add more functionality to that to that thing. There's just a lot of innovation nowadays going in this, packaging space. And and with the whole CHIPS Act and everything, there's a lot of money from the government. There was a lot of money in the government in this? I don't know.

We'll see past tense. Okay. Right? So so, you know, you know and the poses are, you know, getting more complex. They're a bit more expensive, but they do help a lot with modularity and and increasing the, I mean, decreasing the the communication bandwidth and everything else like that.

So, you know, it's always a cost balance. Right? So, nowadays, I think a lot of modern processors, Intel, AMD, and NVIDIA, I'll move towards this active and tech prototype of technology. Okay. Right.

So the one one one of the challenges we saw before, was that because we moved to this, right, latency is an issue. So, you know, in in video, I've actually been looking at this too. Right? So these are these are papers from ISCA, internationalist component in computer architecture. It's one of the top conferences in in computer architecture.

So there's a paper from NVIDIA research. So this kinda gives you an idea of what research NVIDIA was doing in the back end a long time ago and how that kinda kinda trickles into products nowadays. So this was a paper from, I wanna say, about, like, eight years back. But, you know, this could gives you a good idea of just how far ahead NVIDIA is. Right?

Or industry is. There there always, like, four or five generations that had an industry. Alright. So so this was a internship project from a student at Arizona State. So they went to NVIDIA.

This is the the paper that came out of it. Right? So the this was, back in 2018 before any any chip with devices were out, or it was starting to come out. Right? So so, basically, you know, we have this performance increase, for every generation of GPUs that was coming out.

Basically, how can we sustain it? Right? Basically, how do we how do we keep extending Moore's law? You know, this is just a nice background on the full of the path of your process. Basically, the the end of morose law is expected to at seven nanometers.

Right now, we're at four, two, two nanometers? Three. Three nanometers? Okay. You know, all of that is a lie ever since, I think, 32 nanometers or so.

All those never has been marketing terminology. Right? I don't know if you know that. No. Okay.

So even though it's, like, seven nanometers or 12 or four, like, physically, they've they've always been, like, around 17 or so or you know? So so in the past, you know, when when these nanometers were, like, in the hundreds, they would match exactly the physical dimensions of the transistors. But starting around, like, ten, fifteen years ago, they started diverging, and now these are just marking terminologies. Right? And then I think and so they said they have, like, an angstrom process or something like that nowadays.

Right? Even smaller than nanometer. Physically, it's still in a nanometer range. It's just advertisement. But, anyways, everything is slowing down.

Right? And and that that's that's a that's the truth to it. Right? So because of, the way, you know, we're building systems nowadays, right, especially in the data center space and in cloud space, we no longer view the GPU as a singular system. So, you know, we we learned about, you know, what goes on in a chip.

And then on a board, we learned about multi GPU programming. And then in the system, now we have Ethernet, Mellanox, Ethernet bands, on all this type of stuff. Right? So with packaging now, we we add another layer to this integration level where we have something kinda in between the chip, and the board. Right?

So in terms of the amount of bandwidth that you have, it's about, you know, an order of magnitude less than a chip and about an order of magnitude, more than the board. Right? So you have this kinda in between space. The other thing that's very important with designing computer systems nowadays is not just the performance and the and the throughput that we worry about, but also the amount of energy that we consume. Right?

I don't know if you've seen the news a lot. Like, AI is consuming more power than the whole city of in the all of Switzerland in a year or something like that. That. Right? Or using as much water as all of UK or whatnot.

Yeah. Yeah. Did you see, like, the rooms of the servers, that they use to host the the, like, AM AI computation and stuff? They're so and machine is, like, so big. Yeah.

Yeah. They're, like, big data factories, I think. Yeah. Right. So so those are have always always been there.

Now they're just even larger and larger and larger. Right? Yeah. Yeah. So we we don't have any data centers here, but when when I was looking for a job interview, like, I know some places like Syracuse in Utah, they actually have, like, research data centers.

Because it's so cold there, they would open up the walls of the data center and just let the cold air in in the winter and just naturally cool the data centers. Mhmm. Like, they would just blow the hot air in and then, you know, they they dehumidify and everything else like that and then blow the air out. So they do, like, natural cooling and stuff. Right?

So there there's a lot of, research and work in industry to see how we can improve the energy efficiency of data centers. Right? Like, Microsoft specifically has been one of the forefronts they've been looking at, basically, submersing the whole server. Have you have you scared of this? You can submerse the whole server and and, like, I put it underwater.

Yeah. You put it underwater. Right? So they built these, like, containers, watertight, and put it in the water so that it would just naturally use the water and it can cool down the whole, like, container, essentially. Thing I didn't get about that was how the heck is it gonna resist all the motion?

The process up. Yeah. I'm sure there's mechanical engineers for that. Right? The other thing that's more crazy is this is something called immersive computing where you just take the raw server boards and you stick it in a pool of nonconductive liquid like mineral oil or something like that.

Right? So so not just take a container in the ocean, but you literally just take the servers directly and put it in nonconductive liquid. So so they've been looking at stuff like that too. So some data centers have actually moved to that. And the new NVIDIA b 200 system or something like that, it's it's all if you look at the whole server rack, it's nothing but liquid too.

It's like the whole thing is just liquid food nowadays. The the the GPUs are just so hot. You know, they they've been melting the consumer great GPU cards. Right? They're just so hot that you just have to liquid food on nowadays.

If an animal eats whatever animal eats, instead of it, will it become a? Followed by evolution. Right? Yeah. So, you know, energy is also becoming a big issue.

Just the data movement itself also increases by a couple of, in the word of magnitude. So so not this slide, but a couple slides later, we'll look at how energy impacts things. But this one specifically is how do we influence latency. Right? But, basically, NVIDIA has been looking at because of this this chiplet design.

Right? How do we improve the latency? How do we improve the energy aspect of things? And what's the implications? So so, you know, basically, this is how potentially, a GPU module looks like.

Basically, I I'm gonna I think GPU guys will put up this on the GPU module. Each of these are are specific chiplets. So if you ever pop the lid of GPU, you're gonna see, like, four four chiplets, and every stack even will be another chiplet, etcetera. Right? So this is an architectural view of things.

Right? So, you know, if you just look at a single chiplet like this, it looks like the GPU that we know. Right? You have an SM of an l one. It goes to the crossbar.

You have l two and then you have a DRAM. That's one GPU. But if you turn this into a chiplet, this this crossbar is basically connecting it now where your l one NSMs can connect and read the other GPU memory as well. Right? So then we have non uniform memory access.

Right? This is kinda like how multicore CPU starts to look like with multi socket and everything else like that type of thing. So so, you know, the the modern GPUs nowadays, all look like this. So so have you heard of, like, multi instance GPU for some of the, like, a 100 GPUs? No?

Okay. So so modern GPUs, like, a 100, it's like one physical card. But because they're built using chip technology now, you could basically, with, like, NVIDIA SMI, you could just configure it where you could split your GPU into, for example, three GPUs, you know, two GPUs of one slice of GPU and another GPU of two slice of GPU resources. Right? So with the a 100, they have seven slices, and you can partition that up to to expose, you know, up to seven GPUs, for for example.

So when you run NVIDIA SMI, you actually see seven physical GPUs. And then AMD AMD one does the same thing nowadays too, at least in the data center space. I don't think any of the consumer ones does this. Most consumer cards, you don't have enough SMs. Right?

So so you don't you don't partition it. But, anyways, right, this is this is how a a multi chip module GPU looks like. It's very similar to what we learned in class. Just kind of slightly partition up, where the memories, would would would interface with each other. Right.

So you can kinda tell that. Right? You know, if I wanna access memory from a different chiplet, right, there's a larger path that I have to go through, and then we're gonna increase latency, essentially. So what they found is that depending on the bandwidth of my interconnect link between my chiplets, I can have significant slowdown in in my workloads. Right?

So so, you know, for example, you know, if my bandwidth is, like, 1.3 terabytes per you know, there's about a 15% slowdown on memory in terms of computation. It's it's it's a memory access. So, you know, if NVIDIA wants to sell you a chip, a brand new GPU that's 10% slower, no one's gonna buy that. Right? So now, you know, this is really a major issue that NVIDIA has to solve.

And, you know, this was the problem that they have before they they commercialize everything. So how do we kinda solve that? Right? How do we make sure that we have enough bandwidth and enough architectural tricks to play in the back end so that we could sell you a slower chip? But because of all these tweaks, you still get the same performance that you expect.

So they they came out with several new designs. Basically, you know, the solution to a lot of things is a cache. Right? So we have something called 1.5 cache. We play with the way we do scheduling of the workload, and then we do different ways of allocating their inner pages, essentially.

Right? So so in a nutshell, these are all pretty intuitive. So the thing they created was a 1.5 cache. Right? By definition, it's just between one and two.

Right? So, hence, 1.5. However, the main goal here is because, you know, we wanna slow down we we actually we wanna speed up the access from, you know, this SM to my remote DRAM. So the goal of this 1.5 access to basically only cache my remote memory accesses. So if I need something from another chiplet, I'm gonna cache it, and I'm gonna reuse it.

Right? If it's on my own chiplet, I don't need to cache that. There's no benefit of a 1.5 cache for that. So this essentially can can, you know, speed up a lot of remote accesses and have, you know, minimal area overheads. A lot of things we do in our architecture space, the area overhead is very critical to to also be aware of.

Another thing that they did was they played around with, CTA scheduling. So by default and this is kinda intuitive if you wanna think of it. Right? Like, if you did, you have a let's think of a matrix multiply algorithm. Right?

So, you know, your matrix multiply has several SMs, threat blocks that you create. Right? So the way you would typically partition up your threat blocks, now you have their block zero, one, two, three, four, five, six, seven, and so on and so on and so on. Right? By default, the way the hardware scheduler would schedule this across SMs and chiplets is round robin.

Actually, hold up. This is a bad example because this still works. Alright. Let let's make a weird number. 83456789.

Okay. Right. So the way the way the default scheduler will work is that, you know, your flight was scheduled round robin. Like, so 01234567891011. Right?

So when we schedule in this way, you know, 048048, they they don't share a lot of locality. Right? So so, basically, the small tweak here was to schedule essentially neighboring ascent to the same asset, neighboring center cluster to the same ascent to to preserve locality. So instead of doing a round robin way, the the way, I guess, modern modern GPS does this nowadays would be, you know, one zero one two three four five six seven. So, you know, in this case here, we all share the locality from here.

There'll be a lot more better efficient usage of of your your caches. Right? So this is done at the hardware level. On the research side of things, you know, some researchers realized this already, and they were able to do software tricks in order to to do that type of scheduling of software, even with the scheduling. Right?

So so there's there's tricks you could play on a software level as well. But, you know, if you do this in the hard way, then you don't have to do that in software. It makes it easier for people. And then the last one, I I think I'm I lost the site for that. It's it's called touch page page placement.

So if you could think of it like when I do my CUDA malloc, right, I'm not gonna allocate data anywhere yet. So, basically, wherever the SM that needs that memory that's where I'm gonna allocate it. Right? So rather than random me randomly putting my my memory pages randomly, I'm gonna put it on a place that touches it so that way I improve locality as well. K.

So so, anyways, there's a lot of these architectural tricks that they played for. I'm sure there's a lot more internally that they played with in this. So, in the video, right, they they don't you know, when we when we design these new GPUs and play with new features, we don't fabricate a chip to test it out. Right? That takes a long time, and it's it's very expensive and we can't iterate fast enough.

So a lot of times in industry and in academia, when we design new GPUs, what we do is we play with architectural simulators. So they have a software model of how the hardware works, and they essentially just implement all of these, new features in that software model. Right? So they software model all of these, GPUs, and they run-in a whole bunch of benchmarks. And that's how they kinda converge on future generation of GPU design essentially.

Right? So so I think, like, if you're in industry, sometimes you might hear the term like pathfinding or something like that. These architectural designs, teams, they do the path finding to figure out, you know, how do you design the architecture. And then you have a whole bunch of design engineers that implement it, and then and then it becomes product. Right?

So so this is the team that does that. So they they what they found is that, with an optimized multi chip module with all of these tip, tricks and and tweaks that they were able to do, they're able to lower the bandwidth requirement by about five x. Right? So so in the past, basically, you're gonna get that, like, amount of traffic going between the chiplet decreased by five x. So, therefore, I don't need as much bandwidth between the chiplet to get the performance that I need.

In terms of raw performance speed up, with all of these enhancements, right, we get about a 22% improvement in performance. Right? That's about the the you know, from generation to generation, that's about the performance you see from GPUs nowadays. Right? About 20%.

Yeah. Right? So, you know, every generation, they they they do this microarchitectural, exploration, so to to get this, you know, 20% benefit every every time. So so that's the the point of view from the performance point of view. Right?

And then, you know, the next year, they had another paper for the same team looking at the energy efficiency side of things. So, basically, when you transmit energy over metal layers in in silicon, it's a lot less expensive than pushing it through a multichip module going all the way through the interposer and then up again. There's a lot more, energy consumption per per bit of data that you're transferring. So now they're looking at, you know, what are the implications of all of these, you know, increased energy cost and computation. Mainly because, you know, energy is such an important thing nowadays when we're designing these these AI MLM data centers.

So so there's a lot of different ways of integration that we saw before. Right? The way we're looking at nowadays mostly is, you know, on package integration with inter posers. But nowadays, they wanna look at specifically the energy of things. So as you increase the number of GPMs or triplets on a GPU, the energy actually increases significantly.

So even though your performance may stay the same just because of data movement, now all of a sudden your GPU consumed two times the amount of power. Right? For a consumer card, you know, your your power cable is already melting. You can't have another two x amount, you know, of power on your on your whole GPU. You're literally gonna need to get, like, a whole circuit breaker just for your computer nowadays.

Right? It's gonna use 20 amps, 15 amps, or whatever. So, you know, this is another major issue that, internally a lot of these companies, a and d and Nvidia and Intel, been looking at. So energy is now the order design constraint. It's not just performance.

So so in order to, you know, estimate how much energy, consumption they consume, right, it also doesn't make sense for them to fabricate a chip and then measure the power, you know, as a what if scenario. So a lot of the times what we do is we do a lot of analytical modeling of things. And, similarly, they they do analytical modeling here too. So so the thing, you know so so one of the novel things of this work was how can we estimate the energy consumption of a workload without actually building the the GPU itself? So they have to build a energy model.

They call it GPU dual. And, analytically, you can think of it as if I could derive the energy of every single instruction and the energy of every memory loaded store, and I can figure out how much energy that consumes. My analytical model is just as simple as, you know, like, the number of ads times the energy per ad, you know, for every single instruction type plus the number of loads times the energy per load. Right? So so, realistically, this gives us a pretty good estimate of of that energy consumption of future GPUs.

So they will design this whole framework of trying to reverse engineer and figure out how much So, basically, it's just a loop that runs assembly code over and over again in order to identify the energy of every single load or or ad instruction. And they were able to validate it, and it was, like, it was pretty accurate. Was it here? Yeah. They they could find very interesting data.

Like, for example, every ad that you consume, it it would consume, you know, like, point one five nanojoules. Right? So it's a very, very accurate way of measuring things. So now this way, I don't need a real GPU to estimate the power. We could have the hardware engineers kind of, you know, scale this number up or down just depending on the processor technology or the interconnect.

And then it's just a matter of when I run the workload, I just do a instruction count, and they just multiply this, and then they get my energy estimate, essentially. Right? So this way, in the street, you're able to kind of iterate quickly on different different, you know, design parameters when we're trying to figure out how to build the best processor or GPU. Right? So so using this energy model, right, they they were able to see that.

It's mostly accurate. In terms of, your floating point using different memories, that's about 98% accurate. And then our real world workloads, you know, some of the errors would be, like, 40% off or 50% off, but I have, which is 90% accurate. That doesn't seem very good, but that's actually really actually really good. And my adviser my PhD adviser worked at Intel working on simulators.

He said that a lot of times, these simulators are about 25% accurate, and that's that sounds horrible. Right? So anything better than 20, you know, 25 error, you know, it's a really it's a really great simulator. So that means that a lot of times when these engineers, you know, do these explorations on simulators, the rule of thumb in Intel was, if you got a 10% improvement in the simulator, but it's not implemented, it's a 1% improvement. Right?

And what percent is still great for a lot of processors because we're just constantly just trying to squeeze out, as much performance as possible. So yeah. You know? This is the accuracy that we're dealing with. Right?

So so using that that energy model, right, they're able to create a new metric that essentially could tell us more or less, you know, the energy efficiency compared to extra set of strong scaling. So, you know, if I have an ideal monolithic processor, how slow how much less energy efficient am I compared to an ideal monolithic processor, that type of thing. Right? So so, essentially, you wanted to be a 100%. 30% means that I'm only 50% ideal.

And either way, it means larger is better. Right. So, again, they they did the simulation model, and they looked at various type of interconnects between the chiplets. And defining was essentially that, you know, the more chiplets I have, the less efficient they are. Right?

You know, this was already something that they they they already suspected, and this kinda validated everything. So how can we essentially avoid that? Right? Or, also, what's the ideal number of chiplets I should have? So because of this model, they were able to identify that, you start losing energy when you start having more than eight chiplets for GPUs.

You can kinda see this now because the a 100, you have about seven chiplets. And then with the a and d and my 300, they also have, eight chiplets per per GPU also as well. Right? So so all of this modeling and everything else like that, you can kinda see the effect of that on the actual product itself. You don't wanna see more than more than eight chiplets to the line mainly because it becomes way too energy inefficient.

And then, you know, if you have two or four, you know, it's it's a bit more energy efficient, but you don't gain too much in terms of cost savings from you and stuff like that. Right? So the sweet spots are around eight, seven, or eight. And that's kind of what we're seeing in the modern is in the center data center scout GPUs. So so, you know, this is kinda how a lot of these research informs the product nowadays.

You can kinda see it now in product, so it's just kinda cool. And, similarly, right, they were also able to analyze, if my chiplet bandwidth is one or two or four times faster than we have now, what would be the, energy efficiency improvement that we could expect to gain? So, basically, they say that if you want to have the same energy efficiency of our modern chips with, you know, newer chiplet designs, we would basically need to increase the bandwidth by four x. So, you know, that kinda gives a go for the the communication, in our kind of engineers. Right?

Now your goal is to make this four x faster. You know? That that's the goal for us to, essentially make a product run the same amount of energy as as old old GPUs, essentially. Right? So this also helps you set the goals for for different engineering teams as well and so on.

So so, yeah, that that's more or less, you know, the a lot of the insights that's that's kinda derived from from these architectural modeling of things. So yeah. So so, you know, this was back, in, like, 2018. So a lot of research nowadays, you know, kind of do all this, and and the software analysis as well. I want that.

But but, yeah, a lot of the research nowadays in in GPU architecture and and industries is how do we design shipless to make it energy efficient and perform it well, what keeping cost well. Okay. Any questions? No? Okay.

I think that's all I have for today. But, hopefully, this is the last technical lecture, but the whole point here is, hopefully, you know, this can kinda bridge the knowledge that we have in class to what's happening in the industry nowadays. So, you know, if you look at modern GPU views and how to design new GPUs, hopefully, everything's trying to make sense. You know, really, that's the whole goal for this class. Okay.

Great. I'll make some of the announcements later on, and I'll post it.